{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. url을 읽는다.\n",
    "# 최종 정답. 아래꺼 볼 필요 x\n",
    "NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "query = input(str(\"키워드를 입력하세요: \"))\n",
    "\n",
    "# 나중에 input으로 바꿀 수있다. \n",
    "for i in range(1,11):\n",
    "    params = {\n",
    "        'where': 'news',\n",
    "        'query': query,\n",
    "        'start': int((i*10)+1)\n",
    "    }\n",
    "    resp = requests.get(NAVER_SEARCH_URL, params=params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    ul_contents = soup.find('ul', class_='type01')\n",
    "    new_ul = []\n",
    "    for content in ul_contents:\n",
    "        ul_contents.contents\n",
    "        if not str(content).strip():\n",
    "            continue\n",
    "        new_ul.append(content)\n",
    "    news_list = []\n",
    "\n",
    "    for li in new_ul:\n",
    "        news_dict = {}\n",
    "        a_tag = li.find('dl').find('dt').find('a')\n",
    "#    print(a_tag)\n",
    "        news_dict['link'] = a_tag['href']\n",
    "        news_dict['title'] = a_tag.text\n",
    "\n",
    "#         b_tag = li.find('dl').find('dd', class_ = \"txt_inline\").find('span')\n",
    "#         news_dict['company'] = b_tag.contents[0]\n",
    "#     # 왜 contents[0] 인가요? 리스트로 접근하는 방법입니다.\n",
    "    \n",
    "#         c_tag = b_tag.find_next('dd')\n",
    "#         news_dict['script'] = c_tag.text\n",
    "#    print(news_dict)\n",
    "        news_list.append(news_dict)\n",
    "    \n",
    "    pprint(news_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_ul)\n",
    "# 몇개가 있어?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 심화. \n",
    "#### 한 페이지에 있는 기사내용 긁어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답.\n",
    "news_list = []\n",
    "NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "# 찾고자하는 페이지의 URL과 변수선언\n",
    "query = input(str(\"키워드를 입력하세요: \"))\n",
    "# 찾고자 하는 키워드를 입력하세요.\n",
    "\n",
    "for i in range(1,11):\n",
    "    # start_var = 1\n",
    "    params = {\n",
    "        'where': 'news',\n",
    "        'query': query,\n",
    "        'start': int(i*10)+1,\n",
    "    }\n",
    "    resp = requests.get(NAVER_SEARCH_URL, params=params)\n",
    "    # 검색결과를 가져온다. (page 별로)\n",
    "    \n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    # 검색결과를 soup로 만든다.\n",
    "    \n",
    "    # ul의 ' ' 제거\n",
    "    ul_contents = soup.find('div', class_='type01')\n",
    "    new_ul = []\n",
    "    for content in ul_contents:\n",
    "        ul_contents.contents\n",
    "        if not str(content).strip():\n",
    "            continue\n",
    "        new_ul.append(content)\n",
    "    # 컨텐츠 추가 완료 \n",
    "                \n",
    "    # 리스트 탐색\n",
    "    for li in new_ul:\n",
    "        news_dict = {}\n",
    "        a_tag = li.find('dl').find('dt').find('a')\n",
    "\n",
    "        news_dict['link'] = a_tag['href']\n",
    "        news_dict['title'] = a_tag.text\n",
    "        \n",
    "        print(news_dict['link'], '에 들어갑니다.')\n",
    "        # 디테일 페이지 링크를 들어갑니다.\n",
    "        detail_resp = requests.get(news_dict['link'])\n",
    "        detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')\n",
    "        news_dict['body'] = detail_soup.find('body')\n",
    "\n",
    "        b_tag = li.find('dl').find('dd', class_ = \"txt_inline\").find('span')\n",
    "        news_dict['company'] = b_tag.contents[0]\n",
    "        # 왜 contents[0] 인가요? 리스트로 접근하는 방법입니다.\n",
    "    \n",
    "        c_tag = b_tag.find_next('dd')\n",
    "        news_dict['script'] = c_tag.text\n",
    "        news_list.append(news_dict)\n",
    "    \n",
    "    pprint(news_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개발 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. url을 파악한다.\n",
    "2. url에 접속한다.\n",
    "3. html 파일을 soup로 만든다. \n",
    "4. soup에서 각 항목들을 정리한다. # 제일어려움\n",
    "5. soup에서 상세페이지의 href를 가져온다.\n",
    "6. 각 상세페이지의 href를 requests.get() 한다.\n",
    "7. soup로 만들고 상세페이지의 각 항목들을 정리한다.\n",
    "8. 반목문을 통해서 pagination을 이동한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예시\n",
    "\n",
    "< a href = 뭐있음, class = 'link' active >는 예시가 있습니다.\n",
    "href를 가지고 오자.\n",
    "1. a_tag['href']\n",
    "2. a_tag.attr\n",
    "    {'href': ......\n",
    "     'class': 'link' active\n",
    "    }\n",
    "3. a_tag.attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스토픽 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url을 파악한다.\n",
    "# url에 접속한다.\n",
    "# html 파일을 soup로 만든다.\n",
    "# soup에서 각 항목들을 정리한다. # 제일어려움\n",
    "# soup에서 상세페이지의 href를 가져온다.\n",
    "# 각 상세페이지의 href를 requests.get() 한다.\n",
    "# soup로 만들고 상세페이지의 각 항목들을 정리한다.\n",
    "# 반목문을 통해서 pagination을 이동한다.\n",
    "\n",
    "NAVER_SEARCH = \"https://search.naver.com/search.naver\"\n",
    "resp = requests.get(NAVER_SEARCH)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "NAVER_NEWS_TOPIC = soup.find('div', class_= \"realtime_srch _aside_news_tab\")\n",
    "\n",
    "for ol in NAVER_NEWS_TOPIC:\n",
    "    news_topic_dict = {}\n",
    "    a_tag = ol.find('li').find('a').find('span', class_=\"keyword\")\n",
    "    news_topic_dict['title'] = a_tag['span']\n",
    "\n",
    "print(news_topic_dict)\n",
    "# 오류: 'NoneType' object is not iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 방법3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_topic = soup.find('ol', class_=\"lst_realtime_srch _tab_area\")\n",
    "news_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. contents 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_tags = news_topic.contents\n",
    "new_list = []\n",
    "for t in li_tags:\n",
    "    title = t.find('span', class_='tit').text\n",
    "    new_list.append(title)\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. sibilings 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_tag = news_topic.li\n",
    "\n",
    "new_list2 = []\n",
    "\n",
    "title = li.find('span', class_='tit').text\n",
    "new_list2.append(li_tag.text)\n",
    "# 첫번째 리스트는 for 문을 돌지 않는다. 아래 코드가 sibilings를 찾는 것이기 때문이다.\n",
    "\n",
    "for x in li_tag.find_next_siblings('li'):\n",
    "    title - li.find('span', class_='tit').text\n",
    "    new_list2.append(li.text)\n",
    "    \n",
    "new_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_all(recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_lists = news_topic.find_all('li', recursive=False)\n",
    "len(li_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list3 = []\n",
    "for li in li_lists:\n",
    "    title = li.find('span', class_='tit').text\n",
    "    new_list3.append(title)\n",
    "    \n",
    "new_list3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
