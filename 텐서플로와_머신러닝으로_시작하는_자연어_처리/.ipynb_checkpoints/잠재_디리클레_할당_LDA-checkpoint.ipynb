{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잠재 디리클레 할당 모델 \n",
    "알파와 베파는 상수\n",
    "단어들로부터 확률을 추출 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA의 가정\n",
    "1. **단어 교환성(exchangeability):** 단어 교환성은 순서를 고려하지 않고, 빈도 수만을 사용한다.  \n",
    "    이를 기반으로, 단어들의 교환성을 구한다.\n",
    "    \n",
    "### LDA할당 절차\n",
    "1. 토픽 개수를 k개로 설정. D개의 전체 문서에, k개의 토픽이 분포되어 있다고 가정\n",
    "2. 모든 단어를 k개 토픽 중 하나에 임의 할당  \n",
    "    그 결과, 각 문서는 토픽을 가지고, 토픽은 단어 분포를 가지게 된다. \n",
    "3. 재할당을 반복\n",
    "    2.에서 임의로 할당하였지만, 올바르게 할당되었다고 가정한다.\n",
    "    p: 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율\n",
    "    p: 단어 w를 갖고 있는 모든 문서들 중 토픽 t가 할당된 비율\n",
    "    p: p x p에 따라 토픽할당\n",
    "    \n",
    "결국, 문서, 단어, 토픽의 개수는 사용자가 정한다.  \n",
    "그 이후, 지속적인 재할당 작업을 반복하며, 안정화(**변동이 없을 때까지**)가 되도록 한다. \n",
    "\n",
    "### 한계\n",
    "parameter의 개수 설정값에 따라서 너무 달라진다.  \n",
    "어느정도의 수준에서 parameter숫자를 정해야하는지에 대해서 의견이 분분하다..   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 잠재의미분석(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ls = ['Cute kitty',\n",
    "           'Eat rice or cake',\n",
    "           'Kitty and hamster',\n",
    "           'Eat bread',\n",
    "           'Rice, bread and cake',\n",
    "           'Cute hamster eats bread and cake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 소문자로 변환 \n",
    "lower_docs_ls = []\n",
    "\n",
    "for docs in docs_ls:\n",
    "    docs = [word.lower() for word in docs.split()]\n",
    "    lower_docs_ls.extend(docs)\n",
    "\n",
    "# 모든 단어들\n",
    "lower_docs_ls\n",
    "\n",
    "# 중복이 제거된 Unique한 단어들\n",
    "unique_word_ls = list(set(lower_docs_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'kitty', 'eat', 'rice', 'or', 'cake', 'kitty', 'and', 'hamster', 'eat', 'bread', 'rice,', 'bread', 'and', 'cake', 'cute', 'hamster', 'eats', 'bread', 'and', 'cake']\n",
      "['kitty', 'cute', 'and', 'eat', 'bread', 'rice,', 'rice', 'cake', 'hamster', 'eats', 'or']\n"
     ]
    }
   ],
   "source": [
    "print(lower_docs_ls)\n",
    "print(unique_word_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
