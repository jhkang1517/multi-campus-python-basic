{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 유사도 측정\n",
    " - 클래스 초기화: 문장 리스트\n",
    " - 출력1: 벡터별 TD-IDF 출력\n",
    " - 출력2: 벡터간 코사인 유사도 계산 및 출력 (벡터 목록 중 2개만 추출하여 계산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors:  {'ruled': [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0], 'india': [0.6438410362258904, 0.0, 0.2575364144903562, 0.18395458177882582]}\n",
      "Vectors:  {'chalukyas': [0.0, 0.7954314537066303, 0.0, 0.0], 'ruled': [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0], 'badami': [0.0, 0.7954314537066303, 0.0, 0.0]}\n",
      "['ruled india', 'Chalukyas ruled Badami', 'So many kingdoms ruled India', 'Lalbagh is a botanical garden in India']\n",
      "ruled -> [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0]\n",
      "india -> [0.6438410362258904, 0.0, 0.2575364144903562, 0.18395458177882582]\n",
      "\tsimilarity of document 0 with others\n",
      "[[1.         0.29088811 0.46216171 0.19409143]]\n",
      "\tsimilarity of document 1 with others\n",
      "[[0.29088811 1.         0.13443735 0.        ]]\n",
      "\tsimilarity of document 2 with others\n",
      "[[0.46216171 0.13443735 1.         0.08970163]]\n",
      "\tsimilarity of document 3 with others\n",
      "[[0.19409143 0.         0.08970163 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class TextSimilarityExample:\n",
    "    # __init__ ->\n",
    "    def __init__(self):\n",
    "        self.statements = [\n",
    "            'ruled india',\n",
    "            'Chalukyas ruled Badami',\n",
    "            'So many kingdoms ruled India',\n",
    "            'Lalbagh is a botanical garden in India'\n",
    "        ]\n",
    "        \n",
    "# 배운 걸로 재 구현 해보자\n",
    "    def TF(self, sentence):\n",
    "        # .lower를 사용해서 전부 소문자로 변형 후 token화\n",
    "        # 근데 왜 소문자로 만들어야 하나요?\n",
    "        words = nltk.word_tokenize(sentence.lower()) \n",
    "\n",
    "        # 단어 빈도수 count\n",
    "        count_dict = {} # 숫자를 셀 빈 dictionary를 만든다. \n",
    "        for word in words: # words 리스트 안에 word를 하나씩 꺼내고\n",
    "            count_dict.setdefault(word, 0) # 기본 값은 0으로 둔다. \n",
    "            count_dict[word] += 1 # 같은 이름을 가진 word가 등장하면 1씩 += 해준다. \n",
    "        \n",
    "        # TF 구하기\n",
    "        dictionary = {} # TF를 구하는 새로운 dictionary\n",
    "        for key in count_dict.keys(): # 딕셔너리의 key값, 즉, 단어들을 하나씩 가져온다.\n",
    "            norm = count_dict[key]/float(len(words)) # val 값을 전체 단어 수로 나누어준다. 빈도수 측정 공식\n",
    "            dictionary[key] = norm # 값을 빈도값으로 재정의 한다.\n",
    "        return dictionary # 딕셔너리를 반환한다.\n",
    "\n",
    "    def IDF(self):\n",
    "        # 먼저 idf를 구할 공식을 def idf 로 정의한다.\n",
    "        def idf(TotalNumberOfDocuments, NumberOfDocumentsWithThisWord):\n",
    "            return 1.0 + math.log(TotalNumberOfDocuments/NumberOfDocumentsWithThisWord)\n",
    "        \n",
    "        numDocuments = len(self.statements) # 전체 문장의 개수 (이걸 왜 Documents라고 하지?)\n",
    "        uniqueWords = {}\n",
    "        idfValues = {}\n",
    "        \n",
    "        for sentence in self.statements: # 문장을 문장리스트에서 하나씩 가져온다.\n",
    "            for word in nltk.word_tokenize(sentence.lower()): # token화 시킨다.\n",
    "                if word not in uniqueWords: # 만약 단어가 빈 딕셔너리에 없으면, \n",
    "                    uniqueWords[word] = 1 # 그 단어를 추가하고 1값을 준다.\n",
    "                else:\n",
    "                    uniqueWords[word] += 1 # 만약 단어가 있으면, += 1을 해준다.\n",
    "                    \n",
    "        for word in uniqueWords: # 다시 단어를 꺼내온다.\n",
    "            idfValues[word] = idf(numDocuments, uniqueWords[word]) # 공식을 통해 idf 값을 구한다.\n",
    "        return idfValues\n",
    "\n",
    "    # TF-IDF 구하는 공식\n",
    "    def TF_IDF(self, query): \n",
    "        words = nltk.word_tokenize(query.lower()) # 먼저 단어들을 토큰화\n",
    "        idf = self.IDF() # idf는 위에서 정의한 IDF를 그대로 활용한다.\n",
    "        vectors = {} # vector 값을 받을 빈 딕셔너리 생성\n",
    "        for sentence in self.statements: # 문장리스트에서 문장을 꺼내온다.\n",
    "            tf = self.TF(sentence) # TF 역시 위에서 정의한 TF를 그대로 활용한다. \n",
    "            \n",
    "            for word in words: # 단어를 하나씩 꺼내온다.\n",
    "                tfv = tf[word] if word in tf else 0.0 # tfv의 값은 단어가 tf에 있을 때만 받고 ,그 외는 0\n",
    "                idfv = idf[word] if word in idf else 0.0 # idfv의 값은 단어가 idf에 있을 때만 받고, 그 외는 0\n",
    "                mul = tfv * idfv # 곱해준다. # 이거 왜 곱하는거야?!?!\n",
    "                \n",
    "                if word not in vectors: # 그 다음, 단어가 vector에 없으면 (빈 딕셔너리기 때문에 당연히 없음)\n",
    "                    vectors[word] = [] # 벡터의 단어를 key로 하는 key와 빈 리스트 val 값을 만들고,\n",
    "                vectors[word].append(mul) # 거기에 mul 값을 추가한다.\n",
    "        print(\"Vectors: \", vectors)\n",
    "        return vectors # 벡터 완성\n",
    "\n",
    "    def displayVectors(self, vectors):\n",
    "        print(self.statements) # 문장들 확인\n",
    "        for word in vectors: # 만약 단어가 벡터안에 있으면, \n",
    "            print(\"{} -> {}\".format(word, vectors[word])) # 단어는 -> 이 벡터로 변했다.\n",
    "\n",
    "####################################### 여기까지 이해 끝냄 #######################################            \n",
    "    \n",
    "# 아나\n",
    "    def cosineSimilarity(self):\n",
    "        vec = TfidfVectorizer() # vec는 TF-IDF Vectorizer를 활용\n",
    "        matrix = vec.fit_transform(self.statements) # 문장을 바로 박아서 matrix를 생성한다.\n",
    "\n",
    "        # 1이상 5미만의 for문을 돌린다. \n",
    "        for j in range(1, 5):\n",
    "            i = j - 1\n",
    "            # j = 1, 2, 3, 4\n",
    "            # i = 0, 1, 2, 3\n",
    "            print(\"\\tsimilarity of document {} with others\".format(i)) # 문장 위치(인덱스)를 나타내는 문장 출력\n",
    "            similarity = cosine_similarity(matrix[i:j], matrix) # 왜 0과 1의 유사도지? 0과 0이여야지 않나?\n",
    "            print(similarity)\n",
    "\n",
    "    def demo(self):\n",
    "        inputQuery = self.statements[0]\n",
    "        vectors = self.TF_IDF(inputQuery)\n",
    "        \n",
    "        inputQuery2 = self.statements[1]\n",
    "        vectors2 = self.TF_IDF(inputQuery2)\n",
    "        \n",
    "        self.displayVectors(vectors)\n",
    "        self.cosineSimilarity()\n",
    "\n",
    "similarity = TextSimilarityExample()\n",
    "similarity.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'djieljfaaaejf'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"djieljfAAAEJF\"\n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'사과': 1, '바나나': 1, '딸기': 1})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['사과','바나나','딸기']\n",
    "a = nltk.FreqDist(words)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict.keys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {'ruled': [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0], 'india': [0.6438410362258904, 0.0, 0.2575364144903562, 0.18395458177882582]}\n",
    "\n",
    "# 실패한 코드 \n",
    "# 코사인 유사도 구하기\n",
    "def cosineSimilarity(self):\n",
    "    temp_list = []\n",
    "    # x와 y, 두 벡터의 코사인 유사도를 계산하는 함수\n",
    "\n",
    "    # for문돌려서 벡터 두개 꺼내오고,\n",
    "    for word in vectors:\n",
    "        # print(vectors[word])\n",
    "        temp_list.append(vectors[word])\n",
    "\n",
    "    print(temp_list)\n",
    "    # x, y로 수정한다음에\n",
    "    x = temp_list[0]\n",
    "    y = temp_list[1]\n",
    "    \n",
    "    nominator = np.dot(x,y) # 분자\n",
    "    donominator = np.linalg.norm(x) * np.linalg.norm(y) # 분모\n",
    "    similarity = nominator / donominator\n",
    "    print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 뉴스 요약\n",
    " - 입력: 네이버 뉴스, url, 요약 비율\n",
    " - 출력: matrix 혹은 그래프 활용 textrank 구현 이용한 문서 요약\n",
    " \n",
    "출력: 요약문(요약비율 적용), 원문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def summarizeNaverNews(url, ratio):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "8월부터 이마트, 홈플러스 등 전국 대형매장 판매온라인, 한정판매 완판 기록하며 소비자 인기 입증포기하지 마라탕면.\n",
      "===============\n",
      "한정판 세트를 단독 판매한 11번가에서는 판매 시간 동안 실시간 쇼핑 검색어 1위를 달리며 높은 관심을 입증했고, 전체 판매량 3위에 이름을 올렸다.풀무원은 앞으로 11번가 외에도 다양한 온라인 채널을 통해 소비자를 위한 세트 구성 및 프로모션을 진행할 계획이다.이기욱 풀무원식품 생면식감 사업부 PM(Product Manager)은 “’포기하지 마라탕면’은 두 차례의 온라인 한정 판매서 단기간에 완판되는 저력을 보여줬다.\n",
      "===============\n",
      "더불어, 라면 조리 시 기름으로 인해 맛이 퍼지는 현상이 나타나지 않아 정통 마라탕에 가까운 국물 맛을 구현해냈다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.summarization.summarizer import summarize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import networkx as nx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=018&aid=0004435771'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# soup\n",
    "content = soup.find('div', id='articleBodyContents').text\n",
    "# 나중에 개행문자와 '오류를 우회하기 위한 함수 추가와 같은 것들 제거할 것'\n",
    "Text = content\n",
    "Text = Text.strip()\n",
    "Text = Text.lstrip()\n",
    "Text = Text.replace('\\n','')\n",
    "Text = Text.replace('// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}','')\n",
    "Text = Text.replace(\"이윤화 (akfdl34@edaily.co.kr)네이버 홈에서 ‘이데일리’ 뉴스 [구독하기▶]꿀잼가득 [영상보기▶] , 청춘뉘우스~ [스냅타임▶]＜ⓒ종합 경제정보 미디어 이데일리 - 무단전재 & 재배포 금지＞\",'')\n",
    "# print(Text)\n",
    "\n",
    "\n",
    "# 문장간 유사도 측정 (BOW를 활용한 코사인 유사도 측정)\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    # 각 문장을 소문자로 변환\n",
    "    sentence1 = [word.lower() for word in sentence1.split()]\n",
    "    sentence2 = [word.lower() for word in sentence2.split()]\n",
    "    \n",
    "    # BOW 생성을 위한 unique한 단어로 배열 생성\n",
    "    words_ls = list(set(sentence1 + sentence2))\n",
    "    \n",
    "    bow1 = [0] * len(words_ls)\n",
    "    bow2 = [0] * len(words_ls)\n",
    "    \n",
    "    # 첫번째 문장 BoW 생성\n",
    "    for word in sentence1:\n",
    "        bow1[words_ls.index(word)] += 1\n",
    "        \n",
    "    # 두번째 문장 BoW 생성\n",
    "    for word in sentence2:\n",
    "        bow2[words_ls.index(word)] += 1\n",
    "        \n",
    "    return cosine_similarity(bow1, bow2)\n",
    "\n",
    "# 코사인 유사도 (1. 단어의 표현 예제 참고)\n",
    "def cosine_similarity(x, y):\n",
    "    # x와 y, 두 벡터의 코사인 유사도를 계산하는 함수\n",
    "    nominator = np.dot(x,y) # 분자\n",
    "    donominator = np.linalg.norm(x)*np.linalg.norm(y) # 분모\n",
    "    return nominator / donominator\n",
    "\n",
    "##############################################\n",
    "\n",
    "def sentences(text):\n",
    "#     print(text.split('.'))\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def connect(nodes):\n",
    "    return [(start, end, sentence_similarity(start, end)) for start in nodes for end in nodes if start is not end]\n",
    "\n",
    "def rank(nodes,edges):\n",
    "    graph=nx.diamond_graph()\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_weighted_edges_from(edges)\n",
    "    \n",
    "#     nx.draw(graph)\n",
    "#     plt.show()\n",
    "    \n",
    "    return nx.pagerank(graph)\n",
    "\n",
    "def summarize(text,num_summaries=6):\n",
    "    nodes=sentences(text)\n",
    "    edges=connect(nodes)\n",
    "    scores=rank(nodes,edges)\n",
    "    #print(nodes)\n",
    "    return sorted(scores,key=scores.get)[:num_summaries]\n",
    "\n",
    "summary = summarize(Text, 3)\n",
    "for sentence in summary:\n",
    "    print('='*15)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize(text, linesinSummary=10):\n",
    "#     text = sent_tokenize(text)\n",
    "#     weighted_edge = buildMatrix(text)\n",
    "#     score = scoring(weighted_edge)\n",
    "#     # print(score)\n",
    "    \n",
    "#     # -item[1] 은 오름차순 정렬\n",
    "#     # item[1] 은 내림차순 정렬\n",
    "#     rankedSentenceIndexes = [item[0] for item in sorted(enumerate(score), key=lambda item: -item[1])]\n",
    "#     selectedSentences = sorted(rankedSentenceIndexes[:linesinSummary])\n",
    "#     summary = itemgetter(*selectedSentences)(text)\n",
    "    \n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 핵심키워드 추출\n",
    " - 입력: 네이버 뉴스, url, 핵심 키워드 개수\n",
    " - 출력: matrix 혹은 그래프 활용 textrank 구현을 이용한 문서 요약\n",
    " - gensim 사용 가능, 허나 결과의 퀄리티를 높여볼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['생면식감', '포기하지', '한정판', '것으로', '온라인']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.summarization import keywords\n",
    "import re\n",
    "\n",
    "\n",
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=018&aid=0004435771'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# soup\n",
    "content = soup.find('div', id='articleBodyContents').text\n",
    "# 나중에 개행문자와 '오류를 우회하기 위한 함수 추가와 같은 것들 제거할 것'\n",
    "Text = content\n",
    "Text = Text.replace('\\n','')\n",
    "Text = re.sub(\"네이버 홈에서 [^\\t\\r\\n\\v\\f]*\",\"\",Text)\n",
    "Text = Text.replace('// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}','')\n",
    "\n",
    "def extractKeywordNaverNews(url, count):\n",
    "    pass\n",
    "\n",
    "gensim_keywords = keywords(Text, words=5).split('\\n')\n",
    "print(gensim_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['8월부터', '이마트', ',', '홈플러스', '등', '전국', '대형매장', '판매온라인', ',', '한정판매', '완판', '기록하며', '소비자', '인기', '입증포기하지', '마라탕면.', '(', '사진=풀무원', ')', '[', '이데일리', '이윤화', '기자', ']', '풀무원', '생면식감', '‘', '포기하지', '마라탕면', '’', '이', '오프라인을', '통해', '선보인다.풀무원식품은', '기름에', '튀기지', '않은', '비유탕', '건면으로', '만들어', '면의', '쫄깃함과', '국물의', '얼얼함이', '살아있는', '생면식감', '포기하지', '마라탕면을', '온라인에', '이어', '전국', '오프라인', '매장서', '판매한다고', '31일', '밝혔다.대형', '할인점', '이마트', ',', '홈플러스', ',', '롯데마트와', '코스트코까지', '입점이', '확정돼', '8월부터', '전국', '곳곳에서', '포기하지', '마라탕면을', '만나볼', '수', '있다.생면식감', '포기하지', '마라탕면은', '지난', '5일', '온라인', '쇼핑몰', '11번가에서', '한정판', '판매를', '시작한', '지', '100분', '만에', '1000세트', '(', '8000봉지', ')', '가', '완판됐다.', '이어', '추가로', '준비한', '2만', '봉지도', '4일', '만에', '조기', '소진됐다.이에', '풀무원은', '지난', '25일', '2차', '한정판', '앵콜', '판매를', '시작했는데', '이', '역시', '반응이', '좋았다.', '포기하지', '마라탕면', '8봉지와', '한화이글스', '마스코트', '‘', '수리', '’', '를', '새긴', '레트로컵으로', '구성한', '한정판', '2000세트', '(', '1만', '6000봉지', ')', '를', '판매한', '결과', '이', '또한', '9시간', '만에', '완판됐다.', '한정판', '세트를', '단독', '판매한', '11번가에서는', '판매', '시간', '동안', '실시간', '쇼핑', '검색어', '1위를', '달리며', '높은', '관심을', '입증했고', ',', '전체', '판매량', '3위에', '이름을', '올렸다.풀무원은', '앞으로', '11번가', '외에도', '다양한', '온라인', '채널을', '통해', '소비자를', '위한', '세트', '구성', '및', '프로모션을', '진행할', '계획이다.이기욱', '풀무원식품', '생면식감', '사업부', 'PM', '(', 'Product', 'Manager', ')', '은', '“', '’', '포기하지', '마라탕면', '’', '은', '두', '차례의', '온라인', '한정', '판매서', '단기간에', '완판되는', '저력을', '보여줬다.', '현재', '마라', '트렌드에', '더해', ',', '한화이글스와', '협업한', '패키지에', '대한', '팬들의', '만족도가', '높아', '가능했던', '것으로', '생각한다', '”', '며', '“', '이제는', '전국', '오프라인', '매장', '판매를', '통해', '더', '많은', '소비자에게', '생면식감', '마라탕면의', '쫄깃한', '면발과', '중독성', '있는', '국물', '맛이', '사랑받을', '것으로', '보인다', '”', '고', '말했다.한편', '포기하지', '마라탕면은', '기름에', '튀기지', '않은', '비유탕', '(', '非油湯', ')', '건면', '시장을', '선도하고', '있는', '풀무원이', '중국', '정통', '마라탕을', '라면으로', '만든', '제품이다.', '일반', '라면과는', '달리', '기름에', '튀기지', '않아', '정통', '마라탕처럼', '면의', '쫄깃함이', '잘', '살아있다.', '더불어', ',', '라면', '조리', '시', '기름으로', '인해', '맛이', '퍼지는', '현상이', '나타나지', '않아', '정통', '마라탕에', '가까운', '국물', '맛을', '구현해냈다.이윤화', '(', 'akfdl34', '@', 'edaily.co.kr', ')']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1. Tokenized\n",
    "text = TreebankWordTokenizer().tokenize(Text)\n",
    "\n",
    "print(\"Tokenized Text: \\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_tag: \n",
      "\n",
      "[('8월부터', 'CD'), ('이마트', 'NN'), (',', ','), ('홈플러스', 'NNP'), ('등', 'NNP'), ('전국', 'NNP'), ('대형매장', 'NNP'), ('판매온라인', 'NNP'), (',', ','), ('한정판매', 'NNP'), ('완판', 'NNP'), ('기록하며', 'NNP'), ('소비자', 'NNP'), ('인기', 'NNP'), ('입증포기하지', 'NNP'), ('마라탕면.', 'NNP'), ('(', '('), ('사진=풀무원', 'NNP'), (')', ')'), ('[', 'VBP'), ('이데일리', 'JJ'), ('이윤화', 'NNP'), ('기자', 'NNP'), (']', 'NNP'), ('풀무원', 'NNP'), ('생면식감', 'NNP'), ('‘', 'NNP'), ('포기하지', 'NNP'), ('마라탕면', 'NNP'), ('’', 'NNP'), ('이', 'NNP'), ('오프라인을', 'NNP'), ('통해', 'NNP'), ('선보인다.풀무원식품은', 'NNP'), ('기름에', 'NNP'), ('튀기지', 'NNP'), ('않은', 'NNP'), ('비유탕', 'NNP'), ('건면으로', 'NNP'), ('만들어', 'NNP'), ('면의', 'NNP'), ('쫄깃함과', 'NNP'), ('국물의', 'NNP'), ('얼얼함이', 'NNP'), ('살아있는', 'NNP'), ('생면식감', 'NNP'), ('포기하지', 'NNP'), ('마라탕면을', 'NNP'), ('온라인에', 'NNP'), ('이어', 'NNP'), ('전국', 'NNP'), ('오프라인', 'NNP'), ('매장서', 'NNP'), ('판매한다고', 'VBD'), ('31일', 'CD'), ('밝혔다.대형', 'NNP'), ('할인점', 'NNP'), ('이마트', 'NNP'), (',', ','), ('홈플러스', 'NNP'), (',', ','), ('롯데마트와', 'NNP'), ('코스트코까지', 'NNP'), ('입점이', 'NNP'), ('확정돼', 'VBD'), ('8월부터', 'CD'), ('전국', 'NNP'), ('곳곳에서', 'NNP'), ('포기하지', 'NNP'), ('마라탕면을', 'NNP'), ('만나볼', 'NNP'), ('수', 'NNP'), ('있다.생면식감', 'NNP'), ('포기하지', 'NNP'), ('마라탕면은', 'NNP'), ('지난', 'VBD'), ('5일', 'CD'), ('온라인', 'NNP'), ('쇼핑몰', 'VBD'), ('11번가에서', 'CD'), ('한정판', 'NNP'), ('판매를', 'NNP'), ('시작한', 'NNP'), ('지', 'VBD'), ('100분', 'CD'), ('만에', 'JJ'), ('1000세트', 'CD'), ('(', '('), ('8000봉지', 'CD'), (')', ')'), ('가', 'NN'), ('완판됐다.', 'NNP'), ('이어', 'NNP'), ('추가로', 'NNP'), ('준비한', 'VBD'), ('2만', 'CD'), ('봉지도', 'JJ'), ('4일', 'CD'), ('만에', 'JJ'), ('조기', 'NNP'), ('소진됐다.이에', 'NNP'), ('풀무원은', 'NNP'), ('지난', 'VBD'), ('25일', 'CD'), ('2차', 'CD'), ('한정판', 'NN'), ('앵콜', 'NNP'), ('판매를', 'NNP'), ('시작했는데', 'NNP'), ('이', 'NNP'), ('역시', 'NNP'), ('반응이', 'NNP'), ('좋았다.', 'NNP'), ('포기하지', 'NNP'), ('마라탕면', 'VBD'), ('8봉지와', 'CD'), ('한화이글스', 'NNP'), ('마스코트', 'NNP'), ('‘', 'NNP'), ('수리', 'NNP'), ('’', 'NNP'), ('를', 'NNP'), ('새긴', 'NNP'), ('레트로컵으로', 'NNP'), ('구성한', 'NNP'), ('한정판', 'VBZ'), ('2000세트', 'CD'), ('(', '('), ('1만', 'CD'), ('6000봉지', 'CD'), (')', ')'), ('를', 'NN'), ('판매한', 'NNP'), ('결과', 'NNP'), ('이', 'NNP'), ('또한', 'VBD'), ('9시간', 'CD'), ('만에', 'NNP'), ('완판됐다.', 'NNP'), ('한정판', 'NNP'), ('세트를', 'NNP'), ('단독', 'NNP'), ('판매한', 'VBD'), ('11번가에서는', 'CD'), ('판매', 'NNP'), ('시간', 'NNP'), ('동안', 'NNP'), ('실시간', 'NNP'), ('쇼핑', 'NNP'), ('검색어', 'VBD'), ('1위를', 'CD'), ('달리며', 'NNP'), ('높은', 'NNP'), ('관심을', 'NNP'), ('입증했고', 'NNP'), (',', ','), ('전체', 'NNP'), ('판매량', 'VBZ'), ('3위에', 'CD'), ('이름을', 'NN'), ('올렸다.풀무원은', 'NNP'), ('앞으로', 'VBZ'), ('11번가', 'CD'), ('외에도', 'NN'), ('다양한', 'NNP'), ('온라인', 'NNP'), ('채널을', 'NNP'), ('통해', 'NNP'), ('소비자를', 'NNP'), ('위한', 'NNP'), ('세트', 'NNP'), ('구성', 'NNP'), ('및', 'NNP'), ('프로모션을', 'NNP'), ('진행할', 'NNP'), ('계획이다.이기욱', 'NNP'), ('풀무원식품', 'NNP'), ('생면식감', 'NNP'), ('사업부', 'NNP'), ('PM', 'NNP'), ('(', '('), ('Product', 'NNP'), ('Manager', 'NNP'), (')', ')'), ('은', 'VBP'), ('“', 'JJ'), ('’', 'NNP'), ('포기하지', 'NNP'), ('마라탕면', 'NNP'), ('’', 'NNP'), ('은', 'NNP'), ('두', 'NNP'), ('차례의', 'NNP'), ('온라인', 'NNP'), ('한정', 'NNP'), ('판매서', 'NNP'), ('단기간에', 'NNP'), ('완판되는', 'NNP'), ('저력을', 'NNP'), ('보여줬다.', 'NNP'), ('현재', 'NNP'), ('마라', 'NNP'), ('트렌드에', 'NNP'), ('더해', 'NNP'), (',', ','), ('한화이글스와', 'NNP'), ('협업한', 'NNP'), ('패키지에', 'NNP'), ('대한', 'NNP'), ('팬들의', 'NNP'), ('만족도가', 'NNP'), ('높아', 'NNP'), ('가능했던', 'NNP'), ('것으로', 'NNP'), ('생각한다', 'NNP'), ('”', 'NNP'), ('며', 'NNP'), ('“', 'NNP'), ('이제는', 'NNP'), ('전국', 'NNP'), ('오프라인', 'NNP'), ('매장', 'NNP'), ('판매를', 'NNP'), ('통해', 'NNP'), ('더', 'NNP'), ('많은', 'NNP'), ('소비자에게', 'NNP'), ('생면식감', 'NNP'), ('마라탕면의', 'NNP'), ('쫄깃한', 'NNP'), ('면발과', 'NNP'), ('중독성', 'NNP'), ('있는', 'NNP'), ('국물', 'NNP'), ('맛이', 'NNP'), ('사랑받을', 'NNP'), ('것으로', 'NNP'), ('보인다', 'NNP'), ('”', 'NNP'), ('고', 'NNP'), ('말했다.한편', 'NNP'), ('포기하지', 'NNP'), ('마라탕면은', 'NNP'), ('기름에', 'NNP'), ('튀기지', 'NNP'), ('않은', 'NNP'), ('비유탕', 'NNP'), ('(', '('), ('非油湯', 'NNP'), (')', ')'), ('건면', 'VBP'), ('시장을', 'JJ'), ('선도하고', 'NNP'), ('있는', 'NNP'), ('풀무원이', 'NNP'), ('중국', 'NNP'), ('정통', 'NNP'), ('마라탕을', 'NNP'), ('라면으로', 'NNP'), ('만든', 'NNP'), ('제품이다.', 'NNP'), ('일반', 'NNP'), ('라면과는', 'NNP'), ('달리', 'NNP'), ('기름에', 'NNP'), ('튀기지', 'NNP'), ('않아', 'NNP'), ('정통', 'NNP'), ('마라탕처럼', 'NNP'), ('면의', 'NNP'), ('쫄깃함이', 'NNP'), ('잘', 'NNP'), ('살아있다.', 'NNP'), ('더불어', 'NNP'), (',', ','), ('라면', 'NNP'), ('조리', 'NNP'), ('시', 'NNP'), ('기름으로', 'NNP'), ('인해', 'NNP'), ('맛이', 'NNP'), ('퍼지는', 'NNP'), ('현상이', 'NNP'), ('나타나지', 'NNP'), ('않아', 'NNP'), ('정통', 'NNP'), ('마라탕에', 'NNP'), ('가까운', 'NNP'), ('국물', 'NNP'), ('맛을', 'NNP'), ('구현해냈다.이윤화', 'NNP'), ('(', '('), ('akfdl34', 'JJ'), ('@', 'NNP'), ('edaily.co.kr', 'NN'), (')', ')')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# 2. POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "print(\"POS_tag: \\n\")\n",
    "print(POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8월부터', '이마트', ',', '홈플러스', '등', '전국', '대형매장', '판매온라인', ',', '한정판매', '완판', '기록하며', '소비자', '인기', '입증포기하지', '마라탕면.', '(', '사진=풀무원', ')', '[', '이데일리', '이윤화', '기자', ']', '풀무원', '생면식감', '‘', '포기하지', '마라탕면', '’', '이', '오프라인을', '통해', '선보인다.풀무원식품은', '기름에', '튀기지', '않은', '비유탕', '건면으로', '만들어', '면의', '쫄깃함과', '국물의', '얼얼함이', '살아있는', '생면식감', '포기하지', '마라탕면을', '온라인에', '이어', '전국', '오프라인', '매장서', '판매한다고', '31일', '밝혔다.대형', '할인점', '이마트', ',', '홈플러스', ',', '롯데마트와', '코스트코까지', '입점이', '확정돼', '8월부터', '전국', '곳곳에서', '포기하지', '마라탕면을', '만나볼', '수', '있다.생면식감', '포기하지', '마라탕면은', '지난', '5일', '온라인', '쇼핑몰', '11번가에서', '한정판', '판매를', '시작한', '지', '100분', '만에', '1000세트', '(', '8000봉지', ')', '가', '완판됐다.', '이어', '추가로', '준비한', '2만', '봉지도', '4일', '만에', '조기', '소진됐다.이에', '풀무원은', '지난', '25일', '2차', '한정판', '앵콜', '판매를', '시작했는데', '이', '역시', '반응이', '좋았다.', '포기하지', '마라탕면', '8봉지와', '한화이글스', '마스코트', '‘', '수리', '’', '를', '새긴', '레트로컵으로', '구성한', '한정판', '2000세트', '(', '1만', '6000봉지', ')', '를', '판매한', '결과', '이', '또한', '9시간', '만에', '완판됐다.', '한정판', '세트를', '단독', '판매한', '11번가에서는', '판매', '시간', '동안', '실시간', '쇼핑', '검색어', '1위를', '달리며', '높은', '관심을', '입증했고', ',', '전체', '판매량', '3위에', '이름을', '올렸다.풀무원은', '앞으로', '11번가', '외에도', '다양한', '온라인', '채널을', '통해', '소비자를', '위한', '세트', '구성', '및', '프로모션을', '진행할', '계획이다.이기욱', '풀무원식품', '생면식감', '사업부', 'PM', '(', 'Product', 'Manager', ')', '은', '“', '’', '포기하지', '마라탕면', '’', '은', '두', '차례의', '온라인', '한정', '판매서', '단기간에', '완판되는', '저력을', '보여줬다.', '현재', '마라', '트렌드에', '더해', ',', '한화이글스와', '협업한', '패키지에', '대한', '팬들의', '만족도가', '높아', '가능했던', '것으로', '생각한다', '”', '며', '“', '이제는', '전국', '오프라인', '매장', '판매를', '통해', '더', '많은', '소비자에게', '생면식감', '마라탕면의', '쫄깃한', '면발과', '중독성', '있는', '국물', '맛이', '사랑받을', '것으로', '보인다', '”', '고', '말했다.한편', '포기하지', '마라탕면은', '기름에', '튀기지', '않은', '비유탕', '(', '非油湯', ')', '건면', '시장을', '선도하고', '있는', '풀무원이', '중국', '정통', '마라탕을', '라면으로', '만든', '제품이다.', '일반', '라면과는', '달리', '기름에', '튀기지', '않아', '정통', '마라탕처럼', '면의', '쫄깃함이', '잘', '살아있다.', '더불어', ',', '라면', '조리', '시', '기름으로', '인해', '맛이', '퍼지는', '현상이', '나타나지', '않아', '정통', '마라탕에', '가까운', '국물', '맛을', '구현해냈다.이윤화', '(', 'akfdl34', '@', 'edaily.co.kr', ')']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_text = []\n",
    "# 각 토큰의 표제어 추출\n",
    "for word in POS_tag:\n",
    "    lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=get_wordnet_pos(word[1]))))\n",
    "    \n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이마트', '홈플러스', '등', '전국', '대형매장', '판매온라인', '한정판매', '완판', '기록하며', '소비자', '인기', '입증포기하지', '마라탕면.', '사진=풀무원', '이데일리', '이윤화', '기자', '풀무원', '생면식감', '‘', '포기하지', '’', '이', '오프라인을', '통해', '선보인다.풀무원식품은', '기름에', '튀기지', '않은', '비유탕', '건면으로', '만들어', '면의', '쫄깃함과', '국물의', '얼얼함이', '살아있는', '생면식감', '포기하지', '마라탕면을', '온라인에', '이어', '전국', '오프라인', '매장서', '밝혔다.대형', '할인점', '이마트', '홈플러스', '롯데마트와', '코스트코까지', '입점이', '전국', '곳곳에서', '포기하지', '마라탕면을', '만나볼', '수', '있다.생면식감', '포기하지', '마라탕면은', '온라인', '판매를', '시작한', '만에', '가', '완판됐다.', '이어', '추가로', '봉지도', '만에', '조기', '소진됐다.이에', '풀무원은', '앵콜', '판매를', '시작했는데', '이', '역시', '반응이', '좋았다.', '포기하지', '한화이글스', '마스코트', '‘', '수리', '’', '를', '새긴', '레트로컵으로', '구성한', '를', '결과', '이', '만에', '완판됐다.', '세트를', '단독', '판매', '시간', '동안', '실시간', '쇼핑', '달리며', '높은', '관심을', '입증했고', '전체', '이름을', '올렸다.풀무원은', '외에도', '다양한', '온라인', '채널을', '통해', '소비자를', '위한', '세트', '구성', '및', '프로모션을', '진행할', '계획이다.이기욱', '풀무원식품', '생면식감', '사업부', 'PM', 'Product', 'Manager', '“', '’', '포기하지', '’', '두', '차례의', '온라인', '한정', '판매서', '단기간에', '완판되는', '저력을', '보여줬다.', '현재', '마라', '트렌드에', '더해', '한화이글스와', '협업한', '패키지에', '대한', '팬들의', '만족도가', '높아', '가능했던', '것으로', '생각한다', '”', '며', '“', '이제는', '전국', '오프라인', '매장', '판매를', '통해', '더', '많은', '소비자에게', '생면식감', '마라탕면의', '쫄깃한', '면발과', '중독성', '있는', '국물', '맛이', '사랑받을', '것으로', '보인다', '”', '고', '말했다.한편', '포기하지', '마라탕면은', '기름에', '튀기지', '않은', '비유탕', '非油湯', '시장을', '선도하고', '있는', '풀무원이', '중국', '정통', '마라탕을', '라면으로', '만든', '제품이다.', '일반', '라면과는', '달리', '기름에', '튀기지', '않아', '정통', '마라탕처럼', '면의', '쫄깃함이', '잘', '살아있다.', '더불어', '라면', '조리', '시', '기름으로', '인해', '맛이', '퍼지는', '현상이', '나타나지', '않아', '정통', '마라탕에', '가까운', '국물', '맛을', '구현해냈다.이윤화', 'akfdl34', 'edaily.co.kr']\n"
     ]
    }
   ],
   "source": [
    "# 4. 불용어 처리 + 불필요한 품사 제거\n",
    "stopwords = [] # 불용어 배열\n",
    "\n",
    "# 추출 키워드 대상이 되는 품사 지정 (N: 명사, J: 형용사)\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS']\n",
    "\n",
    "# 추출 키워드 대상 품사가 아닌 토큰은 불용어로 등록\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "        \n",
    "# punctuation 을 불용어로 추가\n",
    "punctuations = list(str(string.punctuation))\n",
    "stopwords = stopwords + punctuations\n",
    "\n",
    "# 사용자 정의 토큰을 불용어로 추가\n",
    "stopwords_plus = ['t','isn']\n",
    "stopwords = stopwords + stopwords_plus\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords:\n",
    "        processed_text.append(word)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['수리', '실시간', '단기간에', '선도하고', '더', 'PM', '만족도가', '얼얼함이', '시작한', '완판되는', '이어', '외에도', '잘', 'Product', '프로모션을', '반응이', '및', '”', '를', '등', '말했다.한편', '중국', '구현해냈다.이윤화', '제품이다.', '대형매장', '마라탕을', '더불어', '시장을', '기자', '계획이다.이기욱', '선보인다.풀무원식품은', '조기', '저력을', '소비자에게', '풀무원식품', '가', '이윤화', '정통', '구성한', '오프라인을', '한정', '통해', '수', '국물', '기름으로', '차례의', '홈플러스', '소진됐다.이에', '매장서', '건면으로', '국물의', '이제는', '맛이', '며', '전체', '마라탕면은', '다양한', 'edaily.co.kr', '단독', '더해', '이', '많은', '마라탕에', '협업한', '매장', '레트로컵으로', '마스코트', '이름을', '할인점', '“', '사업부', '진행할', '면발과', '온라인에', '마라탕처럼', '전국', '밝혔다.대형', '한정판매', '’', '대한', '非油湯', '조리', '관심을', '만나볼', '한화이글스', '높은', '올렸다.풀무원은', '달리', '완판됐다.', '트렌드에', '결과', '맛을', '이마트', '입증했고', '소비자', '만들어', '쫄깃함과', '라면으로', '만든', '라면', '이데일리', '인기', '패키지에', '판매를', '않은', '사진=풀무원', '판매온라인', '새긴', '기름에', '가능했던', 'Manager', '기록하며', '온라인', '않아', '소비자를', '코스트코까지', '가까운', '시작했는데', '현재', 'akfdl34', '중독성', '시간', '인해', '입증포기하지', '한화이글스와', '있는', '포기하지', '시', '판매', '완판', '마라탕면의', '사랑받을', '채널을', '일반', '생각한다', '위한', '살아있는', '현상이', '앵콜', '쫄깃함이', '높아', '고', '동안', '보여줬다.', '비유탕', '살아있다.', '풀무원이', '퍼지는', '마라탕면을', '롯데마트와', '만에', '면의', '있다.생면식감', '세트', '세트를', '마라', '생면식감', '나타나지', '마라탕면.', '추가로', '역시', '봉지도', '좋았다.', '팬들의', '쫄깃한', '곳곳에서', '풀무원', '‘', '보인다', '라면과는', '두', '것으로', '풀무원은', '판매서', '쇼핑', '튀기지', '입점이', '오프라인', '달리며', '구성']\n"
     ]
    }
   ],
   "source": [
    "# Unique한 token 목록 생성\n",
    "vocabulary = list(set(processed_text))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 생성\n",
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "# 토큰별로 그래프 edge를 Matrix 형태로 생성\n",
    "weighted_edge = np.zeros((vocab_len, vocab_len),dtype=np.float32)\n",
    "\n",
    "# 각 토큰 노트 별로 점수계산을 위한 배열 생성\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "# coocurrence를 판단하기 위한 window 사이즈 설정\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "# ~20행까지 같은 단어는 skip 하는 코드 \n",
    "for i in range(0, vocab_len):\n",
    "    score[i] = 1\n",
    "    for j in range(0, vocab_len):\n",
    "        if j == i:\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # 마지막 사이즈에서 window size를 차감한 만큼만 for문이 돈다. \n",
    "            for window_start in range(0, (len(processed_text) - window_size)):\n",
    "                \n",
    "                window_end = window_start + window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                # 탐색하고 있는 두 단어가 윈도(window)에 동시 등장할 경우 edge로 연결한다.\n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j) \n",
    "                        # math.fabs -> 절대값을 취하는 코드 \n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드의 score 계산\n",
    "inout = np.zeros((vocab_len), dtype=np.float32)\n",
    "\n",
    "for i in range(0, vocab_len):\n",
    "    for j in range(0, vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d = 0.85 # 임의의 상수\n",
    "threshold = 0.0001 # convergence threshold\n",
    "# threshold가 뭔데?\n",
    "# 전의 계산된 score의 합과 현재 계산된 score의 합과의 차이 \n",
    "# 계산을 여러번 할수록 아주 조금씩 줄어들게 된다. \n",
    "\n",
    "for iter in range(0, MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0, vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0, vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "        \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: # convergence condition\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of 수리: 0.7103317\n",
      "Score of 실시간: 0.9730625\n",
      "Score of 단기간에: 0.9198625\n",
      "Score of 선도하고: 0.8194917\n",
      "Score of 더: 0.7668544\n",
      "Score of PM: 0.81875026\n",
      "Score of 만족도가: 0.93915427\n",
      "Score of 얼얼함이: 0.82716465\n",
      "Score of 시작한: 0.72532356\n",
      "Score of 완판되는: 0.94598675\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10): # vocab_len:\n",
    "    print('Score of '+vocabulary[i]+\": \"+str(score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "포기하지 : 4.1886163\n",
      "생면식감 : 2.7699673\n",
      "전국 : 2.6275177\n",
      "’ : 2.243547\n",
      "정통 : 2.112884\n",
      "온라인 : 2.0414855\n",
      "만에 : 2.009934\n",
      "통해 : 2.0073943\n",
      "판매를 : 1.9603643\n",
      "이 : 1.9152642\n"
     ]
    }
   ],
   "source": [
    "# 핵심 단어 추출\n",
    "sorted_index = np.flip(np.argsort(score,0))\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print('Keywords:\\n')\n",
    "\n",
    "for i in range(0, keywords_num):\n",
    "    print(str(vocabulary[sorted_index[i]]) + \" : \" + str(score[sorted_index[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
