{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잠재의미분석 (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 실습 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    def TopicModeling(self) :\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class LSA:\n",
    "    def __init__(self, doc_ls, topic_num):\n",
    "        pass\n",
    "    \n",
    "    # tdm matrix 생성\n",
    "    def TDM(self, doc_ls):\n",
    "        pass\n",
    "    \n",
    "    # tdm matrix 특이값 분해(SVD)\n",
    "    # U, s, Vt 로 분해\n",
    "    def SVD(self, tdm):\n",
    "        pass\n",
    "    \n",
    "     # 토픽별 주요 키워드 출력\n",
    "  def TopicModeling(self) :\n",
    "    pass\n",
    "  \n",
    "  # 단어 벡터 행렬 생성 dot(U,s)  \n",
    "  def TermVectorMatrix(self, u, s):\n",
    "    pass\n",
    "  \n",
    "  # 문서 벡터 행렬 생성 dot(s,Vt).T \n",
    "  def DocVectorMatrix(self, s, vt):\n",
    "    pass\n",
    "  \n",
    "  # 키워드를 입력했을 때 단어 벡터 반환\n",
    "  def GetTermVector(self, term):\n",
    "    pass\n",
    "  \n",
    "  # 문서를 입력했을 때 문서 벡터 반환\n",
    "  def GetDocVector(self, doc):\n",
    "    pass\n",
    "  \n",
    "  # 단어-문서 벡터 행렬 생성\n",
    "  def TermDocVectorMatrix(self, u, s, vt):\n",
    "    pass\n",
    "  \n",
    "  # 단어 벡터 행렬에서 단어 간 코사인 유사도 측정하여 행렬형태로 반환\n",
    "  def TermSimilarityMatrix(self, term_vec_matrix):\n",
    "    pass\n",
    "  \n",
    "  # 두개 단어를 입력했을 때 코사인 유사도 반환\n",
    "  def GetTermSimilarity(self, term1, term2):\n",
    "    pass\n",
    "  \n",
    "  # 문서 벡터 행렬에서 문서 간 코사인 유사도 측정하여 행렬형태로 반환\n",
    "  def DocSimilarityMartrix(self, doc_vec_matrix):\n",
    "    pass\n",
    "  \n",
    "  # 두개 문서를 입력했을 때 코사인 유사도 반환\n",
    "  def GetDocSimilarity(self, doc1, doc2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8f970187e7b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m          ]\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mlsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_ls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mlsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTopicModeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mlsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetTermSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'사과'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'바나나'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LSA' is not defined"
     ]
    }
   ],
   "source": [
    "doc_ls = ['바나나 사과 포도 포도',\n",
    "         '사과 포도',\n",
    "         '포도 바나나',\n",
    "         '짜장면 짬뽕 탕수욕',\n",
    "         '볶음밥 탕수욕',\n",
    "         '짜장면 짬뽕',\n",
    "         '라면 스시',\n",
    "         '스시',\n",
    "         '가츠동 스시 소바',\n",
    "         '된장찌개 김치찌개 김치',\n",
    "         '김치 된장',\n",
    "         '비빔밥 김치'\n",
    "         ]\n",
    "\n",
    "lsa = LSA(doc_ls, 3)\n",
    "lsa.TopicModeling()\n",
    "lsa.GetTermSimilarity('사과','바나나')\n",
    "lsa.GetTermSimilarity('사과','짜장면')\n",
    "lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n",
    "lsa.GetDocSimilarity('사과 포도', '라면 스시')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 실습 예제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import randomized_svd\n",
    "\n",
    "class LSA :\n",
    "    def __init__(self, doc_ls, topic_num):\n",
    "        self.doc_ls = doc_ls\n",
    "        self.topic_num = topic_num\n",
    "        self.term2idx, self.idx2term = self.toIdxDict(' '.join(doc_ls).split())\n",
    "        self.doc2idx, self.idx2doc = self.toIdxDict(doc_ls)\n",
    "\n",
    "        self.tdm = self.TDM(doc_ls)\n",
    "        self.U, self.s, self.VT = self.SVD(self.tdm)\n",
    "\n",
    "        #print(self.s)\n",
    "        #print(self.U[:,:1].round(2))\n",
    "\n",
    "        self.term_mat = self.TermVectorMatrix(self.U, self.s, topic_num)\n",
    "        self.doc_mat = self.DocVectorMatrix(self.s, self.VT, topic_num)\n",
    "        self.term_doc_mat = self.TermDocVectorMatrix(self.U, self.s, self.VT, topic_num)\n",
    "\n",
    "        #print(self.term2idx.keys())\n",
    "        #print(self.term_mat.round(2))\n",
    "\n",
    "        self.term_sim = self.TermSimilarityMatrix(self.term_mat)\n",
    "        self.doc_sim = self.DocSimilarityMartrix(self.doc_mat)\n",
    "\n",
    "    # 리스트내 값을 index로 변환하는 dict과 \n",
    "    # index를 리스트내 값으로 변환하는 dict\n",
    "    def toIdxDict(self, ls) :\n",
    "        any2idx = defaultdict(lambda : len(any2idx))\n",
    "        idx2any = defaultdict()\n",
    "\n",
    "        for item in ls:\n",
    "            any2idx[item]\n",
    "            idx2any[any2idx[item]] = item\n",
    "\n",
    "        return any2idx, idx2any\n",
    "\n",
    "    def TDM(self, doc_ls):\n",
    "        # 행(토큰크기), 열(문서갯수)로 TDM 생성\n",
    "        tdm = np.zeros([len(self.term2idx.keys()), len(doc_ls)])\n",
    "\n",
    "        for doc_idx, doc in enumerate(doc_ls) :\n",
    "            for term in doc.split() :\n",
    "            #등장한 단어를 dictionary에서 위치를 탐색하여 빈도수 세기\n",
    "                tdm[self.term2idx[term], doc_idx] += 1\n",
    "    \n",
    "        return tdm\n",
    "\n",
    "    # 특이값 분해\n",
    "    def SVD(self, tdm):\n",
    "        U, s, VT = randomized_svd(tdm, \n",
    "                          n_components=15,\n",
    "                          n_iter=5,\n",
    "                          random_state=None)\n",
    "\n",
    "        #U, s, VT = np.linalg.svd(tdm)\n",
    "        return U, s, VT\n",
    "\n",
    "    # 토픽별 주요 키워드 출력\n",
    "    def TopicModeling(self, count = 3) :\n",
    "        topic_num = self.topic_num\n",
    "\n",
    "        for i in range(topic_num) :\n",
    "            score = self.U[:,i:i+1].T\n",
    "            sorted_index = np.flip(np.argsort(-score),0)\n",
    "\n",
    "            a = []\n",
    "            for j in sorted_index[0,: count] :\n",
    "                a.append((self.idx2term[j], score[0,j].round(3)))\n",
    "\n",
    "        print(\"Topic {} - {}\".format(i+1,a ))\n",
    "\n",
    "    def vectorSimilarity(self, matrix) :\n",
    "        similarity = np.zeros([matrix.shape[1], matrix.shape[1]])\n",
    "\n",
    "        for i in range(matrix.shape[1]) :\n",
    "            for j in range(matrix.shape[1]) :\n",
    "                similarity[i,j] =  cosine_similarity(matrix[:,i].T, matrix[:,j].T)\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    # 키워드를 입력했을 때 단어 벡터 반환\n",
    "    def GetTermVector(self, term):\n",
    "        vec = self.term_mat[self.term2idx[term]:self.term2idx[term]+1,:]\n",
    "        print('{} = {}'.format(term, vec))\n",
    "        return vec\n",
    "\n",
    "    # 문서를 입력했을 때 문서 벡터 반환\n",
    "    def GetDocVector(self, doc):\n",
    "        vec = self.doc_mat.T[self.doc2idx[doc]:self.doc2idx[doc]+1,:]\n",
    "        print('{} = {}'.format(doc, vec))\n",
    "        return vec\n",
    "\n",
    "    def Compression(self, round_num=0) :\n",
    "        print(self.tdm)\n",
    "        print(self.term_doc_mat.round(round_num))\n",
    "\n",
    "    def TermVectorMatrix(self, u, s, topic_num):\n",
    "        term_mat = np.matrix(u[:, :topic_num])# * np.diag(s[:topic_num])\n",
    "        return term_mat\n",
    "\n",
    "    def DocVectorMatrix(self, s, vt, topic_num):\n",
    "        doc_mat = np.diag(s[:topic_num]) * np.matrix(vt[:topic_num,:])\n",
    "        return doc_mat\n",
    "\n",
    "    def TermDocVectorMatrix(self, u, s, vt, topic_num):\n",
    "        term_doc_mat = np.matrix(u[:, :topic_num]) * np.diag(s[:topic_num])  * np.matrix(vt[:topic_num,:])\n",
    "        return term_doc_mat\n",
    "\n",
    "    def TermSimilarityMatrix(self, termVectorMatrix):\n",
    "        return self.vectorSimilarity(termVectorMatrix.T)\n",
    "\n",
    "    def GetTermSimilarity(self, term1, term2):\n",
    "        sim = self.term_sim[self.term2idx[term1], self.term2idx[term2]]\n",
    "        print(\"({},{}) term similarity = {}\".format(term1, term2, sim))\n",
    "        return sim \n",
    "\n",
    "    def DocSimilarityMartrix(self,docVectorMatrix):    \n",
    "        return self.vectorSimilarity(docVectorMatrix) \n",
    "\n",
    "    def GetDocSimilarity(self, doc1, doc2):\n",
    "        sim = self.doc_sim[self.doc2idx[doc1], self.doc2idx[doc2]]\n",
    "        print(\"('{}','{}') doc similarity = {}\".format(doc1, doc2, sim))\n",
    "        return sim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 토픽 모델링 ==\n",
      "Topic 4 - [('스시', 0.552), ('김치', 0.371), ('가츠동', 0.277), ('소바', 0.277)]\n",
      "\n",
      "== 단어 벡터 ==\n",
      "사과 = [[ 0.34843127 -0.19370961  0.01592593  0.03744775]]\n",
      "짜장면 = [[ 0.48563449  0.58415588 -0.07468389 -0.18737521]]\n",
      "\n",
      "== 단어 유사도 ==\n",
      "(사과,바나나) term similarity = 1.0\n",
      "(사과,짜장면) term similarity = 0.15191335214807028\n",
      "(포도,짜장면) term similarity = 0.15191335214807003\n",
      "(사과,스시) term similarity = -0.04097825202732763\n",
      "\n",
      "== 문서 벡터 ==\n",
      "사과 포도 = [[ 1.04529381 -0.58112882  0.04777778  0.11234324]]\n",
      "짜장면 짬뽕 = [[ 0.61011838  0.93971158 -0.17760018 -0.53682795]]\n",
      "\n",
      "== 문서 유사도 ==\n",
      "('사과 포도','포도 바나나') doc similarity = 1.0\n",
      "('사과 포도','라면 스시') doc similarity = -0.038506882113500146\n",
      "\n",
      "== 토픽 차원수로 압축 ==\n",
      "[[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[ 1.  0.  0. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
      " [ 1.  0.  0. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
      " [ 2.  1.  1. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
      " [ 1.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.]\n",
      " [ 0. -0. -0.  1.  0.  1. -0.  0. -0. -0.  0. -0.]\n",
      " [ 0. -0. -0.  1.  0.  0. -0.  0. -0. -0.  0. -0.]\n",
      " [-0. -0. -0.  0.  0.  0. -0.  0. -0. -0.  0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.  0.  0.  0. -0. -0. -0.]\n",
      " [ 0. -0. -0.  0. -0.  0.  1.  1.  1. -0.  0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.  0.  0.  1. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.  0.  0.  1. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]\n",
      " [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]\n",
      " [ 0. -0. -0. -0. -0.  0. -0.  0. -0.  1.  1.  1.]\n",
      " [ 0. -0. -0.  0.  0.  0. -0.  0. -0.  0.  0.  0.]\n",
      " [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "doc_ls = ['바나나 사과 포도 포도 짜장면',\n",
    "         '사과 포도',\n",
    "         '포도 바나나',\n",
    "         '짜장면 짬뽕 탕수육',\n",
    "         '볶음밥 탕수육',\n",
    "         '짜장면 짬뽕',\n",
    "         '라면 스시',\n",
    "         '스시 짜장면',\n",
    "         '가츠동 스시 소바',\n",
    "         '된장찌개 김치찌개 김치',\n",
    "         '김치 된장 짜장면',\n",
    "         '비빔밥 김치'\n",
    "         ]\n",
    "\n",
    "lsa = LSA(doc_ls, 4)\n",
    "X = lsa.TDM(doc_ls)\n",
    "print('== 토픽 모델링 ==')\n",
    "lsa.TopicModeling(4)\n",
    "print('\\n== 단어 벡터 ==')\n",
    "lsa.GetTermVector('사과')\n",
    "lsa.GetTermVector('짜장면')\n",
    "print('\\n== 단어 유사도 ==')\n",
    "lsa.GetTermSimilarity('사과','바나나')\n",
    "lsa.GetTermSimilarity('사과','짜장면')\n",
    "lsa.GetTermSimilarity('포도','짜장면')\n",
    "lsa.GetTermSimilarity('사과','스시')\n",
    "print('\\n== 문서 벡터 ==')\n",
    "lsa.GetDocVector('사과 포도')\n",
    "lsa.GetDocVector('짜장면 짬뽕')\n",
    "print('\\n== 문서 유사도 ==')\n",
    "lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n",
    "lsa.GetDocSimilarity('사과 포도', '라면 스시')\n",
    "print('\\n== 토픽 차원수로 압축 ==')\n",
    "lsa.Compression(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 sklearn LSA 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ls = ['바나나 사과 포도 포도',\n",
    "         '사과 포도',\n",
    "         '포도 바나나',\n",
    "         '짜장면 짬뽕 탕수욕',\n",
    "         '볶음밥 탕수욕',\n",
    "         '짜장면 짬뽕',\n",
    "         '라면 스시',\n",
    "         '스시',\n",
    "         '가츠동 스시 소바',\n",
    "         '된장찌개 김치찌개 김치',\n",
    "         '김치 된장',\n",
    "         '비빔밥 김치'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.4418867148991048\n",
      "  (0, 9)\t0.4418867148991048\n",
      "  (0, 15)\t0.7806870451028085\n",
      "  (1, 9)\t0.7494645793884686\n",
      "  (1, 15)\t0.662044442799776\n",
      "  (2, 6)\t0.7494645793884686\n",
      "  (2, 15)\t0.662044442799776\n",
      "  (3, 12)\t0.5773502691896258\n",
      "  (3, 13)\t0.5773502691896258\n",
      "  (3, 14)\t0.5773502691896258\n",
      "  (4, 14)\t0.6515208683486948\n",
      "  (4, 7)\t0.7586307126040724\n",
      "  (5, 12)\t0.7071067811865476\n",
      "  (5, 13)\t0.7071067811865476\n",
      "  (6, 5)\t0.7966850663542188\n",
      "  (6, 11)\t0.6043946600096447\n",
      "  (7, 11)\t1.0\n",
      "  (8, 11)\t0.47271637301705055\n",
      "  (8, 0)\t0.6231128431839633\n",
      "  (8, 10)\t0.6231128431839633\n",
      "  (9, 4)\t0.6231128431839633\n",
      "  (9, 2)\t0.6231128431839633\n",
      "  (9, 1)\t0.47271637301705055\n",
      "  (10, 1)\t0.6043946600096447\n",
      "  (10, 3)\t0.7966850663542188\n",
      "  (11, 1)\t0.6043946600096447\n",
      "  (11, 8)\t0.7966850663542188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(doc_ls)\n",
    "print(X)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=4, algorithm='randomized', n_iter=100)\n",
    "svd_model.fit(X)\n",
    "\n",
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('포도', 0.78069), ('바나나', 0.44189), ('사과', 0.44189)]\n",
      "Topic 2: [('스시', 0.8864), ('라면', 0.33189), ('소바', 0.2282)]\n",
      "Topic 3: [('짬뽕', 0.6258), ('짜장면', 0.6258), ('탕수욕', 0.43614)]\n",
      "Topic 4: [('김치', 0.76421), ('된장', 0.37119), ('비빔밥', 0.37119)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=3):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 실습 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA :\n",
    "    def __init__(self, doc_ls, topic_num, alpha = 0.1, beta = 0.001):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = topic_num\n",
    "\n",
    "    def RandomlyAssignTopic(self, doc_ls):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def IterateAssignTopic(self) :\n",
    "        pass\n",
    "\n",
    "    # 토픽별 주요 키워드 출력\n",
    "    def TopicModeling(self) :\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 실습 예제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords  \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "class LDA :\n",
    "    def __init__(self, docs, topic_num, alpha = 0.1, beta = 0.001):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = topic_num\n",
    "        self.docs = docs\n",
    "        \n",
    "        \n",
    "    def RandomlyAssignTopic(self, docs) :\n",
    "        dic = defaultdict()\n",
    "        t2i = defaultdict(lambda : len(t2i))\n",
    "        i2t = defaultdict()\n",
    "        d = 0\n",
    "        w = 0\n",
    "    \n",
    "        wnl = WordNetLemmatizer()\n",
    "        stopword = stopwords.words('english')\n",
    "        stopword.append(',')\n",
    "\n",
    "        # 임의의 토픽을 할당\n",
    "        for tokens in [word_tokenize(doc) for doc in docs] :\n",
    "            for token in [wnl.lemmatize(token.lower()) for token in tokens \n",
    "                        if token not in stopword] :\n",
    "                            i2t[t2i[token]] = token\n",
    "                            dic[(d, t2i[token], w)] = random.randint(0,self.k-1)\n",
    "                            w += 1\n",
    "                            d += 1\n",
    "        \n",
    "        print(dic)\n",
    "        return dic, t2i, i2t\n",
    "\n",
    "    \n",
    "    def CountDocTopic(self, t2i) :\n",
    "        docs = np.zeros((self.k, len(self.docs)))\n",
    "        terms = np.zeros((self.k, len(t2i.keys())))\n",
    "\n",
    "\n",
    "        #문서별 토큰별 빈도수 계산\n",
    "        for (d, n, w) in self.term_topic.keys() :\n",
    "            topic = self.term_topic[(d, n, w)]\n",
    "            docs[topic, d] += 1 + self.alpha\n",
    "            terms[topic, n] += 1 + self.beta\n",
    "\n",
    "        #비어있는 값는 값에 alpha, beta 설정\n",
    "        docs = np.where(docs==0.0, self.alpha, docs) \n",
    "        terms = np.where(terms==0.0, self.beta, terms)\n",
    "\n",
    "        print(\"단어 토픽별 빈도\")\n",
    "        print(terms.round(2))\n",
    "        print(\"문서 토픽별 빈도\")\n",
    "        print(docs.round(1))\n",
    "\n",
    "        return docs, terms\n",
    "\n",
    "    \n",
    "    def IterateAssignTopic(self, docs, terms, i2t) :\n",
    "    #한개의 단어씩 주제 배정\n",
    "        prev = {}\n",
    "    \n",
    "        while prev != self.term_topic:\n",
    "            for (d, n, w) in self.term_topic.keys() :\n",
    "                topic = [0, 0]\n",
    "\n",
    "                docs[self.term_topic[(d, n, w)], d] -= (1 + self.alpha)\n",
    "                terms[self.term_topic[(d, n, w)], n] -= (1 + self.beta)\n",
    "\n",
    "                docs = np.where(docs==0.0, self.alpha, docs) \n",
    "                terms = np.where(terms==0.0, self.beta, terms)\n",
    "\n",
    "                print()\n",
    "                print(\"{}(d:{}, n:{}, w:{}) = topic:{}\".format(i2t[n], d, n, w, self.term_topic[(d, n, w)]))\n",
    "                print(\"문서 토픽별 빈도\")\n",
    "                print(docs.round(1))\n",
    "                print(\"단어 토픽별 빈도\")\n",
    "                print(terms.round(3))\n",
    "\n",
    "\n",
    "                prev = self.term_topic\n",
    "\n",
    "                for t in range(self.k) :\n",
    "                    p_t_d = docs[t, d]/docs[:,d:d+1].sum()\n",
    "                    p_w_t = terms[t, n]/terms[t:t+1,:].sum()\n",
    "                    prob = p_t_d * p_w_t\n",
    "\n",
    "                    if topic[1] < prob :\n",
    "                        topic = [t, prob]\n",
    "\n",
    "                print(\"topic {} 일 확률 = {}/{} * {}/{} = {} * {} = {}\".format(t\n",
    "                    ,docs[t, d].round(1) , docs[:,d:d+1].sum().round(1)\n",
    "                    ,terms[t, d].round(3) , terms[t:t+1,:].sum().round(3)\n",
    "                    , p_t_d.round(3), p_w_t.round(4), prob.round(4)))\n",
    "\n",
    "\n",
    "            if docs[topic[0], d] < 1 : docs[topic[0], d] = 0;\n",
    "            if terms[topic[0], n] < 1 : terms[topic[0], n] = 0;\n",
    "\n",
    "            #확률이 가장 큰 토픽을 할당  \n",
    "            self.term_topic[(d, n, w)] = topic[0]\n",
    "            docs[topic[0], d] += (1 + self.alpha)\n",
    "            terms[topic[0], n] += (1 + self.beta)\n",
    "\n",
    "            print(\"할당된 토픽:{}\".format(self.term_topic[(d, n, w)]))\n",
    "            print(\"=\"*50)\n",
    "\n",
    "        return terms\n",
    "\n",
    "  # 토픽별 주요 키워드 출력\n",
    "    def TopicModeling(self, count=3) :\n",
    "        self.term_topic, t2i, i2t = self.RandomlyAssignTopic(self.docs)\n",
    "        docs, terms = self.CountDocTopic(t2i)\n",
    "        terms = self.IterateAssignTopic(docs, terms, i2t)\n",
    "\n",
    "        score = terms / terms.sum(axis=1, keepdims=True)\n",
    "\n",
    "        for i in range(self.k) :\n",
    "            print(\"\\nTopic {}\".format(i+1))\n",
    "            sorted_index = np.flip(np.argsort(score[i]),0)[:count]\n",
    "            for j in sorted_index :\n",
    "              #pass\n",
    "              print(\"({}={})\".format(i2t[j], score[i,j].round(3)), end = ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {(0, 0, 0): 1, (1, 1, 1): 0, (2, 2, 2): 1, (3, 3, 3): 0, (4, 4, 4): 0, (5, 1, 5): 0, (6, 5, 6): 1, (7, 2, 7): 0, (8, 6, 8): 1, (9, 3, 9): 0, (10, 6, 10): 1, (11, 4, 11): 0, (12, 0, 12): 0, (13, 5, 13): 0, (14, 7, 14): 1, (15, 6, 15): 0, (16, 4, 16): 1})\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a219e2776212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_ls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTopicModeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-6a4dfc43ff00>\u001b[0m in \u001b[0;36mTopicModeling\u001b[1;34m(self, count)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mTopicModeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterm_topic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomlyAssignTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCountDocTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterateAssignTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-6a4dfc43ff00>\u001b[0m in \u001b[0;36mCountDocTopic\u001b[1;34m(self, t2i)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterm_topic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mtopic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterm_topic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mterms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "doc_ls = [\"Cute kitty\",\n",
    "\"Eat rice or cake\",\n",
    "\"Kitty and hamster\",\n",
    "\"Eat bread\",\n",
    "\"Rice, bread and cake\",\n",
    "\"Cute hamster eats bread and cake\"]\n",
    "\n",
    "lda = LDA(doc_ls, 2)\n",
    "lda.TopicModeling(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
