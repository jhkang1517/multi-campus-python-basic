{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 유사도 측정\n",
    " - 클래스 초기화: 문장 리스트\n",
    " - 출력1: 벡터별 TD-IDF 출력\n",
    " - 출력2: 벡터간 코사인 유사도 계산 및 출력 (벡터 목록 중 2개만 추출하여 계산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors:  {'ruled': [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0], 'india': [0.6438410362258904, 0.0, 0.2575364144903562, 0.18395458177882582]}\n",
      "Vectors:  {'chalukyas': [0.0, 0.7954314537066303, 0.0, 0.0], 'ruled': [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0], 'badami': [0.0, 0.7954314537066303, 0.0, 0.0]}\n",
      "['ruled india', 'Chalukyas ruled Badami', 'So many kingdoms ruled India', 'Lalbagh is a botanical garden in India']\n",
      "ruled -> [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0]\n",
      "india -> [0.6438410362258904, 0.0, 0.2575364144903562, 0.18395458177882582]\n",
      "\tsimilarity of document 0 with others\n",
      "[[1.         0.29088811 0.46216171 0.19409143]]\n",
      "\tsimilarity of document 1 with others\n",
      "[[0.29088811 1.         0.13443735 0.        ]]\n",
      "\tsimilarity of document 2 with others\n",
      "[[0.46216171 0.13443735 1.         0.08970163]]\n",
      "\tsimilarity of document 3 with others\n",
      "[[0.19409143 0.         0.08970163 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class TextSimilarityExample:\n",
    "    # __init__ ->\n",
    "    def __init__(self):\n",
    "        self.statements = [\n",
    "            'ruled india',\n",
    "            'Chalukyas ruled Badami',\n",
    "            'So many kingdoms ruled India',\n",
    "            'Lalbagh is a botanical garden in India'\n",
    "        ]\n",
    "        \n",
    "# 배운 걸로 재 구현 해보자\n",
    "    def TF(self, sentence):\n",
    "        # .lower를 사용해서 전부 소문자로 변형 후 token화\n",
    "        # 근데 왜 소문자로 만들어야 하나요?\n",
    "        words = nltk.word_tokenize(sentence.lower()) \n",
    "\n",
    "        # 단어 빈도수 count\n",
    "        count_dict = {} # 숫자를 셀 빈 dictionary를 만든다. \n",
    "        for word in words: # words 리스트 안에 word를 하나씩 꺼내고\n",
    "            count_dict.setdefault(word, 0) # 기본 값은 0으로 둔다. \n",
    "            count_dict[word] += 1 # 같은 이름을 가진 word가 등장하면 1씩 += 해준다. \n",
    "        \n",
    "        # TF 구하기\n",
    "        dictionary = {} # TF를 구하는 새로운 dictionary\n",
    "        for key in count_dict.keys(): # 딕셔너리의 key값, 즉, 단어들을 하나씩 가져온다.\n",
    "            norm = count_dict[key]/float(len(words)) # val 값을 전체 단어 수로 나누어준다. 빈도수 측정 공식\n",
    "            dictionary[key] = norm # 값을 빈도값으로 재정의 한다.\n",
    "        return dictionary # 딕셔너리를 반환한다.\n",
    "\n",
    "    def IDF(self):\n",
    "        # 먼저 idf를 구할 공식을 def idf 로 정의한다.\n",
    "        def idf(TotalNumberOfDocuments, NumberOfDocumentsWithThisWord):\n",
    "            return 1.0 + math.log(TotalNumberOfDocuments/NumberOfDocumentsWithThisWord)\n",
    "        \n",
    "        numDocuments = len(self.statements) # 전체 문장의 개수 (이걸 왜 Documents라고 하지?)\n",
    "        uniqueWords = {}\n",
    "        idfValues = {}\n",
    "        \n",
    "        for sentence in self.statements: # 문장을 문장리스트에서 하나씩 가져온다.\n",
    "            for word in nltk.word_tokenize(sentence.lower()): # token화 시킨다.\n",
    "                if word not in uniqueWords: # 만약 단어가 빈 딕셔너리에 없으면, \n",
    "                    uniqueWords[word] = 1 # 그 단어를 추가하고 1값을 준다.\n",
    "                else:\n",
    "                    uniqueWords[word] += 1 # 만약 단어가 있으면, += 1을 해준다.\n",
    "                    \n",
    "        for word in uniqueWords: # 다시 단어를 꺼내온다.\n",
    "            idfValues[word] = idf(numDocuments, uniqueWords[word]) # 공식을 통해 idf 값을 구한다.\n",
    "        return idfValues\n",
    "\n",
    "    # TF-IDF 구하는 공식\n",
    "    def TF_IDF(self, query): \n",
    "        words = nltk.word_tokenize(query.lower()) # 먼저 단어들을 토큰화\n",
    "        idf = self.IDF() # idf는 위에서 정의한 IDF를 그대로 활용한다.\n",
    "        vectors = {} # vector 값을 받을 빈 딕셔너리 생성\n",
    "        for sentence in self.statements: # 문장리스트에서 문장을 꺼내온다.\n",
    "            tf = self.TF(sentence) # TF 역시 위에서 정의한 TF를 그대로 활용한다. \n",
    "            \n",
    "            for word in words: # 단어를 하나씩 꺼내온다.\n",
    "                tfv = tf[word] if word in tf else 0.0 # tfv의 값은 단어가 tf에 있을 때만 받고 ,그 외는 0\n",
    "                idfv = idf[word] if word in idf else 0.0 # idfv의 값은 단어가 idf에 있을 때만 받고, 그 외는 0\n",
    "                mul = tfv * idfv # 곱해준다. # 이거 왜 곱하는거야?!?!\n",
    "                \n",
    "                if word not in vectors: # 그 다음, 단어가 vector에 없으면 (빈 딕셔너리기 때문에 당연히 없음)\n",
    "                    vectors[word] = [] # 벡터의 단어를 key로 하는 key와 빈 리스트 val 값을 만들고,\n",
    "                vectors[word].append(mul) # 거기에 mul 값을 추가한다.\n",
    "        print(\"Vectors: \", vectors)\n",
    "        return vectors # 벡터 완성\n",
    "\n",
    "    def displayVectors(self, vectors):\n",
    "        print(self.statements) # 문장들 확인\n",
    "        for word in vectors: # 만약 단어가 벡터안에 있으면, \n",
    "            print(\"{} -> {}\".format(word, vectors[word])) # 단어는 -> 이 벡터로 변했다.\n",
    "\n",
    "####################################### 여기까지 이해 끝냄 #######################################            \n",
    "    \n",
    "# 아나\n",
    "    def cosineSimilarity(self):\n",
    "        vec = TfidfVectorizer() # vec는 TF-IDF Vectorizer를 활용\n",
    "        matrix = vec.fit_transform(self.statements) # 문장을 바로 박아서 matrix를 생성한다.\n",
    "\n",
    "        # 1이상 5미만의 for문을 돌린다. \n",
    "        for j in range(1, 5):\n",
    "            i = j - 1\n",
    "            # j = 1, 2, 3, 4\n",
    "            # i = 0, 1, 2, 3\n",
    "            print(\"\\tsimilarity of document {} with others\".format(i)) # 문장 위치(인덱스)를 나타내는 문장 출력\n",
    "            similarity = cosine_similarity(matrix[i:j], matrix) # 왜 0과 1의 유사도지? 0과 0이여야지 않나?\n",
    "            print(similarity)\n",
    "\n",
    "    def demo(self):\n",
    "        inputQuery = self.statements[0]\n",
    "        vectors = self.TF_IDF(inputQuery)\n",
    "        \n",
    "        inputQuery2 = self.statements[1]\n",
    "        vectors2 = self.TF_IDF(inputQuery2)\n",
    "        \n",
    "        self.displayVectors(vectors)\n",
    "        self.cosineSimilarity()\n",
    "\n",
    "similarity = TextSimilarityExample()\n",
    "similarity.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'djieljfaaaejf'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"djieljfAAAEJF\"\n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'사과': 1, '바나나': 1, '딸기': 1})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['사과','바나나','딸기']\n",
    "a = nltk.FreqDist(words)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict.keys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {'ruled': [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0], 'india': [0.6438410362258904, 0.0, 0.2575364144903562, 0.18395458177882582]}\n",
    "\n",
    "# 실패한 코드 \n",
    "# 코사인 유사도 구하기\n",
    "def cosineSimilarity(self):\n",
    "    temp_list = []\n",
    "    # x와 y, 두 벡터의 코사인 유사도를 계산하는 함수\n",
    "\n",
    "    # for문돌려서 벡터 두개 꺼내오고,\n",
    "    for word in vectors:\n",
    "        # print(vectors[word])\n",
    "        temp_list.append(vectors[word])\n",
    "\n",
    "    print(temp_list)\n",
    "    # x, y로 수정한다음에\n",
    "    x = temp_list[0]\n",
    "    y = temp_list[1]\n",
    "    \n",
    "    nominator = np.dot(x,y) # 분자\n",
    "    donominator = np.linalg.norm(x) * np.linalg.norm(y) # 분모\n",
    "    similarity = nominator / donominator\n",
    "    print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 뉴스 요약\n",
    " - 입력: 네이버 뉴스, url, 요약 비율\n",
    " - 출력: matrix 혹은 그래프 활용 textrank 구현 이용한 문서 요약\n",
    " \n",
    "출력: 요약문(요약비율 적용), 원문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def summarizeNaverNews(url, ratio):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "8월부터 이마트, 홈플러스 등 전국 대형매장 판매온라인, 한정판매 완판 기록하며 소비자 인기 입증포기하지 마라탕면.\n",
      "===============\n",
      "한정판 세트를 단독 판매한 11번가에서는 판매 시간 동안 실시간 쇼핑 검색어 1위를 달리며 높은 관심을 입증했고, 전체 판매량 3위에 이름을 올렸다.풀무원은 앞으로 11번가 외에도 다양한 온라인 채널을 통해 소비자를 위한 세트 구성 및 프로모션을 진행할 계획이다.이기욱 풀무원식품 생면식감 사업부 PM(Product Manager)은 “’포기하지 마라탕면’은 두 차례의 온라인 한정 판매서 단기간에 완판되는 저력을 보여줬다.\n",
      "===============\n",
      "더불어, 라면 조리 시 기름으로 인해 맛이 퍼지는 현상이 나타나지 않아 정통 마라탕에 가까운 국물 맛을 구현해냈다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.summarization.summarizer import summarize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import networkx as nx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=018&aid=0004435771'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# soup\n",
    "content = soup.find('div', id='articleBodyContents').text\n",
    "# 나중에 개행문자와 '오류를 우회하기 위한 함수 추가와 같은 것들 제거할 것'\n",
    "Text = content\n",
    "Text = Text.strip()\n",
    "Text = Text.lstrip()\n",
    "Text = Text.replace('\\n','')\n",
    "Text = Text.replace('// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}','')\n",
    "Text = Text.replace(\"이윤화 (akfdl34@edaily.co.kr)네이버 홈에서 ‘이데일리’ 뉴스 [구독하기▶]꿀잼가득 [영상보기▶] , 청춘뉘우스~ [스냅타임▶]＜ⓒ종합 경제정보 미디어 이데일리 - 무단전재 & 재배포 금지＞\",'')\n",
    "# print(Text)\n",
    "\n",
    "\n",
    "# 문장간 유사도 측정 (BOW를 활용한 코사인 유사도 측정)\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    # 각 문장을 소문자로 변환\n",
    "    sentence1 = [word.lower() for word in sentence1.split()]\n",
    "    sentence2 = [word.lower() for word in sentence2.split()]\n",
    "    \n",
    "    # BOW 생성을 위한 unique한 단어로 배열 생성\n",
    "    words_ls = list(set(sentence1 + sentence2))\n",
    "    \n",
    "    bow1 = [0] * len(words_ls)\n",
    "    bow2 = [0] * len(words_ls)\n",
    "    \n",
    "    # 첫번째 문장 BoW 생성\n",
    "    for word in sentence1:\n",
    "        bow1[words_ls.index(word)] += 1\n",
    "        \n",
    "    # 두번째 문장 BoW 생성\n",
    "    for word in sentence2:\n",
    "        bow2[words_ls.index(word)] += 1\n",
    "        \n",
    "    return cosine_similarity(bow1, bow2)\n",
    "\n",
    "# 코사인 유사도 (1. 단어의 표현 예제 참고)\n",
    "def cosine_similarity(x, y):\n",
    "    # x와 y, 두 벡터의 코사인 유사도를 계산하는 함수\n",
    "    nominator = np.dot(x,y) # 분자\n",
    "    donominator = np.linalg.norm(x)*np.linalg.norm(y) # 분모\n",
    "    return nominator / donominator\n",
    "\n",
    "##############################################\n",
    "\n",
    "def sentences(text):\n",
    "#     print(text.split('.'))\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def connect(nodes):\n",
    "    return [(start, end, sentence_similarity(start, end)) for start in nodes for end in nodes if start is not end]\n",
    "\n",
    "def rank(nodes,edges):\n",
    "    graph=nx.diamond_graph()\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_weighted_edges_from(edges)\n",
    "    \n",
    "#     nx.draw(graph)\n",
    "#     plt.show()\n",
    "    \n",
    "    return nx.pagerank(graph)\n",
    "\n",
    "def summarize(text,num_summaries=6):\n",
    "    nodes=sentences(text)\n",
    "    edges=connect(nodes)\n",
    "    scores=rank(nodes,edges)\n",
    "    #print(nodes)\n",
    "    return sorted(scores,key=scores.get)[:num_summaries]\n",
    "\n",
    "summary = summarize(Text, 3)\n",
    "for sentence in summary:\n",
    "    print('='*15)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize(text, linesinSummary=10):\n",
    "#     text = sent_tokenize(text)\n",
    "#     weighted_edge = buildMatrix(text)\n",
    "#     score = scoring(weighted_edge)\n",
    "#     # print(score)\n",
    "    \n",
    "#     # -item[1] 은 오름차순 정렬\n",
    "#     # item[1] 은 내림차순 정렬\n",
    "#     rankedSentenceIndexes = [item[0] for item in sorted(enumerate(score), key=lambda item: -item[1])]\n",
    "#     selectedSentences = sorted(rankedSentenceIndexes[:linesinSummary])\n",
    "#     summary = itemgetter(*selectedSentences)(text)\n",
    "    \n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
