{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 핵심 키워드 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text rank (누가 나의 텍스트를 많이 참조하는가?)\n",
    " * TextRank: Bringing Order into Texts (논문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 노드가 연결된 노드에게 균등하게 score를 준다.  \n",
    "자신의 노드가 n개에 연결되어 있으면, 1/n의 만큼 균등하게 score를 준다.  \n",
    "## 1. TF-IDF\n",
    "#### sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"The cat sat on my face. I hate a cat.\"\n",
    "d2 = \"The dog sat on my bed. I love a dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn 활용 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "document_ls = [d1, d2]\n",
    "# print(document_ls)\n",
    "# > ['The cat sat on my face. I hate a cat.', 'The dog sat on my bed. I love a dog']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(document_ls)\n",
    "# print(tfidf)\n",
    "\n",
    "word2id = defaultdict(lambda: 0)\n",
    "# print(word2id)\n",
    "# > {}\n",
    "\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "#     print(word2id[feature])\n",
    "#     0, 1, 2, 3, 4, 5... 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataframe 으로 변환하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>face</th>\n",
       "      <th>hate</th>\n",
       "      <th>love</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bed       cat       dog      face      hate      love        my  \\\n",
       "0  0.000000  0.706006  0.000000  0.353003  0.353003  0.000000  0.251164   \n",
       "1  0.353003  0.000000  0.706006  0.000000  0.000000  0.353003  0.251164   \n",
       "\n",
       "         on       sat       the  \n",
       "0  0.251164  0.251164  0.251164  \n",
       "1  0.251164  0.251164  0.251164  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "count_vect_df = pd.DataFrame(tfidf.todense(), columns=vectorizer.get_feature_names())\n",
    "count_vect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.70600557, 0.        , 0.35300279, 0.35300279,\n",
       "         0.        , 0.25116439, 0.25116439, 0.25116439, 0.25116439],\n",
       "        [0.35300279, 0.        , 0.70600557, 0.        , 0.        ,\n",
       "         0.35300279, 0.25116439, 0.25116439, 0.25116439, 0.25116439]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF score가 높은 순으로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'hate', 'face'], dtype='<U4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "feature_array = np.array(vectorizer.get_feature_names())\n",
    "tfidf_sorting = np.argsort(tfidf[0].toarray()).flatten()[::-1] # 정렬하는 코드\n",
    "\n",
    "n = 3\n",
    "top_n = feature_array[tfidf_sorting][:n] # 3개를 보여준다. (0, 1, 2)\n",
    "\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.businessinsider.sg/faang-stocks-are-a-dead-trade-for-the-next-several-months-2018-3/amp/\n",
    "\n",
    "Text = \"The FAANG stocks won’t see much more growth in the near future, according to Bill Studebaker, founder and Chief Investment Officer of Robo Global. \\\n",
    "Studebaker argues we are seeing a 'reallocation' that will continue from large-cap tech stocks into market-weight stocks. \\\n",
    "The FAANG stocks have had a rough few weeks, and have been hit hard since March 12. \\\n",
    "One FAANG to look out for, in the midst of all this, is Amazon, according to Studebaker. \\\n",
    "The stock market is seeing a 'reallocation' out of FAANG stocks, which are not where the smart money is, founder and Chief Investment Officer of Robo Global Bill Studebaker told Business Insider. \\\n",
    "The FAANG stocks (Facebook, Apple, Amazon, Netflix, Google) are all down considerably since March 12, a trend that accelerated when news of a massive Facebook data scandal broke, sending the tech-heavy Nasdaq into a downward frenzy. \\\n",
    "Investors are wondering what’s next. \\\n",
    "And what’s next isn’t good news for FAANG stock optimists, Studebaker thinks. 'This is a dead trade' for the next several months, he said. 'I wouldn’t expect there to be a lot of performance attribution coming from the FAANG stocks,' he added. That is, if the stock market is to see gains in the next several months, they will largely not come from the big tech companies. \\\n",
    "The market is seeing a 'reallocation out of large-cap technology, into other parts of the market,' he said. And this trend could continue for the foreseeable future. 'When you get these reallocation trades, a de-risking, this can go on for months and months.' The FAANG’s are pricey stocks, he said, pointing out that investors will 'factor in the law of big numbers,' he said. 'Just because they’re big cap doesn’t mean they’re safe,' he added. \\\n",
    "Still, he doesn’t necessarily think that investors are going to shift drastically into value stocks. 'With an increasingly favorable macro backdrop, you have strong growth demand.' \\\n",
    "Studebaker, who runs an artificial intelligence and robotics exchange-traded fund with $4 billion in assets under management, thinks that AI and robotics are better areas of growth. His ETF is up 27% in the past year, while the FAANG stocks are also largely up over that same span, even if they are down since March 12. \\\n",
    "While many point to artificial intelligence as an area that will be a boost to Google and Amazon, Studebaker doesn’t see that as a sign of significant growth for the FAANGs. He pointed out that 'eighty to ninety percent of their businesses are still search,' and that 'AI doesn’t really move the needle on the business.' He also said 'the revenue mix [attributable to AI] in those businesses are insignificant.' \\\n",
    "And while he’s not bullish on FAANG’s, he does say that the one FAANG to still watch out for is Amazon, simply because ecommerce still represents a small portion of the global retail market, giving the company room to grow.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 토큰화 (Tokenization)\n",
    " 분석 텍스트 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['The', 'FAANG', 'stocks', 'won', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'according', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global.', 'Studebaker', 'argues', 'we', 'are', 'seeing', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stocks', 'into', 'market-weight', 'stocks.', 'The', 'FAANG', 'stocks', 'have', 'had', 'a', 'rough', 'few', 'weeks', ',', 'and', 'have', 'been', 'hit', 'hard', 'since', 'March', '12.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'is', 'Amazon', ',', 'according', 'to', 'Studebaker.', 'The', 'stock', 'market', 'is', 'seeing', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stocks', ',', 'which', 'are', 'not', 'where', 'the', 'smart', 'money', 'is', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'told', 'Business', 'Insider.', 'The', 'FAANG', 'stocks', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'are', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerated', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'broke', ',', 'sending', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy.', 'Investors', 'are', 'wondering', 'what', '’', 's', 'next.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimists', ',', 'Studebaker', 'thinks.', \"'This\", 'is', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'months', ',', 'he', 'said.', \"'\", 'I', 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'coming', 'from', 'the', 'FAANG', 'stocks', ',', \"'\", 'he', 'added.', 'That', 'is', ',', 'if', 'the', 'stock', 'market', 'is', 'to', 'see', 'gains', 'in', 'the', 'next', 'several', 'months', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'companies.', 'The', 'market', 'is', 'seeing', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'parts', 'of', 'the', 'market', ',', \"'\", 'he', 'said.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trades', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'months', 'and', 'months.', \"'\", 'The', 'FAANG', '’', 's', 'are', 'pricey', 'stocks', ',', 'he', 'said', ',', 'pointing', 'out', 'that', 'investors', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'numbers', ',', \"'\", 'he', 'said.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investors', 'are', 'going', 'to', 'shift', 'drastically', 'into', 'value', 'stocks.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand.', \"'\", 'Studebaker', ',', 'who', 'runs', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'assets', 'under', 'management', ',', 'thinks', 'that', 'AI', 'and', 'robotics', 'are', 'better', 'areas', 'of', 'growth.', 'His', 'ETF', 'is', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stocks', 'are', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'are', 'down', 'since', 'March', '12.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'as', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'as', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs.', 'He', 'pointed', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'businesses', 'are', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business.', \"'\", 'He', 'also', 'said', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'businesses', 'are', 'insignificant.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'does', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'is', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represents', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'giving', 'the', 'company', 'room', 'to', 'grow', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = TreebankWordTokenizer().tokenize(Text)\n",
    "\n",
    "print(\"Tokenized Text: \\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 품사부착 (POS Tagging)\n",
    " 토큰화된 텍스트에 품사 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('won', 'VBD'), ('’', 'JJ'), ('t', 'NN'), ('see', 'VBP'), ('much', 'RB'), ('more', 'JJR'), ('growth', 'NN'), ('in', 'IN'), ('the', 'DT'), ('near', 'JJ'), ('future', 'NN'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Bill', 'NNP'), ('Studebaker', 'NNP'), (',', ','), ('founder', 'NN'), ('and', 'CC'), ('Chief', 'NNP'), ('Investment', 'NNP'), ('Officer', 'NNP'), ('of', 'IN'), ('Robo', 'NNP'), ('Global.', 'NNP'), ('Studebaker', 'NNP'), ('argues', 'VBZ'), ('we', 'PRP'), ('are', 'VBP'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), (\"'\", 'POS'), ('that', 'WDT'), ('will', 'MD'), ('continue', 'VB'), ('from', 'IN'), ('large-cap', 'JJ'), ('tech', 'NN'), ('stocks', 'NNS'), ('into', 'IN'), ('market-weight', 'JJ'), ('stocks.', 'NN'), ('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('have', 'VBP'), ('had', 'VBD'), ('a', 'DT'), ('rough', 'JJ'), ('few', 'JJ'), ('weeks', 'NNS'), (',', ','), ('and', 'CC'), ('have', 'VBP'), ('been', 'VBN'), ('hit', 'VBN'), ('hard', 'JJ'), ('since', 'IN'), ('March', 'NNP'), ('12.', 'CD'), ('One', 'NNP'), ('FAANG', 'NNP'), ('to', 'TO'), ('look', 'VB'), ('out', 'RP'), ('for', 'IN'), (',', ','), ('in', 'IN'), ('the', 'DT'), ('midst', 'NN'), ('of', 'IN'), ('all', 'PDT'), ('this', 'DT'), (',', ','), ('is', 'VBZ'), ('Amazon', 'NNP'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Studebaker.', 'NNP'), ('The', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), (\"'\", 'POS'), ('out', 'IN'), ('of', 'IN'), ('FAANG', 'NNP'), ('stocks', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('not', 'RB'), ('where', 'WRB'), ('the', 'DT'), ('smart', 'JJ'), ('money', 'NN'), ('is', 'VBZ'), (',', ','), ('founder', 'NN'), ('and', 'CC'), ('Chief', 'NNP'), ('Investment', 'NNP'), ('Officer', 'NNP'), ('of', 'IN'), ('Robo', 'NNP'), ('Global', 'NNP'), ('Bill', 'NNP'), ('Studebaker', 'NNP'), ('told', 'VBD'), ('Business', 'NNP'), ('Insider.', 'NNP'), ('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('(', '('), ('Facebook', 'NNP'), (',', ','), ('Apple', 'NNP'), (',', ','), ('Amazon', 'NNP'), (',', ','), ('Netflix', 'NNP'), (',', ','), ('Google', 'NNP'), (')', ')'), ('are', 'VBP'), ('all', 'DT'), ('down', 'RB'), ('considerably', 'RB'), ('since', 'IN'), ('March', 'NNP'), ('12', 'CD'), (',', ','), ('a', 'DT'), ('trend', 'NN'), ('that', 'WDT'), ('accelerated', 'VBD'), ('when', 'WRB'), ('news', 'NN'), ('of', 'IN'), ('a', 'DT'), ('massive', 'JJ'), ('Facebook', 'NNP'), ('data', 'NN'), ('scandal', 'NN'), ('broke', 'VBD'), (',', ','), ('sending', 'VBG'), ('the', 'DT'), ('tech-heavy', 'JJ'), ('Nasdaq', 'NNP'), ('into', 'IN'), ('a', 'DT'), ('downward', 'JJ'), ('frenzy.', 'NN'), ('Investors', 'NNS'), ('are', 'VBP'), ('wondering', 'VBG'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('next.', 'DT'), ('And', 'CC'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('next', 'JJ'), ('isn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('good', 'JJ'), ('news', 'NN'), ('for', 'IN'), ('FAANG', 'NNP'), ('stock', 'NN'), ('optimists', 'NNS'), (',', ','), ('Studebaker', 'NNP'), ('thinks.', 'NN'), (\"'This\", 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('dead', 'JJ'), ('trade', 'NN'), (\"'\", \"''\"), ('for', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('he', 'PRP'), ('said.', 'VBD'), (\"'\", \"''\"), ('I', 'PRP'), ('wouldn', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('expect', 'VBP'), ('there', 'EX'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('performance', 'NN'), ('attribution', 'NN'), ('coming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('added.', 'VBZ'), ('That', 'DT'), ('is', 'VBZ'), (',', ','), ('if', 'IN'), ('the', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('see', 'VB'), ('gains', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('they', 'PRP'), ('will', 'MD'), ('largely', 'RB'), ('not', 'RB'), ('come', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('big', 'JJ'), ('tech', 'NN'), ('companies.', 'VBZ'), ('The', 'DT'), ('market', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), ('out', 'IN'), ('of', 'IN'), ('large-cap', 'JJ'), ('technology', 'NN'), (',', ','), ('into', 'IN'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('market', 'NN'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('said.', 'VBZ'), ('And', 'CC'), ('this', 'DT'), ('trend', 'NN'), ('could', 'MD'), ('continue', 'VB'), ('for', 'IN'), ('the', 'DT'), ('foreseeable', 'JJ'), ('future.', 'NN'), (\"'When\", 'POS'), ('you', 'PRP'), ('get', 'VBP'), ('these', 'DT'), ('reallocation', 'NN'), ('trades', 'NNS'), (',', ','), ('a', 'DT'), ('de-risking', 'NN'), (',', ','), ('this', 'DT'), ('can', 'MD'), ('go', 'VB'), ('on', 'IN'), ('for', 'IN'), ('months', 'NNS'), ('and', 'CC'), ('months.', 'NN'), (\"'\", \"''\"), ('The', 'DT'), ('FAANG', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('are', 'VBP'), ('pricey', 'JJ'), ('stocks', 'NNS'), (',', ','), ('he', 'PRP'), ('said', 'VBD'), (',', ','), ('pointing', 'VBG'), ('out', 'RP'), ('that', 'IN'), ('investors', 'NNS'), ('will', 'MD'), (\"'factor\", 'VB'), ('in', 'IN'), ('the', 'DT'), ('law', 'NN'), ('of', 'IN'), ('big', 'JJ'), ('numbers', 'NNS'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('said.', 'VBZ'), (\"'Just\", 'CC'), ('because', 'IN'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('big', 'JJ'), ('cap', 'NN'), ('doesn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('mean', 'NN'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('safe', 'JJ'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('added.', 'VBZ'), ('Still', 'RB'), (',', ','), ('he', 'PRP'), ('doesn', 'VBZ'), ('’', 'JJ'), ('t', 'NNS'), ('necessarily', 'RB'), ('think', 'VBP'), ('that', 'IN'), ('investors', 'NNS'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('shift', 'VB'), ('drastically', 'RB'), ('into', 'IN'), ('value', 'NN'), ('stocks.', 'NN'), (\"'With\", 'CD'), ('an', 'DT'), ('increasingly', 'RB'), ('favorable', 'JJ'), ('macro', 'NN'), ('backdrop', 'NN'), (',', ','), ('you', 'PRP'), ('have', 'VBP'), ('strong', 'JJ'), ('growth', 'NN'), ('demand.', 'NN'), (\"'\", \"''\"), ('Studebaker', 'NNP'), (',', ','), ('who', 'WP'), ('runs', 'VBZ'), ('an', 'DT'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('robotics', 'NNS'), ('exchange-traded', 'JJ'), ('fund', 'NN'), ('with', 'IN'), ('$', '$'), ('4', 'CD'), ('billion', 'CD'), ('in', 'IN'), ('assets', 'NNS'), ('under', 'IN'), ('management', 'NN'), (',', ','), ('thinks', 'VBZ'), ('that', 'IN'), ('AI', 'NNP'), ('and', 'CC'), ('robotics', 'NNS'), ('are', 'VBP'), ('better', 'JJR'), ('areas', 'NNS'), ('of', 'IN'), ('growth.', 'NN'), ('His', 'PRP$'), ('ETF', 'NN'), ('is', 'VBZ'), ('up', 'RP'), ('27', 'CD'), ('%', 'NN'), ('in', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('year', 'NN'), (',', ','), ('while', 'IN'), ('the', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('are', 'VBP'), ('also', 'RB'), ('largely', 'RB'), ('up', 'IN'), ('over', 'IN'), ('that', 'DT'), ('same', 'JJ'), ('span', 'NN'), (',', ','), ('even', 'RB'), ('if', 'IN'), ('they', 'PRP'), ('are', 'VBP'), ('down', 'JJ'), ('since', 'IN'), ('March', 'NNP'), ('12.', 'CD'), ('While', 'IN'), ('many', 'JJ'), ('point', 'NN'), ('to', 'TO'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('as', 'IN'), ('an', 'DT'), ('area', 'NN'), ('that', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('a', 'DT'), ('boost', 'NN'), ('to', 'TO'), ('Google', 'NNP'), ('and', 'CC'), ('Amazon', 'NNP'), (',', ','), ('Studebaker', 'NNP'), ('doesn', 'VBZ'), ('’', 'JJ'), ('t', 'NNS'), ('see', 'VBP'), ('that', 'IN'), ('as', 'IN'), ('a', 'DT'), ('sign', 'NN'), ('of', 'IN'), ('significant', 'JJ'), ('growth', 'NN'), ('for', 'IN'), ('the', 'DT'), ('FAANGs.', 'NNP'), ('He', 'PRP'), ('pointed', 'VBD'), ('out', 'RP'), ('that', 'IN'), (\"'eighty\", 'VBZ'), ('to', 'TO'), ('ninety', 'VB'), ('percent', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('businesses', 'NNS'), ('are', 'VBP'), ('still', 'RB'), ('search', 'RB'), (',', ','), (\"'\", \"''\"), ('and', 'CC'), ('that', 'IN'), (\"'AI\", 'NNP'), ('doesn', 'VBD'), ('’', 'NNP'), ('t', 'NN'), ('really', 'RB'), ('move', 'VB'), ('the', 'DT'), ('needle', 'NN'), ('on', 'IN'), ('the', 'DT'), ('business.', 'NN'), (\"'\", 'POS'), ('He', 'PRP'), ('also', 'RB'), ('said', 'VBD'), (\"'the\", 'JJ'), ('revenue', 'NN'), ('mix', 'NN'), ('[', 'NNP'), ('attributable', 'NN'), ('to', 'TO'), ('AI', 'NNP'), (']', 'NNP'), ('in', 'IN'), ('those', 'DT'), ('businesses', 'NNS'), ('are', 'VBP'), ('insignificant.', 'JJ'), (\"'\", 'POS'), ('And', 'CC'), ('while', 'IN'), ('he', 'PRP'), ('’', 'VBZ'), ('s', 'PRP'), ('not', 'RB'), ('bullish', 'VB'), ('on', 'IN'), ('FAANG', 'NNP'), ('’', 'NNP'), ('s', 'NN'), (',', ','), ('he', 'PRP'), ('does', 'VBZ'), ('say', 'VB'), ('that', 'IN'), ('the', 'DT'), ('one', 'CD'), ('FAANG', 'NNP'), ('to', 'TO'), ('still', 'RB'), ('watch', 'VB'), ('out', 'RP'), ('for', 'IN'), ('is', 'VBZ'), ('Amazon', 'NNP'), (',', ','), ('simply', 'RB'), ('because', 'IN'), ('ecommerce', 'NN'), ('still', 'RB'), ('represents', 'VBZ'), ('a', 'DT'), ('small', 'JJ'), ('portion', 'NN'), ('of', 'IN'), ('the', 'DT'), ('global', 'JJ'), ('retail', 'JJ'), ('market', 'NN'), (',', ','), ('giving', 'VBG'), ('the', 'DT'), ('company', 'NN'), ('room', 'NN'), ('to', 'TO'), ('grow', 'VB'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "POS_tag = nltk.pos_tag(text) # 품사 부착\n",
    "\n",
    "print(POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 표제어 추출 (Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'FAANG', 'stock', 'won', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'according', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global.', 'Studebaker', 'argues', 'we', 'are', 'seeing', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stock', 'into', 'market-weight', 'stocks.', 'The', 'FAANG', 'stock', 'have', 'had', 'a', 'rough', 'few', 'weeks', ',', 'and', 'have', 'been', 'hit', 'hard', 'since', 'March', '12.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'is', 'Amazon', ',', 'according', 'to', 'Studebaker.', 'The', 'stock', 'market', 'is', 'seeing', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stock', ',', 'which', 'are', 'not', 'where', 'the', 'smart', 'money', 'is', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'told', 'Business', 'Insider.', 'The', 'FAANG', 'stock', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'are', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerated', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'broke', ',', 'sending', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy.', 'Investors', 'are', 'wondering', 'what', '’', 's', 'next.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimists', ',', 'Studebaker', 'thinks.', \"'This\", 'is', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'months', ',', 'he', 'said.', \"'\", 'I', 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'coming', 'from', 'the', 'FAANG', 'stock', ',', \"'\", 'he', 'added.', 'That', 'is', ',', 'if', 'the', 'stock', 'market', 'is', 'to', 'see', 'gain', 'in', 'the', 'next', 'several', 'months', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'companies.', 'The', 'market', 'is', 'seeing', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'part', 'of', 'the', 'market', ',', \"'\", 'he', 'said.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trade', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'months', 'and', 'months.', \"'\", 'The', 'FAANG', '’', 's', 'are', 'pricey', 'stock', ',', 'he', 'said', ',', 'pointing', 'out', 'that', 'investors', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'number', ',', \"'\", 'he', 'said.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investors', 'are', 'going', 'to', 'shift', 'drastically', 'into', 'value', 'stocks.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand.', \"'\", 'Studebaker', ',', 'who', 'run', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'assets', 'under', 'management', ',', 'think', 'that', 'AI', 'and', 'robotics', 'are', 'good', 'areas', 'of', 'growth.', 'His', 'ETF', 'is', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stock', 'are', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'are', 'down', 'since', 'March', '12.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'a', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'a', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs.', 'He', 'pointed', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'businesses', 'are', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business.', \"'\", 'He', 'also', 'said', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'businesses', 'are', 'insignificant.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'doe', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'is', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represents', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'giving', 'the', 'company', 'room', 'to', 'grow', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_text = []\n",
    "# 각 토큰의 표제어 추출\n",
    "for word in POS_tag:\n",
    "    lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=get_wordnet_pos(word[1]))))\n",
    "    \n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 불용어(Stopwords) 처리 및 불필요한 품사 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAANG', 'stock', 'more', 'growth', 'near', 'future', 'Bill', 'Studebaker', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global.', 'Studebaker', \"'reallocation\", 'large-cap', 'tech', 'stock', 'market-weight', 'stocks.', 'FAANG', 'stock', 'rough', 'few', 'weeks', 'hard', 'March', 'One', 'FAANG', 'midst', 'Amazon', 'Studebaker.', 'stock', 'market', \"'reallocation\", 'FAANG', 'stock', 'smart', 'money', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global', 'Bill', 'Studebaker', 'Business', 'Insider.', 'FAANG', 'stock', 'Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'March', 'trend', 'news', 'massive', 'Facebook', 'data', 'scandal', 'tech-heavy', 'Nasdaq', 'downward', 'frenzy.', 'Investors', 'next', 'good', 'news', 'FAANG', 'stock', 'optimists', 'Studebaker', 'thinks.', \"'This\", 'dead', 'trade', 'next', 'several', 'months', 'lot', 'performance', 'attribution', 'FAANG', 'stock', 'stock', 'market', 'gain', 'next', 'several', 'months', 'big', 'tech', 'market', \"'reallocation\", 'large-cap', 'technology', 'other', 'part', 'market', 'trend', 'foreseeable', 'future.', 'reallocation', 'trade', 'de-risking', 'months', 'months.', 'FAANG', 'pricey', 'stock', 'investors', 'law', 'big', 'number', 're', 'big', 'cap', 'mean', 're', 'safe', 'investors', 'value', 'stocks.', 'favorable', 'macro', 'backdrop', 'strong', 'growth', 'demand.', 'Studebaker', 'run', 'artificial', 'intelligence', 'robotics', 'exchange-traded', 'fund', 'assets', 'management', 'AI', 'robotics', 'good', 'areas', 'growth.', 'ETF', 'past', 'year', 'FAANG', 'stock', 'same', 'span', 'March', 'many', 'point', 'artificial', 'intelligence', 'area', 'boost', 'Google', 'Amazon', 'Studebaker', 'sign', 'significant', 'growth', 'FAANGs.', 'percent', 'businesses', \"'AI\", 'needle', 'business.', \"'the\", 'revenue', 'mix', 'attributable', 'AI', 'businesses', 'insignificant.', 'FAANG', 'doe', 'FAANG', 'Amazon', 'ecommerce', 'small', 'portion', 'global', 'retail', 'market', 'company', 'room']\n"
     ]
    }
   ],
   "source": [
    "stopwords = [] # 불용어 배열\n",
    "\n",
    "# 추출 키워드 대상이 되는 품사 지정 (N: 명사, J: 형용사)\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS']\n",
    "\n",
    "# 추출 키워드 대상 품사가 아닌 토큰은 불용어로 등록\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "        \n",
    "# punctuation 을 불용어로 추가\n",
    "punctuations = list(str(string.punctuation))\n",
    "stopwords = stopwords + punctuations\n",
    "\n",
    "# 사용자 정의 토큰을 불용어로 추가\n",
    "stopwords_plus = ['t','isn']\n",
    "stopwords = stopwords + stopwords_plus\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords:\n",
    "        processed_text.append(word)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique한 토큰 목록 생성\n",
    " 그래프 생성을 위해서 Unique한 토큰 목록 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Global', 'safe', 'backdrop', 'Chief', 'percent', 'technology', \"'AI\", 'businesses', 'cap', 'data', 'Insider.', 'stocks.', 'Netflix', 'foreseeable', 'area', 'next', 'frenzy.', 'doe', 're', 'rough', 'several', 'robotics', 'Business', \"'This\", 'areas', 'Studebaker', 'number', 'tech-heavy', 'Investors', 'law', 'demand.', 'optimists', 'Studebaker.', 'founder', 'large-cap', 'room', 'growth', 'Apple', 'lot', 'performance', 'artificial', 'stock', 'retail', 'other', 'trade', 'fund', 'year', 'intelligence', 'Facebook', 'revenue', 'de-risking', 'Investment', 'span', 'months', 'hard', 'massive', 'AI', 'sign', 'Robo', 'FAANGs.', 'boost', 'dead', 'business.', 'global', 'market-weight', 'smart', 'thinks.', 'run', 'ecommerce', 'Officer', 'weeks', 'gain', 'future.', 'Global.', 'mix', 'more', 'past', 'Google', 'pricey', 'point', 'market', 'value', 'future', 'tech', 'downward', 'Bill', \"'reallocation\", 'trend', 'Nasdaq', 'assets', 'midst', 'FAANG', 'favorable', 'management', 'March', 'many', 'ETF', 'strong', 'small', 'portion', 'exchange-traded', 'months.', 'attribution', 'good', 'Amazon', 'part', 'company', 'few', 'money', 'investors', 'news', 'big', 'same', 'reallocation', 'One', \"'the\", 'growth.', 'attributable', 'significant', 'needle', 'scandal', 'mean', 'insignificant.', 'macro', 'near']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(processed_text))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 그래프 생성 (weighted edge 계산)\n",
    " * TextRank는 그래프 기반 모델\n",
    " * 각 단어(토큰)은 그래프의 노드(vertex)\n",
    " * weighted_edge 행렬은 노드간 가중치 정보를 담고 있음\n",
    " * weighted_edge[i][j] 는 i번째 단어와 j번째 단어의 가중치를 의미\n",
    " * weighted_edge[i][j] 가 0인 경우는 노드간 연결이 없음을 의미\n",
    " * 모든 노드는 1로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "# 토큰별로 그래프 edge를 Matrix 형태로 생성\n",
    "weighted_edge = np.zeros((vocab_len, vocab_len),dtype=np.float32)\n",
    "\n",
    "# 각 토큰 노트 별로 점수계산을 위한 배열 생성\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "# coocurrence를 판단하기 위한 window 사이즈 설정\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "# ~20행까지 같은 단어는 skip 하는 코드 \n",
    "for i in range(0, vocab_len):\n",
    "    score[i] = 1\n",
    "    for j in range(0, vocab_len):\n",
    "        if j == i:\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # 마지막 사이즈에서 window size를 차감한 만큼만 for문이 돈다. \n",
    "            for window_start in range(0, (len(processed_text) - window_size)):\n",
    "                \n",
    "                window_end = window_start + window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                # 탐색하고 있는 두 단어가 윈도(window)에 동시 등장할 경우 edge로 연결한다.\n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j) \n",
    "                        # math.fabs -> 절대값을 취하는 코드 \n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 노드의 score 계산\n",
    "각 노드와 연결된 weighted edge의 값을 합산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((vocab_len), dtype=np.float32)\n",
    "\n",
    "for i in range(0, vocab_len):\n",
    "    for j in range(0, vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d = 0.85 # 임의의 상수\n",
    "threshold = 0.0001 # convergence threshold\n",
    "# threshold가 뭔데?\n",
    "# 전의 계산된 score의 합과 현재 계산된 score의 합과의 차이 \n",
    "# 계산을 여러번 할수록 아주 조금씩 줄어들게 된다. \n",
    "\n",
    "for iter in range(0, MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0, vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0, vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "        \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: # convergence condition\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of Global: 0.64329684\n",
      "Score of safe: 0.7101814\n",
      "Score of backdrop: 0.79021776\n",
      "Score of Chief: 1.1211275\n",
      "Score of percent: 0.7779519\n",
      "Score of technology: 0.682462\n",
      "Score of 'AI: 0.83814615\n",
      "Score of businesses: 1.4344289\n",
      "Score of cap: 0.70205617\n",
      "Score of data: 0.7879862\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10): # vocab_len:\n",
    "    print('Score of '+vocabulary[i]+\": \"+str(score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) 핵심 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "FAANG : 5.3772945\n",
      "stock : 5.2782774\n",
      "Studebaker : 3.369168\n",
      "market : 2.717907\n",
      "Amazon : 2.3066387\n",
      "growth : 1.9410361\n",
      "March : 1.8605113\n",
      "next : 1.8059615\n",
      "big : 1.7737058\n",
      "months : 1.7517571\n"
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(score,0))\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print('Keywords:\\n')\n",
    "\n",
    "for i in range(0, keywords_num):\n",
    "    print(str(vocabulary[sorted_index[i]]) + \" : \" + str(score[sorted_index[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 35, 106,  73,  64,  78,  32,  17,  31,  71,  65,  10,   0, 108,\n",
       "        90,  22, 101, 112,  75,  37,  29,  12, 102, 114,  26,   5,  57,\n",
       "        67,  52,  38,  82,  19, 105,  39, 122,  30,  43,  55,  50,   8,\n",
       "        60, 124,  46,  66,   1,  81, 118, 121,  14,  95,  79,  68,  13,\n",
       "        61,  23, 107,  54,  72, 113,  70,  59,  92,  42,  76,  97,  28,\n",
       "        24,  98,   4,  63, 123,   9, 100,   2, 116,  93,  99,  96,  45,\n",
       "        16,  89, 117, 120,   6,  84,  27,  74, 119,  88,  49,  62, 115,\n",
       "        83,  51,   3,  69,  34,  33,  58,  20, 110,  85,  77, 109,  11,\n",
       "        87,  48,  18, 103,  40,  44,  47,  21,   7,  56,  86,  53, 111,\n",
       "        15,  94,  36, 104,  80,  25,  41,  91], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_index\n",
    "np.argsort(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flip?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 TextRank 핵심 구 추출\n",
    "#### 1) 불용어를 기준으로 구 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock'], ['more', 'growth'], ['near', 'future'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['FAANG', 'stock'], ['rough', 'few', 'weeks'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['Studebaker.'], ['stock', 'market'], [\"'reallocation\"], ['FAANG', 'stock'], ['smart', 'money'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global', 'Bill', 'Studebaker'], ['Business', 'Insider.'], ['FAANG', 'stock'], ['Facebook'], ['Apple'], ['Amazon'], ['Netflix'], ['Google'], ['March'], ['trend'], ['news'], ['massive', 'Facebook', 'data', 'scandal'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['next'], ['good', 'news'], ['FAANG', 'stock', 'optimists'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'months'], ['lot'], ['performance', 'attribution'], ['FAANG', 'stock'], ['stock', 'market'], ['gain'], ['next', 'several', 'months'], ['big', 'tech'], ['market'], [\"'reallocation\"], ['large-cap', 'technology'], ['other', 'part'], ['market'], ['trend'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['months'], ['months.'], ['FAANG'], ['pricey', 'stock'], ['investors'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['investors'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['Studebaker'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['assets'], ['management'], ['AI'], ['robotics'], ['good', 'areas'], ['growth.'], ['ETF'], ['past', 'year'], ['FAANG', 'stock'], ['same', 'span'], ['March'], ['many', 'point'], ['artificial', 'intelligence'], ['area'], ['boost'], ['Google'], ['Amazon'], ['Studebaker'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['percent'], ['businesses'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['AI'], ['businesses'], ['insignificant.'], ['FAANG'], ['doe'], ['FAANG'], ['Amazon'], ['ecommerce'], ['small', 'portion'], ['global', 'retail', 'market'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    if word in stopwords:\n",
    "        if phrase != \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "            \n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords:\n",
    "        phrase += str(word)\n",
    "        phrase += \" \"\n",
    "        \n",
    "print(phrases)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock'], ['more', 'growth'], ['near', 'future'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['rough', 'few', 'weeks'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['Studebaker.'], ['stock', 'market'], ['smart', 'money'], ['Robo', 'Global', 'Bill', 'Studebaker'], ['Business', 'Insider.'], ['Facebook'], ['Apple'], ['Netflix'], ['Google'], ['trend'], ['news'], ['massive', 'Facebook', 'data', 'scandal'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['next'], ['good', 'news'], ['FAANG', 'stock', 'optimists'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'months'], ['lot'], ['performance', 'attribution'], ['gain'], ['big', 'tech'], ['market'], ['large-cap', 'technology'], ['other', 'part'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['months'], ['months.'], ['FAANG'], ['pricey', 'stock'], ['investors'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['Studebaker'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['assets'], ['management'], ['AI'], ['robotics'], ['good', 'areas'], ['growth.'], ['ETF'], ['past', 'year'], ['same', 'span'], ['many', 'point'], ['area'], ['boost'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['percent'], ['businesses'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['insignificant.'], ['doe'], ['ecommerce'], ['small', 'portion'], ['global', 'retail', 'market'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "unique_phrases = []\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "        \n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock'], ['more', 'growth'], ['near', 'future'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['rough', 'few', 'weeks'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['Studebaker.'], ['stock', 'market'], ['smart', 'money'], ['Robo', 'Global', 'Bill', 'Studebaker'], ['Business', 'Insider.'], ['Apple'], ['Netflix'], ['Google'], ['trend'], ['massive', 'Facebook', 'data', 'scandal'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['good', 'news'], ['FAANG', 'stock', 'optimists'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'months'], ['lot'], ['performance', 'attribution'], ['gain'], ['big', 'tech'], ['large-cap', 'technology'], ['other', 'part'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['months.'], ['pricey', 'stock'], ['investors'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['assets'], ['management'], ['AI'], ['good', 'areas'], ['growth.'], ['ETF'], ['past', 'year'], ['same', 'span'], ['many', 'point'], ['area'], ['boost'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['percent'], ['businesses'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['insignificant.'], ['doe'], ['ecommerce'], ['small', 'portion'], ['global', 'retail', 'market'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    # print(word)\n",
    "    \n",
    "    for phrase in unique_phrases:\n",
    "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase) > 1):\n",
    "            unique_phrases.remove([word])\n",
    "            \n",
    "print(unique_phrases)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 각 구의 Score 계산\n",
    " 앞서 산출한 각 단어별 점수를 합산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: 'FAANG stock', Score: 10.655571937561035\n",
      "Keyword: 'more growth', Score: 2.6003788113594055\n",
      "Keyword: 'near future', Score: 1.4000205397605896\n",
      "Keyword: 'Bill Studebaker', Score: 4.571102142333984\n",
      "Keyword: 'founder', Score: 1.14383864402771\n",
      "Keyword: 'Chief Investment Officer', Score: 3.3657681941986084\n",
      "Keyword: 'Robo Global. Studebaker', Score: 5.150090396404266\n",
      "Keyword: ''reallocation', Score: 1.5940415859222412\n",
      "Keyword: 'large-cap tech stock', Score: 7.522923707962036\n",
      "Keyword: 'market-weight stocks.', Score: 1.8791368007659912\n",
      "Keyword: 'rough few weeks', Score: 2.1873615384101868\n",
      "Keyword: 'hard', Score: 0.740159273147583\n",
      "Keyword: 'March', Score: 1.860511302947998\n",
      "Keyword: 'One FAANG', Score: 6.052494943141937\n",
      "Keyword: 'midst', Score: 0.651337742805481\n",
      "Keyword: 'Amazon', Score: 2.306638717651367\n",
      "Keyword: 'Studebaker.', Score: 0.6348740458488464\n",
      "Keyword: 'stock market', Score: 7.996184349060059\n",
      "Keyword: 'smart money', Score: 1.2878848910331726\n",
      "Keyword: 'Robo Global Bill Studebaker', Score: 6.367474794387817\n",
      "Keyword: 'Business Insider.', Score: 1.2936245799064636\n",
      "Keyword: 'Apple', Score: 0.6629024744033813\n",
      "Keyword: 'Netflix', Score: 0.669349193572998\n",
      "Keyword: 'Google', Score: 1.2295998334884644\n",
      "Keyword: 'trend', Score: 1.2516354322433472\n",
      "Keyword: 'massive Facebook data scandal', Score: 3.591155171394348\n",
      "Keyword: 'tech-heavy Nasdaq', Score: 1.7416781783103943\n",
      "Keyword: 'downward frenzy. Investors', Score: 2.450543999671936\n",
      "Keyword: 'good news', Score: 2.506568670272827\n",
      "Keyword: 'FAANG stock optimists', Score: 11.29088032245636\n",
      "Keyword: 'Studebaker thinks. 'This', Score: 4.80863744020462\n",
      "Keyword: 'dead trade', Score: 2.0441269874572754\n",
      "Keyword: 'next several months', Score: 4.745144605636597\n",
      "Keyword: 'lot', Score: 0.6917908787727356\n",
      "Keyword: 'performance attribution', Score: 1.3675029873847961\n",
      "Keyword: 'gain', Score: 0.6387736201286316\n",
      "Keyword: 'big tech', Score: 2.883545756340027\n",
      "Keyword: 'large-cap technology', Score: 1.8172683715820312\n",
      "Keyword: 'other part', Score: 1.3926620483398438\n",
      "Keyword: 'foreseeable future.', Score: 1.4762863516807556\n",
      "Keyword: 'reallocation trade', Score: 2.066248297691345\n",
      "Keyword: 'de-risking', Score: 0.7012116312980652\n",
      "Keyword: 'months.', Score: 0.6568183898925781\n",
      "Keyword: 'pricey stock', Score: 5.911310374736786\n",
      "Keyword: 'investors', Score: 1.243420958518982\n",
      "Keyword: 'law', Score: 0.6643946766853333\n",
      "Keyword: 'big number', Score: 2.451006770133972\n",
      "Keyword: 're big cap', Score: 3.7597849369049072\n",
      "Keyword: 'mean', Score: 0.7151865363121033\n",
      "Keyword: 're safe', Score: 1.994204342365265\n",
      "Keyword: 'value stocks.', Score: 1.9595847725868225\n",
      "Keyword: 'favorable macro backdrop', Score: 2.339900314807892\n",
      "Keyword: 'strong growth demand.', Score: 3.4072976112365723\n",
      "Keyword: 'run', Score: 0.6875182390213013\n",
      "Keyword: 'artificial intelligence', Score: 2.6378742456436157\n",
      "Keyword: 'robotics exchange-traded fund', Score: 2.988875150680542\n",
      "Keyword: 'assets', Score: 0.8225952386856079\n",
      "Keyword: 'management', Score: 0.8012027740478516\n",
      "Keyword: 'AI', Score: 1.4423471689224243\n",
      "Keyword: 'good areas', Score: 2.078368902206421\n",
      "Keyword: 'growth.', Score: 0.797764778137207\n",
      "Keyword: 'ETF', Score: 0.8033245801925659\n",
      "Keyword: 'past year', Score: 1.4759175181388855\n",
      "Keyword: 'same span', Score: 1.348086655139923\n",
      "Keyword: 'many point', Score: 1.4417730569839478\n",
      "Keyword: 'area', Score: 0.7174943685531616\n",
      "Keyword: 'boost', Score: 0.7034218907356262\n",
      "Keyword: 'sign', Score: 0.6847701668739319\n",
      "Keyword: 'significant growth', Score: 2.6550955772399902\n",
      "Keyword: 'FAANGs.', Score: 0.7565036416053772\n",
      "Keyword: 'percent', Score: 0.7779518961906433\n",
      "Keyword: 'businesses', Score: 1.4344289302825928\n",
      "Keyword: ''AI', Score: 0.8381461501121521\n",
      "Keyword: 'needle', Score: 0.8725197315216064\n",
      "Keyword: 'business.', Score: 0.9000740051269531\n",
      "Keyword: ''the revenue mix', Score: 2.6752268075942993\n",
      "Keyword: 'attributable', Score: 0.829797089099884\n",
      "Keyword: 'insignificant.', Score: 0.6977277398109436\n",
      "Keyword: 'doe', Score: 0.6352519392967224\n",
      "Keyword: 'ecommerce', Score: 0.7225422859191895\n",
      "Keyword: 'small portion', Score: 1.576014220714569\n",
      "Keyword: 'global retail market', Score: 4.263455092906952\n",
      "Keyword: 'company room', Score: 0.5674031972885132\n"
     ]
    }
   ],
   "source": [
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score = 0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += \" \"\n",
    "        phrase_score += score[vocabulary.index(word)]\n",
    "        \n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "    \n",
    "i = 0\n",
    "for keyword in keywords:\n",
    "    print(\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 각 구를 Score로 정렬하게 핵심 구 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "FAANG stock optimists, \n",
      "FAANG stock, \n",
      "stock market, \n",
      "large-cap tech stock, \n",
      "Robo Global Bill Studebaker, \n",
      "One FAANG, \n",
      "pricey stock, \n",
      "Robo Global. Studebaker, \n",
      "Studebaker thinks. 'This, \n",
      "next several months, \n"
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print(\"Keywords:\\n\")\n",
    "\n",
    "for i in range(0, keywords_num):\n",
    "    print(str(keywords[sorted_index[i]])+\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 gensim Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/bc/1415be59292a23ff123298b4b46ec4be80b3bfe72c8d188b58ab2653dee4/gensim-3.8.0.tar.gz (23.4MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Collecting smart_open>=1.7.0 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart_open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart_open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/e5/7ff9a5f23b3e94aefccc18f9d2493ddbeb31725986ab5ae4552387381e01/boto3-1.9.198-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (2019.3.9)\n",
      "Collecting botocore<1.13.0,>=1.12.198 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/2d/74e83c1cc3e8ab5c9675b2990af2f4c2a7b66379627deef4b08780fe6b23/botocore-1.12.198-py2.py3-none-any.whl (5.6MB)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docutils<0.15,>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.198->boto3->smart_open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.198->boto3->smart_open>=1.7.0->gensim) (2.8.0)\n",
      "Building wheels for collected packages: gensim, smart-open\n",
      "  Building wheel for gensim (setup.py): started\n",
      "  Building wheel for gensim (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\User\\AppData\\Local\\pip\\Cache\\wheels\\2c\\19\\c6\\bf38e867cb6e75999e3ff80302eb27bdf488b333efadfbfed7\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\User\\AppData\\Local\\pip\\Cache\\wheels\\5f\\ea\\fb\\5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "Successfully built gensim smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.198 botocore-1.12.198 gensim-3.8.0 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.8.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stocks',\n",
       " 'stock',\n",
       " 'studebaker',\n",
       " 'trade',\n",
       " 'trades',\n",
       " 'amazon',\n",
       " 'tech',\n",
       " 'attribution',\n",
       " 'attributable',\n",
       " 'cap',\n",
       " 'facebook',\n",
       " 'market',\n",
       " 'future',\n",
       " 'growth',\n",
       " 'thinks',\n",
       " 'think',\n",
       " 'frenzy',\n",
       " 'investment',\n",
       " 'big',\n",
       " 'global',\n",
       " 'favorable macro']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "# 원문만 넣으면 바로 중요한 keywords를 추출한다. \n",
    "\n",
    "keywords(Text).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
