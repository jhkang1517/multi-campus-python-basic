{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    ": explicit programming의 한계점 극복  \n",
    "  \n",
    "1. Supervised learning: 지도학습  \n",
    "  정답을 정해놓고, 그 정답을 얼마나 많이 맞췄는지에 대한 검증   \n",
    "  Ex) 이미지 분류  \n",
    "  일반적으로, 현재까지는 비지도학습에 비해 성능이 더 뛰어남  \n",
    "  \n",
    "1-1. Types of supervised learning\n",
    "  regression(선형회귀)\n",
    "  binary classification(이진분류): 특히 감성분석은 좋음, 나쁨의 분류로 구분되기 때문에 이진분류에 포함된다.\n",
    "  multi-label classification\n",
    "  \n",
    "2. Unsupervised learning: 비지도 학습\n",
    "  Ex) Topic Modeling, KNN\n",
    " \n",
    "## TensorFlow\n",
    ": Tensor: Data, Flow: 흐름 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello TensorFlow! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/86/095d2f7829badc207c893dd4ac767e871f6cd547145df797ea26baea4e2e/setuptools-41.2.0-py2.py3-none-any.whl\n",
      "Installing collected packages: setuptools\n",
      "Successfully installed setuptools-41.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.23.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.15.0,>=1.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from protobuf>=3.6.1->tensorflow) (41.2.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Create a constant op\n",
    "# This op is added as a node to the default graph # 기본적으로 그래프를 만들어 주어야 한다.\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "# seart a TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# run the op and get default\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 2.0, 3.0]], [[7.0, 8.0, 9.0]]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a rank 0 tensor; this is a scalar with shape []\n",
    "[1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "node3 = tf.add(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_1:0\", shape=(), dtype=float32) node2: Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
      "node3:  Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"node1:\", node1, \"node2:\", node2)\n",
    "print(\"node3: \", node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2):  [3.0, 4.0]\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"sess.run(node1, node2): \", sess.run([node1, node2]))\n",
    "print(\"sess.run(node3): \", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "# placeholder? \n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, feed_dict={a: 3, b:4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "참고 강의: https://class.coursera.org/ml-003/lecture  \n",
    "  \n",
    "* Cost function(Lost function)\n",
    "  무엇이 가장 비용이 적게 드는가? 로 이해 \n",
    "  제곱을 함으로써, +,- 의 부호를 제거하고 양수화시킨다. (그렇지 않으면 상쇄효과가 발생한다.)\n",
    "  \n",
    "따라서, 선형회귀의 목표는 Minimize cost가 된다. \n",
    ": 비용을 최소화 시키자! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.793031 [1.0005323] [-1.5025755]\n",
      "20 0.2308157 [1.4751785] [-1.2231513]\n",
      "40 0.18872394 [1.4980446] [-1.1457851]\n",
      "60 0.17121251 [1.4789422] [-1.0900441]\n",
      "80 0.15549624 [1.4568435] [-1.0386357]\n",
      "100 0.14122416 [1.4354124] [-0.9898066]\n",
      "120 0.12826204 [1.4149534] [-0.94328773]\n",
      "140 0.116489686 [1.3954525] [-0.89895654]\n",
      "160 0.10579773 [1.3768678] [-0.8567089]\n",
      "180 0.09608724 [1.3591563] [-0.8164467]\n",
      "200 0.08726799 [1.3422773] [-0.7780768]\n",
      "220 0.07925817 [1.3261914] [-0.7415101]\n",
      "240 0.071983546 [1.3108617] [-0.7066618]\n",
      "260 0.06537657 [1.2962523] [-0.6734511]\n",
      "280 0.059376072 [1.2823297] [-0.6418015]\n",
      "300 0.053926308 [1.2690612] [-0.61163926]\n",
      "320 0.04897671 [1.2564163] [-0.5828944]\n",
      "340 0.044481426 [1.2443656] [-0.55550045]\n",
      "360 0.040398736 [1.2328813] [-0.52939385]\n",
      "380 0.036690768 [1.2219366] [-0.5045143]\n",
      "400 0.033323143 [1.2115065] [-0.4808039]\n",
      "420 0.0302646 [1.2015663] [-0.45820785]\n",
      "440 0.027486796 [1.1920935] [-0.43667373]\n",
      "460 0.024963945 [1.1830659] [-0.41615164]\n",
      "480 0.02267263 [1.1744623] [-0.396594]\n",
      "500 0.020591663 [1.1662633] [-0.37795553]\n",
      "520 0.01870167 [1.1584494] [-0.3601929]\n",
      "540 0.016985163 [1.151003] [-0.3432652]\n",
      "560 0.0154262 [1.1439065] [-0.32713306]\n",
      "580 0.014010324 [1.1371434] [-0.31175905]\n",
      "600 0.0127244 [1.1306982] [-0.2971076]\n",
      "620 0.011556517 [1.1245558] [-0.28314465]\n",
      "640 0.010495796 [1.118702] [-0.2698378]\n",
      "660 0.009532464 [1.1131235] [-0.25715643]\n",
      "680 0.008657522 [1.1078072] [-0.24507101]\n",
      "700 0.007862915 [1.1027406] [-0.2335536]\n",
      "720 0.0071412153 [1.0979122] [-0.22257744]\n",
      "740 0.0064857663 [1.0933107] [-0.21211714]\n",
      "760 0.005890478 [1.0889254] [-0.2021484]\n",
      "780 0.005349832 [1.0847464] [-0.19264814]\n",
      "800 0.004858795 [1.0807635] [-0.18359447]\n",
      "820 0.004412836 [1.0769678] [-0.17496611]\n",
      "840 0.004007817 [1.0733507] [-0.16674331]\n",
      "860 0.0036399607 [1.0699035] [-0.15890701]\n",
      "880 0.003305867 [1.0666183] [-0.151439]\n",
      "900 0.0030024408 [1.0634875] [-0.14432196]\n",
      "920 0.002726869 [1.0605037] [-0.13753937]\n",
      "940 0.0024765814 [1.0576603] [-0.1310755]\n",
      "960 0.002249277 [1.0549505] [-0.12491544]\n",
      "980 0.0020428228 [1.052368] [-0.11904488]\n",
      "1000 0.0018553291 [1.049907] [-0.11345024]\n",
      "1020 0.0016850427 [1.0475616] [-0.10811853]\n",
      "1040 0.0015303809 [1.0453264] [-0.10303738]\n",
      "1060 0.0013899215 [1.0431962] [-0.09819504]\n",
      "1080 0.001262352 [1.0411664] [-0.0935803]\n",
      "1100 0.0011464846 [1.0392314] [-0.08918248]\n",
      "1120 0.0010412548 [1.0373877] [-0.0849912]\n",
      "1140 0.0009456861 [1.0356307] [-0.08099687]\n",
      "1160 0.0008588876 [1.0339563] [-0.07719035]\n",
      "1180 0.0007800581 [1.0323604] [-0.07356273]\n",
      "1200 0.00070846104 [1.0308397] [-0.07010558]\n",
      "1220 0.00064343814 [1.0293902] [-0.0668109]\n",
      "1240 0.00058437657 [1.0280089] [-0.063671]\n",
      "1260 0.0005307416 [1.0266926] [-0.0606787]\n",
      "1280 0.00048202672 [1.0254382] [-0.05782703]\n",
      "1300 0.0004377854 [1.0242428] [-0.05510939]\n",
      "1320 0.00039760442 [1.0231035] [-0.05251949]\n",
      "1340 0.0003611139 [1.0220178] [-0.05005134]\n",
      "1360 0.00032796958 [1.0209832] [-0.0476992]\n",
      "1380 0.00029787104 [1.0199972] [-0.04545776]\n",
      "1400 0.00027052796 [1.019057] [-0.0433214]\n",
      "1420 0.00024569783 [1.0181614] [-0.04128538]\n",
      "1440 0.00022314582 [1.0173079] [-0.03934509]\n",
      "1460 0.00020266445 [1.0164946] [-0.03749599]\n",
      "1480 0.00018406427 [1.0157193] [-0.03573382]\n",
      "1500 0.00016717067 [1.0149806] [-0.03405447]\n",
      "1520 0.00015182694 [1.0142765] [-0.03245401]\n",
      "1540 0.0001378923 [1.0136057] [-0.03092882]\n",
      "1560 0.00012523566 [1.0129663] [-0.02947533]\n",
      "1580 0.0001137403 [1.0123569] [-0.02809008]\n",
      "1600 0.00010329986 [1.0117761] [-0.02676995]\n",
      "1620 9.382068e-05 [1.0112227] [-0.02551188]\n",
      "1640 8.520755e-05 [1.0106953] [-0.02431292]\n",
      "1660 7.7388504e-05 [1.0101926] [-0.02317028]\n",
      "1680 7.02833e-05 [1.0097135] [-0.02208129]\n",
      "1700 6.383351e-05 [1.0092571] [-0.02104352]\n",
      "1720 5.797352e-05 [1.008822] [-0.02005454]\n",
      "1740 5.2652704e-05 [1.0084074] [-0.01911201]\n",
      "1760 4.782046e-05 [1.0080123] [-0.0182138]\n",
      "1780 4.343102e-05 [1.0076357] [-0.01735782]\n",
      "1800 3.9444578e-05 [1.0072769] [-0.01654207]\n",
      "1820 3.582425e-05 [1.0069349] [-0.01576463]\n",
      "1840 3.253642e-05 [1.0066088] [-0.01502374]\n",
      "1860 2.955003e-05 [1.0062983] [-0.01431763]\n",
      "1880 2.6837215e-05 [1.0060023] [-0.01364474]\n",
      "1900 2.4374547e-05 [1.0057203] [-0.01300349]\n",
      "1920 2.2136586e-05 [1.0054514] [-0.01239237]\n",
      "1940 2.010522e-05 [1.0051953] [-0.01180997]\n",
      "1960 1.825993e-05 [1.0049511] [-0.01125497]\n",
      "1980 1.6583806e-05 [1.0047184] [-0.01072605]\n",
      "2000 1.5062266e-05 [1.0044967] [-0.01022199]\n",
      "2020 1.367916e-05 [1.0042853] [-0.00974161]\n",
      "2040 1.2424251e-05 [1.0040839] [-0.0092838]\n",
      "2060 1.1283434e-05 [1.003892] [-0.00884745]\n",
      "2080 1.0248125e-05 [1.0037092] [-0.00843166]\n",
      "2100 9.307587e-06 [1.0035348] [-0.00803546]\n",
      "2120 8.452938e-06 [1.0033687] [-0.00765783]\n",
      "2140 7.677344e-06 [1.0032104] [-0.00729796]\n",
      "2160 6.972332e-06 [1.0030595] [-0.00695498]\n",
      "2180 6.332888e-06 [1.0029159] [-0.00662814]\n",
      "2200 5.7516368e-06 [1.0027788] [-0.00631667]\n",
      "2220 5.223781e-06 [1.0026481] [-0.00601986]\n",
      "2240 4.744333e-06 [1.0025238] [-0.00573699]\n",
      "2260 4.3092577e-06 [1.0024052] [-0.00546744]\n",
      "2280 3.91351e-06 [1.0022922] [-0.0052105]\n",
      "2300 3.554488e-06 [1.0021845] [-0.00496567]\n",
      "2320 3.228189e-06 [1.0020819] [-0.00473234]\n",
      "2340 2.9319924e-06 [1.0019841] [-0.00451]\n",
      "2360 2.6630141e-06 [1.0018909] [-0.00429811]\n",
      "2380 2.4187077e-06 [1.001802] [-0.00409621]\n",
      "2400 2.1968624e-06 [1.0017173] [-0.00390377]\n",
      "2420 1.9951474e-06 [1.0016367] [-0.00372036]\n",
      "2440 1.8119908e-06 [1.0015597] [-0.00354557]\n",
      "2460 1.6458551e-06 [1.0014865] [-0.00337902]\n",
      "2480 1.494881e-06 [1.0014166] [-0.00322026]\n",
      "2500 1.3578101e-06 [1.0013502] [-0.00306897]\n",
      "2520 1.233138e-06 [1.0012867] [-0.0029248]\n",
      "2540 1.1201107e-06 [1.0012263] [-0.00278742]\n",
      "2560 1.0171995e-06 [1.0011686] [-0.00265649]\n",
      "2580 9.2395135e-07 [1.0011137] [-0.00253176]\n",
      "2600 8.3918604e-07 [1.0010614] [-0.00241283]\n",
      "2620 7.6225996e-07 [1.0010116] [-0.00229953]\n",
      "2640 6.923585e-07 [1.000964] [-0.00219157]\n",
      "2660 6.28829e-07 [1.0009187] [-0.00208868]\n",
      "2680 5.7123225e-07 [1.0008757] [-0.0019906]\n",
      "2700 5.188168e-07 [1.0008347] [-0.00189713]\n",
      "2720 4.712308e-07 [1.0007957] [-0.00180807]\n",
      "2740 4.2811052e-07 [1.0007583] [-0.00172324]\n",
      "2760 3.8882584e-07 [1.0007225] [-0.00164246]\n",
      "2780 3.5327466e-07 [1.0006889] [-0.00156541]\n",
      "2800 3.208568e-07 [1.0006565] [-0.00149192]\n",
      "2820 2.914289e-07 [1.0006255] [-0.00142196]\n",
      "2840 2.6480507e-07 [1.0005965] [-0.00135524]\n",
      "2860 2.4047978e-07 [1.0005683] [-0.00129167]\n",
      "2880 2.1858824e-07 [1.0005419] [-0.00123117]\n",
      "2900 1.9854394e-07 [1.0005162] [-0.00117339]\n",
      "2920 1.8036815e-07 [1.0004923] [-0.00111845]\n",
      "2940 1.6374621e-07 [1.000469] [-0.00106596]\n",
      "2960 1.4884614e-07 [1.0004473] [-0.00101603]\n",
      "2980 1.3516514e-07 [1.0004258] [-0.00096839]\n",
      "3000 1.2281903e-07 [1.0004063] [-0.00092294]\n",
      "3020 1.1159339e-07 [1.0003872] [-0.00087979]\n",
      "3040 1.0133266e-07 [1.0003688] [-0.00083843]\n",
      "3060 9.2070515e-08 [1.0003519] [-0.00079919]\n",
      "3080 8.364932e-08 [1.0003352] [-0.00076185]\n",
      "3100 7.600423e-08 [1.0003194] [-0.000726]\n",
      "3120 6.902311e-08 [1.0003048] [-0.00069204]\n",
      "3140 6.2753564e-08 [1.0002905] [-0.00065982]\n",
      "3160 5.699876e-08 [1.0002764] [-0.00062882]\n",
      "3180 5.1772357e-08 [1.0002638] [-0.00059929]\n",
      "3200 4.7103583e-08 [1.0002518] [-0.00057138]\n",
      "3220 4.2794593e-08 [1.0002398] [-0.0005448]\n",
      "3240 3.882522e-08 [1.0002283] [-0.00051916]\n",
      "3260 3.5284597e-08 [1.0002178] [-0.00049475]\n",
      "3280 3.2086728e-08 [1.0002078] [-0.00047169]\n",
      "3300 2.9167557e-08 [1.0001982] [-0.0004498]\n",
      "3320 2.6501828e-08 [1.0001887] [-0.00042884]\n",
      "3340 2.406072e-08 [1.0001796] [-0.00040858]\n",
      "3360 2.186602e-08 [1.0001713] [-0.00038935]\n",
      "3380 1.9867594e-08 [1.0001636] [-0.00037116]\n",
      "3400 1.8068627e-08 [1.000156] [-0.00035395]\n",
      "3420 1.6453887e-08 [1.0001489] [-0.00033762]\n",
      "3440 1.4942682e-08 [1.0001417] [-0.00032196]\n",
      "3460 1.3563589e-08 [1.0001347] [-0.00030676]\n",
      "3480 1.2296472e-08 [1.0001284] [-0.00029219]\n",
      "3500 1.1172087e-08 [1.0001225] [-0.00027843]\n",
      "3520 1.0153244e-08 [1.000117] [-0.00026541]\n",
      "3540 9.243641e-09 [1.0001116] [-0.00025311]\n",
      "3560 8.416134e-09 [1.0001066] [-0.00024148]\n",
      "3580 7.662579e-09 [1.0001018] [-0.00023045]\n",
      "3600 6.97176e-09 [1.000097] [-0.00021992]\n",
      "3620 6.336967e-09 [1.0000923] [-0.00020972]\n",
      "3640 5.7423235e-09 [1.0000876] [-0.00019974]\n",
      "3660 5.2139533e-09 [1.0000834] [-0.00019015]\n",
      "3680 4.7316617e-09 [1.0000795] [-0.00018108]\n",
      "3700 4.292044e-09 [1.0000758] [-0.00017251]\n",
      "3720 3.9037893e-09 [1.0000724] [-0.0001644]\n",
      "3740 3.5417809e-09 [1.000069] [-0.00015672]\n",
      "3760 3.2270293e-09 [1.0000659] [-0.00014946]\n",
      "3780 2.9320877e-09 [1.0000631] [-0.00014262]\n",
      "3800 2.6775488e-09 [1.0000602] [-0.00013613]\n",
      "3820 2.437967e-09 [1.0000576] [-0.00013]\n",
      "3840 2.2282727e-09 [1.0000551] [-0.00012421]\n",
      "3860 2.0343738e-09 [1.0000527] [-0.00011873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3880 1.8579319e-09 [1.0000503] [-0.00011349]\n",
      "3900 1.6921019e-09 [1.0000479] [-0.0001084]\n",
      "3920 1.5384778e-09 [1.0000455] [-0.00010342]\n",
      "3940 1.3959524e-09 [1.0000432] [-9.851288e-05]\n",
      "3960 1.2580598e-09 [1.0000409] [-9.364914e-05]\n",
      "3980 1.1398159e-09 [1.0000387] [-8.8969384e-05]\n",
      "4000 1.0302017e-09 [1.0000368] [-8.453756e-05]\n",
      "4020 9.25791e-10 [1.0000352] [-8.0348545e-05]\n",
      "4040 8.4263857e-10 [1.0000334] [-7.639953e-05]\n",
      "4060 7.560601e-10 [1.0000318] [-7.265357e-05]\n",
      "4080 6.8421e-10 [1.0000303] [-6.9117814e-05]\n",
      "4100 6.199012e-10 [1.0000288] [-6.5779546e-05]\n",
      "4120 5.6417687e-10 [1.0000275] [-6.262409e-05]\n",
      "4140 5.118288e-10 [1.0000262] [-5.964186e-05]\n",
      "4160 4.6433613e-10 [1.000025] [-5.6812627e-05]\n",
      "4180 4.2178408e-10 [1.0000238] [-5.414115e-05]\n",
      "4200 3.847731e-10 [1.0000228] [-5.1620278e-05]\n",
      "4220 3.4873912e-10 [1.0000217] [-4.9234506e-05]\n",
      "4240 3.16471e-10 [1.0000209] [-4.6979858e-05]\n",
      "4260 2.8816297e-10 [1.0000199] [-4.4850785e-05]\n",
      "4280 2.6526928e-10 [1.000019] [-4.284449e-05]\n",
      "4300 2.433893e-10 [1.0000181] [-4.094509e-05]\n",
      "4320 2.2037246e-10 [1.0000174] [-3.914424e-05]\n",
      "4340 2.040963e-10 [1.0000167] [-3.7443126e-05]\n",
      "4360 1.8542916e-10 [1.000016] [-3.5836576e-05]\n",
      "4380 1.7214201e-10 [1.0000153] [-3.4320226e-05]\n",
      "4400 1.5753325e-10 [1.0000147] [-3.2886932e-05]\n",
      "4420 1.427954e-10 [1.0000142] [-3.1525164e-05]\n",
      "4440 1.3272938e-10 [1.0000136] [-3.024207e-05]\n",
      "4460 1.2281968e-10 [1.000013] [-2.9033286e-05]\n",
      "4480 1.14019606e-10 [1.0000125] [-2.7886494e-05]\n",
      "4500 1.0407083e-10 [1.0000122] [-2.68001e-05]\n",
      "4520 9.742607e-11 [1.0000117] [-2.5776493e-05]\n",
      "4540 9.013116e-11 [1.0000112] [-2.4808514e-05]\n",
      "4560 8.306363e-11 [1.0000108] [-2.3879482e-05]\n",
      "4580 7.746929e-11 [1.0000105] [-2.3027531e-05]\n",
      "4600 7.33813e-11 [1.0000101] [-2.2210152e-05]\n",
      "4620 6.73405e-11 [1.0000098] [-2.1435288e-05]\n",
      "4640 6.217249e-11 [1.0000094] [-2.070175e-05]\n",
      "4660 5.955769e-11 [1.0000092] [-2.0023448e-05]\n",
      "4680 5.5276672e-11 [1.000009] [-1.936144e-05]\n",
      "4700 5.136395e-11 [1.0000086] [-1.8739562e-05]\n",
      "4720 4.8976528e-11 [1.0000083] [-1.8166163e-05]\n",
      "4740 4.5853692e-11 [1.0000081] [-1.760866e-05]\n",
      "4760 4.2960597e-11 [1.000008] [-1.708056e-05]\n",
      "4780 4.162004e-11 [1.0000077] [-1.6611668e-05]\n",
      "4800 3.8091013e-11 [1.0000075] [-1.6128866e-05]\n",
      "4820 3.7161385e-11 [1.0000073] [-1.5700507e-05]\n",
      "4840 3.5338843e-11 [1.0000072] [-1.5269763e-05]\n",
      "4860 3.2708652e-11 [1.000007] [-1.4891074e-05]\n",
      "4880 3.2159164e-11 [1.0000068] [-1.451914e-05]\n",
      "4900 3.087545e-11 [1.0000067] [-1.4157935e-05]\n",
      "4920 2.8710664e-11 [1.0000066] [-1.3815013e-05]\n",
      "4940 2.785446e-11 [1.0000064] [-1.35273185e-05]\n",
      "4960 2.6172842e-11 [1.0000062] [-1.3255122e-05]\n",
      "4980 2.5337954e-11 [1.0000061] [-1.2963853e-05]\n",
      "5000 2.4690175e-11 [1.0000061] [-1.2684508e-05]\n",
      "5020 2.4169111e-11 [1.0000058] [-1.2440527e-05]\n",
      "5040 2.365752e-11 [1.0000057] [-1.2202503e-05]\n",
      "5060 2.2302752e-11 [1.0000057] [-1.1951368e-05]\n",
      "5080 2.1283123e-11 [1.0000056] [-1.17693735e-05]\n",
      "5100 2.0240995e-11 [1.0000056] [-1.1567114e-05]\n",
      "5120 2.000533e-11 [1.0000055] [-1.1373994e-05]\n",
      "5140 1.9805194e-11 [1.0000054] [-1.119796e-05]\n",
      "5160 1.884004e-11 [1.0000052] [-1.1030673e-05]\n",
      "5180 1.808568e-11 [1.0000052] [-1.0848281e-05]\n",
      "5200 1.7158422e-11 [1.0000051] [-1.0686548e-05]\n",
      "5220 1.7404744e-11 [1.0000051] [-1.0530782e-05]\n",
      "5240 1.6964208e-11 [1.0000051] [-1.0422296e-05]\n",
      "5260 1.7002103e-11 [1.000005] [-1.0328914e-05]\n",
      "5280 1.6768809e-11 [1.000005] [-1.0227583e-05]\n",
      "5300 1.6342483e-11 [1.000005] [-1.0150098e-05]\n",
      "5320 1.6561566e-11 [1.0000049] [-1.0061882e-05]\n",
      "5340 1.6356694e-11 [1.0000049] [-9.980423e-06]\n",
      "5360 1.5641414e-11 [1.0000048] [-9.866777e-06]\n",
      "5380 1.5215088e-11 [1.0000048] [-9.761869e-06]\n",
      "5400 1.480771e-11 [1.0000048] [-9.68041e-06]\n",
      "5420 1.4316252e-11 [1.0000046] [-9.579086e-06]\n",
      "5440 1.3913611e-11 [1.0000046] [-9.470203e-06]\n",
      "5460 1.44240175e-11 [1.0000046] [-9.399475e-06]\n",
      "5480 1.4235724e-11 [1.0000046] [-9.357747e-06]\n",
      "5500 1.4230987e-11 [1.0000046] [-9.3339e-06]\n",
      "5520 1.3548866e-11 [1.0000045] [-9.2607825e-06]\n",
      "5540 1.3348729e-11 [1.0000045] [-9.186875e-06]\n",
      "5560 1.4049799e-11 [1.0000045] [-9.1503125e-06]\n",
      "5580 1.38662415e-11 [1.0000045] [-9.126068e-06]\n",
      "5600 1.336294e-11 [1.0000044] [-9.064078e-06]\n",
      "5620 1.3165173e-11 [1.0000044] [-8.991363e-06]\n",
      "5640 1.29839845e-11 [1.0000044] [-8.9405075e-06]\n",
      "5660 1.3685053e-11 [1.0000044] [-8.92461e-06]\n",
      "5680 1.35062335e-11 [1.0000044] [-8.9095065e-06]\n",
      "5700 1.35062335e-11 [1.0000044] [-8.9015575e-06]\n",
      "5720 1.35062335e-11 [1.0000044] [-8.8936085e-06]\n",
      "5740 1.35062335e-11 [1.0000044] [-8.88566e-06]\n",
      "5760 1.2335022e-11 [1.0000043] [-8.844729e-06]\n",
      "5780 1.2141991e-11 [1.0000043] [-8.754128e-06]\n",
      "5800 1.2628713e-11 [1.0000043] [-8.694128e-06]\n",
      "5820 1.245463e-11 [1.0000043] [-8.664717e-06]\n",
      "5840 1.2449893e-11 [1.0000043] [-8.641665e-06]\n",
      "5860 1.2449893e-11 [1.0000043] [-8.633716e-06]\n",
      "5880 1.2449893e-11 [1.0000043] [-8.625767e-06]\n",
      "5900 1.2449893e-11 [1.0000043] [-8.617818e-06]\n",
      "5920 1.22781785e-11 [1.0000043] [-8.612651e-06]\n",
      "5940 1.22781785e-11 [1.0000043] [-8.612651e-06]\n",
      "5960 1.22781785e-11 [1.0000043] [-8.612651e-06]\n",
      "5980 1.22781785e-11 [1.0000043] [-8.612651e-06]\n",
      "6000 1.22781785e-11 [1.0000043] [-8.612651e-06]\n"
     ]
    }
   ],
   "source": [
    "# Lab 2 Linear Regression\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = x_data * W + b\n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let TensorFlow figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Fit the line\n",
    "    for step in range(6001):\n",
    "        _, cost_val, W_val, b_val = sess.run([train, cost, W, b])\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3456221 [0.854857]\n",
      "1 0.09831023 [0.92259043]\n",
      "2 0.0279638 [0.9587149]\n",
      "3 0.007954148 [0.97798127]\n",
      "4 0.0022625169 [0.9882567]\n",
      "5 0.00064355787 [0.9937369]\n",
      "6 0.00018305605 [0.9966597]\n",
      "7 5.206934e-05 [0.99821854]\n",
      "8 1.4810193e-05 [0.9990499]\n",
      "9 4.212536e-06 [0.9994933]\n",
      "10 1.1981989e-06 [0.99972975]\n",
      "11 3.4075947e-07 [0.9998559]\n",
      "12 9.696913e-08 [0.99992317]\n",
      "13 2.7556064e-08 [0.99995905]\n",
      "14 7.8200495e-09 [0.9999782]\n",
      "15 2.2157043e-09 [0.9999884]\n",
      "16 6.2904465e-10 [0.9999938]\n",
      "17 1.7932204e-10 [0.9999967]\n",
      "18 4.976286e-11 [0.9999983]\n",
      "19 1.4150459e-11 [0.9999991]\n",
      "20 3.6249521e-12 [0.9999995]\n"
     ]
    }
   ],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = W * x_data\n",
    "# We know that W should be 1\n",
    "# But let's use TensorFlow to figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    # 이게 없으면, 학습된 w를 가지고 또다시 학습을 하게 되므로, 초기화가 반드시 필요하다.\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "    for step in range(21):\n",
    "        _, cost_val, W_val = sess.run(\n",
    "            [update, cost, W], feed_dict={X: x_data, Y: y_data}\n",
    "        ) # 실제로는 cost와 W를 제외하고 update만 있어도 작동한다. print를 위해 넣음 \n",
    "        print(step, cost_val, W_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2666664\n",
      "1 1.0177778\n",
      "2 1.0011852\n",
      "3 1.000079\n",
      "4 1.0000052\n",
      "5 1.0000004\n",
      "6 1.0\n",
      "7 1.0\n",
      "8 1.0\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 1.0\n",
      "14 1.0\n",
      "15 1.0\n",
      "16 1.0\n",
      "17 1.0\n",
      "18 1.0\n",
      "19 1.0\n",
      "20 1.0\n",
      "21 1.0\n",
      "22 1.0\n",
      "23 1.0\n",
      "24 1.0\n",
      "25 1.0\n",
      "26 1.0\n",
      "27 1.0\n",
      "28 1.0\n",
      "29 1.0\n",
      "30 1.0\n",
      "31 1.0\n",
      "32 1.0\n",
      "33 1.0\n",
      "34 1.0\n",
      "35 1.0\n",
      "36 1.0\n",
      "37 1.0\n",
      "38 1.0\n",
      "39 1.0\n",
      "40 1.0\n",
      "41 1.0\n",
      "42 1.0\n",
      "43 1.0\n",
      "44 1.0\n",
      "45 1.0\n",
      "46 1.0\n",
      "47 1.0\n",
      "48 1.0\n",
      "49 1.0\n",
      "50 1.0\n",
      "51 1.0\n",
      "52 1.0\n",
      "53 1.0\n",
      "54 1.0\n",
      "55 1.0\n",
      "56 1.0\n",
      "57 1.0\n",
      "58 1.0\n",
      "59 1.0\n",
      "60 1.0\n",
      "61 1.0\n",
      "62 1.0\n",
      "63 1.0\n",
      "64 1.0\n",
      "65 1.0\n",
      "66 1.0\n",
      "67 1.0\n",
      "68 1.0\n",
      "69 1.0\n",
      "70 1.0\n",
      "71 1.0\n",
      "72 1.0\n",
      "73 1.0\n",
      "74 1.0\n",
      "75 1.0\n",
      "76 1.0\n",
      "77 1.0\n",
      "78 1.0\n",
      "79 1.0\n",
      "80 1.0\n",
      "81 1.0\n",
      "82 1.0\n",
      "83 1.0\n",
      "84 1.0\n",
      "85 1.0\n",
      "86 1.0\n",
      "87 1.0\n",
      "88 1.0\n",
      "89 1.0\n",
      "90 1.0\n",
      "91 1.0\n",
      "92 1.0\n",
      "93 1.0\n",
      "94 1.0\n",
      "95 1.0\n",
      "96 1.0\n",
      "97 1.0\n",
      "98 1.0\n",
      "99 1.0\n",
      "100 1.0\n"
     ]
    }
   ],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.0)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, W_val = sess.run([train, W])\n",
    "        print(step, W_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-02c4706d0f0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mgradient_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgvs_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgvs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgvs_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1158\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    472\u001b[0m     \"\"\"\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[0;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[0;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[0;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[0;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[0;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[0;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n\u001b[1;32m--> 261\u001b[1;33m                                                                  type(fetch)))\n\u001b[0m\u001b[0;32m    262\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "# This is optional\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "\n",
    "# Optional: modify gradient if necessary\n",
    "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        gradient_val, gvs_val, _ = sess.run([gradient, gvs, apply_gradients])\n",
    "        print(step, gradient_val, gvs_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-variable linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 4 Multi-variable linear regression\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]]\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]\n",
    "\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(4001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7307833\n",
      "200 0.5715119\n",
      "400 0.5074139\n",
      "600 0.4718242\n",
      "800 0.44758478\n",
      "1000 0.42857108\n",
      "1200 0.41232464\n",
      "1400 0.39775506\n",
      "1600 0.38433787\n",
      "1800 0.37180105\n",
      "2000 0.35999325\n",
      "2200 0.34882212\n",
      "2400 0.33822623\n",
      "2600 0.32816052\n",
      "2800 0.31858906\n",
      "3000 0.30948088\n",
      "3200 0.30080852\n",
      "3400 0.29254702\n",
      "3600 0.28467283\n",
      "3800 0.27716395\n",
      "4000 0.2699997\n",
      "4200 0.26316056\n",
      "4400 0.2566281\n",
      "4600 0.2503851\n",
      "4800 0.24441504\n",
      "5000 0.23870273\n",
      "5200 0.23323363\n",
      "5400 0.22799431\n",
      "5600 0.22297198\n",
      "5800 0.21815474\n",
      "6000 0.21353154\n",
      "6200 0.20909168\n",
      "6400 0.20482557\n",
      "6600 0.20072372\n",
      "6800 0.1967777\n",
      "7000 0.19297928\n",
      "7200 0.18932094\n",
      "7400 0.18579553\n",
      "7600 0.18239634\n",
      "7800 0.17911713\n",
      "8000 0.17595184\n",
      "8200 0.17289506\n",
      "8400 0.16994144\n",
      "8600 0.16708624\n",
      "8800 0.16432472\n",
      "9000 0.16165248\n",
      "9200 0.15906551\n",
      "9400 0.1565599\n",
      "9600 0.15413193\n",
      "9800 0.15177831\n",
      "10000 0.14949557\n",
      "\n",
      "Hypothesis:  [[0.03074028]\n",
      " [0.15884678]\n",
      " [0.30486736]\n",
      " [0.78138196]\n",
      " [0.93957496]\n",
      " [0.9801688 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Lab 5 Logistic Regression Classifier\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "# Optimizer \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 학습하는 부분 \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report # 결과를 측정하는 부분 \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab에서 연결하는 법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.ount = ('/content/drive')\n",
    "# folder = '/content/drive/My Drive/deepall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier 파일읽기 코드 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.971756\n",
      "200 0.7333331\n",
      "400 0.68039155\n",
      "600 0.6572894\n",
      "800 0.6404287\n",
      "1000 0.62592113\n",
      "1200 0.61297643\n",
      "1400 0.60134125\n",
      "1600 0.5908649\n",
      "1800 0.5814247\n",
      "2000 0.5729111\n",
      "2200 0.56522524\n",
      "2400 0.5582785\n",
      "2600 0.5519916\n",
      "2800 0.5462933\n",
      "3000 0.54112047\n",
      "3200 0.53641707\n",
      "3400 0.5321332\n",
      "3600 0.5282246\n",
      "3800 0.5246522\n",
      "4000 0.5213812\n",
      "4200 0.51838106\n",
      "4400 0.5156242\n",
      "4600 0.5130869\n",
      "4800 0.5107475\n",
      "5000 0.5085869\n",
      "5200 0.50658816\n",
      "5400 0.50473636\n",
      "5600 0.50301784\n",
      "5800 0.5014206\n",
      "6000 0.499934\n",
      "6200 0.49854836\n",
      "6400 0.49725494\n",
      "6600 0.49604607\n",
      "6800 0.4949147\n",
      "7000 0.4938545\n",
      "7200 0.4928598\n",
      "7400 0.49192554\n",
      "7600 0.49104688\n",
      "7800 0.49021977\n",
      "8000 0.4894402\n",
      "8200 0.4887048\n",
      "8400 0.4880103\n",
      "8600 0.48735377\n",
      "8800 0.4867326\n",
      "9000 0.4861443\n",
      "9200 0.48558676\n",
      "9400 0.4850579\n",
      "9600 0.4845557\n",
      "9800 0.4840786\n",
      "10000 0.48362476\n",
      "\n",
      "Hypothesis:  [[0.37003183]\n",
      " [0.9154805 ]\n",
      " [0.21602729]\n",
      " [0.9483494 ]\n",
      " [0.08354768]\n",
      " [0.7647524 ]\n",
      " [0.94660914]\n",
      " [0.6240098 ]\n",
      " [0.24575189]\n",
      " [0.53068644]\n",
      " [0.70052207]\n",
      " [0.17214665]\n",
      " [0.16543272]\n",
      " [0.2199429 ]\n",
      " [0.71247756]\n",
      " [0.4538526 ]\n",
      " [0.73282206]\n",
      " [0.86161506]\n",
      " [0.8147425 ]\n",
      " [0.5555971 ]\n",
      " [0.6401038 ]\n",
      " [0.10689518]\n",
      " [0.68355197]\n",
      " [0.74390626]\n",
      " [0.3642101 ]\n",
      " [0.93936753]\n",
      " [0.6559926 ]\n",
      " [0.62544465]\n",
      " [0.6521136 ]\n",
      " [0.40692744]\n",
      " [0.9621741 ]\n",
      " [0.87790984]\n",
      " [0.58168435]\n",
      " [0.7878031 ]\n",
      " [0.37508693]\n",
      " [0.6011569 ]\n",
      " [0.7843533 ]\n",
      " [0.3977098 ]\n",
      " [0.5350236 ]\n",
      " [0.31824094]\n",
      " [0.8445039 ]\n",
      " [0.15713441]\n",
      " [0.4164198 ]\n",
      " [0.03135979]\n",
      " [0.5267336 ]\n",
      " [0.9159291 ]\n",
      " [0.70154345]\n",
      " [0.7093997 ]\n",
      " [0.9460212 ]\n",
      " [0.9376935 ]\n",
      " [0.9446371 ]\n",
      " [0.24789673]\n",
      " [0.32195848]\n",
      " [0.9602661 ]\n",
      " [0.23895976]\n",
      " [0.42620695]\n",
      " [0.06667963]\n",
      " [0.7589261 ]\n",
      " [0.84101737]\n",
      " [0.5098418 ]\n",
      " [0.93055457]\n",
      " [0.70813584]\n",
      " [0.6522975 ]\n",
      " [0.8572492 ]\n",
      " [0.54217255]\n",
      " [0.45803586]\n",
      " [0.9648669 ]\n",
      " [0.760651  ]\n",
      " [0.83716965]\n",
      " [0.70461416]\n",
      " [0.2525223 ]\n",
      " [0.7577009 ]\n",
      " [0.9056169 ]\n",
      " [0.92974293]\n",
      " [0.8595285 ]\n",
      " [0.7762509 ]\n",
      " [0.4248644 ]\n",
      " [0.86663973]\n",
      " [0.90810114]\n",
      " [0.91360456]\n",
      " [0.8638842 ]\n",
      " [0.8441914 ]\n",
      " [0.36394185]\n",
      " [0.80408204]\n",
      " [0.55200076]\n",
      " [0.88440704]\n",
      " [0.5241623 ]\n",
      " [0.90309423]\n",
      " [0.9285145 ]\n",
      " [0.79115593]\n",
      " [0.8253182 ]\n",
      " [0.64373755]\n",
      " [0.7171907 ]\n",
      " [0.62347317]\n",
      " [0.9041424 ]\n",
      " [0.9790907 ]\n",
      " [0.90901136]\n",
      " [0.5846414 ]\n",
      " [0.16190979]\n",
      " [0.65837073]\n",
      " [0.63592726]\n",
      " [0.965997  ]\n",
      " [0.67171514]\n",
      " [0.7229756 ]\n",
      " [0.8866031 ]\n",
      " [0.7276777 ]\n",
      " [0.9305426 ]\n",
      " [0.8626789 ]\n",
      " [0.568232  ]\n",
      " [0.32958367]\n",
      " [0.9432294 ]\n",
      " [0.8515173 ]\n",
      " [0.44822004]\n",
      " [0.3929912 ]\n",
      " [0.62763   ]\n",
      " [0.79084575]\n",
      " [0.85894793]\n",
      " [0.93857807]\n",
      " [0.14030883]\n",
      " [0.7290221 ]\n",
      " [0.8647617 ]\n",
      " [0.6166415 ]\n",
      " [0.64215124]\n",
      " [0.7795745 ]\n",
      " [0.7059267 ]\n",
      " [0.8574369 ]\n",
      " [0.82025975]\n",
      " [0.54896337]\n",
      " [0.57049835]\n",
      " [0.31858003]\n",
      " [0.45877913]\n",
      " [0.75337434]\n",
      " [0.938213  ]\n",
      " [0.8635385 ]\n",
      " [0.8011608 ]\n",
      " [0.8701302 ]\n",
      " [0.4200652 ]\n",
      " [0.81549966]\n",
      " [0.72602177]\n",
      " [0.71848154]\n",
      " [0.89642787]\n",
      " [0.6353291 ]\n",
      " [0.61051697]\n",
      " [0.6720503 ]\n",
      " [0.9115739 ]\n",
      " [0.64918935]\n",
      " [0.44098887]\n",
      " [0.9371995 ]\n",
      " [0.6337202 ]\n",
      " [0.76130134]\n",
      " [0.23399186]\n",
      " [0.32680225]\n",
      " [0.12558189]\n",
      " [0.2377812 ]\n",
      " [0.90760034]\n",
      " [0.8623409 ]\n",
      " [0.95348275]\n",
      " [0.12552068]\n",
      " [0.53432935]\n",
      " [0.8026458 ]\n",
      " [0.64998204]\n",
      " [0.84879893]\n",
      " [0.38238317]\n",
      " [0.79798627]\n",
      " [0.6338122 ]\n",
      " [0.57953376]\n",
      " [0.69325584]\n",
      " [0.88282454]\n",
      " [0.7700525 ]\n",
      " [0.6494111 ]\n",
      " [0.85252994]\n",
      " [0.85940075]\n",
      " [0.95391524]\n",
      " [0.2406517 ]\n",
      " [0.79474664]\n",
      " [0.27039862]\n",
      " [0.37937927]\n",
      " [0.3092518 ]\n",
      " [0.88998455]\n",
      " [0.68779784]\n",
      " [0.932184  ]\n",
      " [0.9017601 ]\n",
      " [0.5979745 ]\n",
      " [0.13032863]\n",
      " [0.17434558]\n",
      " [0.52043444]\n",
      " [0.7481282 ]\n",
      " [0.6766155 ]\n",
      " [0.8281049 ]\n",
      " [0.6600339 ]\n",
      " [0.3626326 ]\n",
      " [0.18541756]\n",
      " [0.9092251 ]\n",
      " [0.41367313]\n",
      " [0.85321283]\n",
      " [0.9062588 ]\n",
      " [0.68145704]\n",
      " [0.6851039 ]\n",
      " [0.56336015]\n",
      " [0.55329156]\n",
      " [0.67773235]\n",
      " [0.9587245 ]\n",
      " [0.7710125 ]\n",
      " [0.8127991 ]\n",
      " [0.13762411]\n",
      " [0.33096358]\n",
      " [0.90895224]\n",
      " [0.22505349]\n",
      " [0.93034935]\n",
      " [0.2797768 ]\n",
      " [0.28565747]\n",
      " [0.52054805]\n",
      " [0.7501162 ]\n",
      " [0.20570868]\n",
      " [0.7580343 ]\n",
      " [0.7481889 ]\n",
      " [0.72012275]\n",
      " [0.64906794]\n",
      " [0.11264306]\n",
      " [0.27403602]\n",
      " [0.71924555]\n",
      " [0.507519  ]\n",
      " [0.92561567]\n",
      " [0.94974613]\n",
      " [0.70573455]\n",
      " [0.3314457 ]\n",
      " [0.01243344]\n",
      " [0.7328249 ]\n",
      " [0.31322718]\n",
      " [0.4551864 ]\n",
      " [0.95113003]\n",
      " [0.6119204 ]\n",
      " [0.956211  ]\n",
      " [0.21247807]\n",
      " [0.13730332]\n",
      " [0.2640965 ]\n",
      " [0.746002  ]\n",
      " [0.90616345]\n",
      " [0.8902337 ]\n",
      " [0.6388824 ]\n",
      " [0.5905467 ]\n",
      " [0.6142768 ]\n",
      " [0.11803737]\n",
      " [0.5649264 ]\n",
      " [0.12032914]\n",
      " [0.594708  ]\n",
      " [0.87652457]\n",
      " [0.65327024]\n",
      " [0.7013043 ]\n",
      " [0.9605942 ]\n",
      " [0.8171557 ]\n",
      " [0.7480624 ]\n",
      " [0.7085493 ]\n",
      " [0.73317325]\n",
      " [0.8715658 ]\n",
      " [0.38832107]\n",
      " [0.48885614]\n",
      " [0.45576376]\n",
      " [0.7863519 ]\n",
      " [0.6569421 ]\n",
      " [0.67967415]\n",
      " [0.7809389 ]\n",
      " [0.2671609 ]\n",
      " [0.3746989 ]\n",
      " [0.47442526]\n",
      " [0.62990814]\n",
      " [0.3152073 ]\n",
      " [0.9203259 ]\n",
      " [0.7605115 ]\n",
      " [0.9225775 ]\n",
      " [0.56285393]\n",
      " [0.75419915]\n",
      " [0.82398236]\n",
      " [0.84951985]\n",
      " [0.60194135]\n",
      " [0.8399879 ]\n",
      " [0.36067057]\n",
      " [0.64815956]\n",
      " [0.739785  ]\n",
      " [0.388665  ]\n",
      " [0.7920482 ]\n",
      " [0.24882397]\n",
      " [0.5482083 ]\n",
      " [0.9495237 ]\n",
      " [0.81041384]\n",
      " [0.86216235]\n",
      " [0.6961651 ]\n",
      " [0.38970423]\n",
      " [0.6218725 ]\n",
      " [0.4183418 ]\n",
      " [0.47441807]\n",
      " [0.6519029 ]\n",
      " [0.6572863 ]\n",
      " [0.6830765 ]\n",
      " [0.54236895]\n",
      " [0.18214154]\n",
      " [0.70659906]\n",
      " [0.92117083]\n",
      " [0.4837174 ]\n",
      " [0.6441154 ]\n",
      " [0.7906146 ]\n",
      " [0.55168736]\n",
      " [0.78574437]\n",
      " [0.43543455]\n",
      " [0.6656115 ]\n",
      " [0.8926581 ]\n",
      " [0.6695023 ]\n",
      " [0.7405897 ]\n",
      " [0.8683181 ]\n",
      " [0.4409264 ]\n",
      " [0.87412524]\n",
      " [0.95516   ]\n",
      " [0.31215012]\n",
      " [0.81482494]\n",
      " [0.28057495]\n",
      " [0.78481686]\n",
      " [0.8169272 ]\n",
      " [0.7238769 ]\n",
      " [0.40729514]\n",
      " [0.77939034]\n",
      " [0.7963444 ]\n",
      " [0.72602904]\n",
      " [0.21197814]\n",
      " [0.8400166 ]\n",
      " [0.88320935]\n",
      " [0.43998405]\n",
      " [0.9536884 ]\n",
      " [0.24628603]\n",
      " [0.7256563 ]\n",
      " [0.9585662 ]\n",
      " [0.25511128]\n",
      " [0.3866802 ]\n",
      " [0.66522104]\n",
      " [0.32212335]\n",
      " [0.18379894]\n",
      " [0.86391807]\n",
      " [0.91269827]\n",
      " [0.84217125]\n",
      " [0.61256695]\n",
      " [0.61782426]\n",
      " [0.61188555]\n",
      " [0.77833873]\n",
      " [0.8199853 ]\n",
      " [0.9470573 ]\n",
      " [0.72018087]\n",
      " [0.74858665]\n",
      " [0.59124976]\n",
      " [0.92642975]\n",
      " [0.94356465]\n",
      " [0.65994716]\n",
      " [0.28874373]\n",
      " [0.63262117]\n",
      " [0.34376183]\n",
      " [0.7872474 ]\n",
      " [0.18558833]\n",
      " [0.26064602]\n",
      " [0.42378932]\n",
      " [0.6634197 ]\n",
      " [0.3227157 ]\n",
      " [0.5886754 ]\n",
      " [0.85246617]\n",
      " [0.6462896 ]\n",
      " [0.8607625 ]\n",
      " [0.9566511 ]\n",
      " [0.78148973]\n",
      " [0.03902066]\n",
      " [0.3468342 ]\n",
      " [0.8460533 ]\n",
      " [0.8765325 ]\n",
      " [0.63745356]\n",
      " [0.27857938]\n",
      " [0.9115516 ]\n",
      " [0.8942279 ]\n",
      " [0.29613298]\n",
      " [0.5700543 ]\n",
      " [0.8259093 ]\n",
      " [0.8669206 ]\n",
      " [0.83554876]\n",
      " [0.8723941 ]\n",
      " [0.89825463]\n",
      " [0.93942595]\n",
      " [0.63998604]\n",
      " [0.610468  ]\n",
      " [0.5533838 ]\n",
      " [0.8144549 ]\n",
      " [0.87089974]\n",
      " [0.23334223]\n",
      " [0.7985961 ]\n",
      " [0.89199734]\n",
      " [0.27992696]\n",
      " [0.48323697]\n",
      " [0.83799076]\n",
      " [0.5846791 ]\n",
      " [0.9026144 ]\n",
      " [0.3026297 ]\n",
      " [0.8302666 ]\n",
      " [0.6704346 ]\n",
      " [0.87681437]\n",
      " [0.38466316]\n",
      " [0.69479674]\n",
      " [0.6945851 ]\n",
      " [0.79477954]\n",
      " [0.07643867]\n",
      " [0.17290843]\n",
      " [0.6348562 ]\n",
      " [0.81554246]\n",
      " [0.38170967]\n",
      " [0.8198056 ]\n",
      " [0.5048051 ]\n",
      " [0.38023195]\n",
      " [0.7809728 ]\n",
      " [0.42587546]\n",
      " [0.8984246 ]\n",
      " [0.8508326 ]\n",
      " [0.65622354]\n",
      " [0.91650057]\n",
      " [0.6692599 ]\n",
      " [0.77402496]\n",
      " [0.35418713]\n",
      " [0.32064432]\n",
      " [0.75412345]\n",
      " [0.46952868]\n",
      " [0.52253413]\n",
      " [0.9015024 ]\n",
      " [0.89992666]\n",
      " [0.91271305]\n",
      " [0.95233107]\n",
      " [0.7015096 ]\n",
      " [0.83610046]\n",
      " [0.38574633]\n",
      " [0.3973807 ]\n",
      " [0.47271666]\n",
      " [0.9415037 ]\n",
      " [0.53401273]\n",
      " [0.18898869]\n",
      " [0.9329737 ]\n",
      " [0.846507  ]\n",
      " [0.49440584]\n",
      " [0.803123  ]\n",
      " [0.00960758]\n",
      " [0.92079556]\n",
      " [0.8005519 ]\n",
      " [0.782111  ]\n",
      " [0.82033587]\n",
      " [0.96801263]\n",
      " [0.6016943 ]\n",
      " [0.7901676 ]\n",
      " [0.6405549 ]\n",
      " [0.8574066 ]\n",
      " [0.23814833]\n",
      " [0.5444675 ]\n",
      " [0.9211141 ]\n",
      " [0.6331614 ]\n",
      " [0.7630992 ]\n",
      " [0.93186295]\n",
      " [0.837811  ]\n",
      " [0.87973046]\n",
      " [0.444313  ]\n",
      " [0.81623125]\n",
      " [0.9568143 ]\n",
      " [0.75897527]\n",
      " [0.6570054 ]\n",
      " [0.32950932]\n",
      " [0.41978228]\n",
      " [0.57137984]\n",
      " [0.6386368 ]\n",
      " [0.53501093]\n",
      " [0.7883033 ]\n",
      " [0.59427124]\n",
      " [0.75114715]\n",
      " [0.8545512 ]\n",
      " [0.7967293 ]\n",
      " [0.6231361 ]\n",
      " [0.5059156 ]\n",
      " [0.6136735 ]\n",
      " [0.9459432 ]\n",
      " [0.8694799 ]\n",
      " [0.23167521]\n",
      " [0.4707916 ]\n",
      " [0.4324271 ]\n",
      " [0.08282861]\n",
      " [0.88935137]\n",
      " [0.14215669]\n",
      " [0.89593863]\n",
      " [0.8619416 ]\n",
      " [0.83440804]\n",
      " [0.62860477]\n",
      " [0.88424885]\n",
      " [0.35417128]\n",
      " [0.7715175 ]\n",
      " [0.9407079 ]\n",
      " [0.38943192]\n",
      " [0.42330426]\n",
      " [0.8894435 ]\n",
      " [0.86383   ]\n",
      " [0.58552647]\n",
      " [0.80563533]\n",
      " [0.7981679 ]\n",
      " [0.7988252 ]\n",
      " [0.3525102 ]\n",
      " [0.7406736 ]\n",
      " [0.8714471 ]\n",
      " [0.59675574]\n",
      " [0.7951093 ]\n",
      " [0.7616224 ]\n",
      " [0.80735326]\n",
      " [0.8396836 ]\n",
      " [0.947065  ]\n",
      " [0.6730312 ]\n",
      " [0.40919766]\n",
      " [0.78243226]\n",
      " [0.7520776 ]\n",
      " [0.97216964]\n",
      " [0.79477125]\n",
      " [0.7043548 ]\n",
      " [0.37024307]\n",
      " [0.72415984]\n",
      " [0.9112206 ]\n",
      " [0.95512813]\n",
      " [0.9207568 ]\n",
      " [0.7272074 ]\n",
      " [0.62702626]\n",
      " [0.8090792 ]\n",
      " [0.42198962]\n",
      " [0.76834804]\n",
      " [0.77097964]\n",
      " [0.8537084 ]\n",
      " [0.61192334]\n",
      " [0.70903116]\n",
      " [0.8534462 ]\n",
      " [0.48009148]\n",
      " [0.476007  ]\n",
      " [0.633771  ]\n",
      " [0.7343564 ]\n",
      " [0.5865755 ]\n",
      " [0.9221809 ]\n",
      " [0.9280279 ]\n",
      " [0.24493486]\n",
      " [0.11743277]\n",
      " [0.8053016 ]\n",
      " [0.5571552 ]\n",
      " [0.2246911 ]\n",
      " [0.83270484]\n",
      " [0.9014041 ]\n",
      " [0.67879647]\n",
      " [0.9412496 ]\n",
      " [0.91504407]\n",
      " [0.8018216 ]\n",
      " [0.82581055]\n",
      " [0.68717337]\n",
      " [0.5779414 ]\n",
      " [0.7635423 ]\n",
      " [0.61091614]\n",
      " [0.14149421]\n",
      " [0.9059015 ]\n",
      " [0.89553386]\n",
      " [0.68140393]\n",
      " [0.9151058 ]\n",
      " [0.8662422 ]\n",
      " [0.9031191 ]\n",
      " [0.63437545]\n",
      " [0.7443474 ]\n",
      " [0.86853504]\n",
      " [0.71422505]\n",
      " [0.87389636]\n",
      " [0.9243368 ]\n",
      " [0.5383593 ]\n",
      " [0.82471573]\n",
      " [0.82216066]\n",
      " [0.45614842]\n",
      " [0.5516042 ]\n",
      " [0.07508862]\n",
      " [0.2554174 ]\n",
      " [0.844239  ]\n",
      " [0.66269827]\n",
      " [0.6723399 ]\n",
      " [0.48981875]\n",
      " [0.9319752 ]\n",
      " [0.46693406]\n",
      " [0.8121207 ]\n",
      " [0.23049167]\n",
      " [0.8926242 ]\n",
      " [0.27195424]\n",
      " [0.79920727]\n",
      " [0.5422217 ]\n",
      " [0.77505684]\n",
      " [0.5572491 ]\n",
      " [0.27887404]\n",
      " [0.77221805]\n",
      " [0.9362968 ]\n",
      " [0.3985843 ]\n",
      " [0.9279789 ]\n",
      " [0.86264396]\n",
      " [0.8539331 ]\n",
      " [0.8187632 ]\n",
      " [0.4224022 ]\n",
      " [0.34413046]\n",
      " [0.64388883]\n",
      " [0.13661316]\n",
      " [0.9541315 ]\n",
      " [0.39051205]\n",
      " [0.93976825]\n",
      " [0.89578676]\n",
      " [0.43772215]\n",
      " [0.18767706]\n",
      " [0.63602334]\n",
      " [0.468482  ]\n",
      " [0.83219934]\n",
      " [0.719749  ]\n",
      " [0.98327374]\n",
      " [0.43412197]\n",
      " [0.6611377 ]\n",
      " [0.79943466]\n",
      " [0.6589147 ]\n",
      " [0.04487875]\n",
      " [0.7660401 ]\n",
      " [0.79205453]\n",
      " [0.85389835]\n",
      " [0.64413464]\n",
      " [0.45210445]\n",
      " [0.6059911 ]\n",
      " [0.90392005]\n",
      " [0.58050025]\n",
      " [0.8184375 ]\n",
      " [0.8036425 ]\n",
      " [0.8884411 ]\n",
      " [0.8134402 ]\n",
      " [0.56198764]\n",
      " [0.78616697]\n",
      " [0.90039146]\n",
      " [0.6773124 ]\n",
      " [0.96727574]\n",
      " [0.809111  ]\n",
      " [0.615409  ]\n",
      " [0.50250506]\n",
      " [0.7920008 ]\n",
      " [0.83763266]\n",
      " [0.4987446 ]\n",
      " [0.695879  ]\n",
      " [0.28857088]\n",
      " [0.63699603]\n",
      " [0.8366307 ]\n",
      " [0.95523494]\n",
      " [0.84366626]\n",
      " [0.77009255]\n",
      " [0.72999704]\n",
      " [0.90502757]\n",
      " [0.49800128]\n",
      " [0.94460726]\n",
      " [0.46422264]\n",
      " [0.7902366 ]\n",
      " [0.34313148]\n",
      " [0.05604666]\n",
      " [0.3349306 ]\n",
      " [0.36218202]\n",
      " [0.69808364]\n",
      " [0.84458375]\n",
      " [0.5875374 ]\n",
      " [0.7682426 ]\n",
      " [0.81240606]\n",
      " [0.59036416]\n",
      " [0.3901719 ]\n",
      " [0.8804146 ]\n",
      " [0.9080004 ]\n",
      " [0.34558284]\n",
      " [0.6036537 ]\n",
      " [0.20400006]\n",
      " [0.41466668]\n",
      " [0.7431205 ]\n",
      " [0.7132845 ]\n",
      " [0.895306  ]\n",
      " [0.9793977 ]\n",
      " [0.20553097]\n",
      " [0.7526351 ]\n",
      " [0.5752076 ]\n",
      " [0.37561542]\n",
      " [0.72092   ]\n",
      " [0.73230755]\n",
      " [0.89490426]\n",
      " [0.7099159 ]\n",
      " [0.49394777]\n",
      " [0.58987445]\n",
      " [0.17812285]\n",
      " [0.65061325]\n",
      " [0.5504319 ]\n",
      " [0.9054696 ]\n",
      " [0.598595  ]\n",
      " [0.6383472 ]\n",
      " [0.80110824]\n",
      " [0.7368445 ]\n",
      " [0.379004  ]\n",
      " [0.75590736]\n",
      " [0.6150167 ]\n",
      " [0.26063642]\n",
      " [0.58096695]\n",
      " [0.9117863 ]\n",
      " [0.8376616 ]\n",
      " [0.6030847 ]\n",
      " [0.80692065]\n",
      " [0.32273334]\n",
      " [0.8301623 ]\n",
      " [0.6273767 ]\n",
      " [0.7877811 ]\n",
      " [0.40930733]\n",
      " [0.68117344]\n",
      " [0.8340646 ]\n",
      " [0.15815163]\n",
      " [0.27006996]\n",
      " [0.78574777]\n",
      " [0.81408614]\n",
      " [0.76922286]\n",
      " [0.9012736 ]\n",
      " [0.7859848 ]\n",
      " [0.7243077 ]\n",
      " [0.7613378 ]\n",
      " [0.7465561 ]\n",
      " [0.68669134]\n",
      " [0.7927315 ]\n",
      " [0.50835574]\n",
      " [0.4611718 ]\n",
      " [0.8820466 ]\n",
      " [0.827842  ]\n",
      " [0.66061914]\n",
      " [0.2965859 ]\n",
      " [0.88612473]\n",
      " [0.7696434 ]\n",
      " [0.8275147 ]\n",
      " [0.69222766]\n",
      " [0.86061233]\n",
      " [0.86218196]\n",
      " [0.7398206 ]\n",
      " [0.40291807]\n",
      " [0.904356  ]\n",
      " [0.9267366 ]\n",
      " [0.2923935 ]\n",
      " [0.14486504]\n",
      " [0.75177014]\n",
      " [0.38205954]\n",
      " [0.7350758 ]\n",
      " [0.36591643]\n",
      " [0.47245234]\n",
      " [0.33215326]\n",
      " [0.7976325 ]\n",
      " [0.87367296]\n",
      " [0.15875712]\n",
      " [0.3784116 ]\n",
      " [0.54074955]\n",
      " [0.49625942]\n",
      " [0.51185215]\n",
      " [0.77610594]\n",
      " [0.16323733]\n",
      " [0.9135155 ]\n",
      " [0.19408867]\n",
      " [0.85807306]\n",
      " [0.7474166 ]\n",
      " [0.7190664 ]\n",
      " [0.8625043 ]\n",
      " [0.68273556]\n",
      " [0.886802  ]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.76679844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict=feed)\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict=feed))\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict=feed)\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.729933\n",
      "200 0.6581517\n",
      "400 0.5547044\n",
      "600 0.46214023\n",
      "800 0.37176144\n",
      "1000 0.2824217\n",
      "1200 0.23179695\n",
      "1400 0.21067806\n",
      "1600 0.19292715\n",
      "1800 0.17781352\n",
      "2000 0.16480358\n",
      "--------------\n",
      "[[1.6026096e-02 9.8396564e-01 8.3604127e-06]] [1]\n",
      "--------------\n",
      "[[0.77004284 0.2144872  0.01546993]] [0]\n",
      "--------------\n",
      "[[1.9288665e-08 3.7374193e-04 9.9962616e-01]] [2]\n",
      "--------------\n",
      "[[1.6026111e-02 9.8396552e-01 8.3604118e-06]\n",
      " [7.7004266e-01 2.1448739e-01 1.5469942e-02]\n",
      " [1.9288665e-08 3.7374193e-04 9.9962616e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# 여기서, 코드를 더 단순화 시키는 cost 함수 \n",
    "# cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "#                                                  labels=Y_one_hot)\n",
    "# cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(step, cost_val)\n",
    "\n",
    "    print('--------------')\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class가 3개 이상인 경우의 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n",
      "one_hot: Tensor(\"one_hot_1:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape one_hot: Tensor(\"Reshape_1:0\", shape=(?, 7), dtype=float32)\n",
      "Step:     0\tCost: 8.873\tAcc: 3.96%\n",
      "Step:   100\tCost: 0.626\tAcc: 83.17%\n",
      "Step:   200\tCost: 0.401\tAcc: 91.09%\n",
      "Step:   300\tCost: 0.297\tAcc: 91.09%\n",
      "Step:   400\tCost: 0.234\tAcc: 94.06%\n",
      "Step:   500\tCost: 0.194\tAcc: 94.06%\n",
      "Step:   600\tCost: 0.167\tAcc: 96.04%\n",
      "Step:   700\tCost: 0.146\tAcc: 98.02%\n",
      "Step:   800\tCost: 0.130\tAcc: 98.02%\n",
      "Step:   900\tCost: 0.117\tAcc: 98.02%\n",
      "Step:  1000\tCost: 0.106\tAcc: 99.01%\n",
      "Step:  1100\tCost: 0.097\tAcc: 99.01%\n",
      "Step:  1200\tCost: 0.089\tAcc: 99.01%\n",
      "Step:  1300\tCost: 0.083\tAcc: 99.01%\n",
      "Step:  1400\tCost: 0.077\tAcc: 99.01%\n",
      "Step:  1500\tCost: 0.072\tAcc: 100.00%\n",
      "Step:  1600\tCost: 0.067\tAcc: 100.00%\n",
      "Step:  1700\tCost: 0.064\tAcc: 100.00%\n",
      "Step:  1800\tCost: 0.060\tAcc: 100.00%\n",
      "Step:  1900\tCost: 0.057\tAcc: 100.00%\n",
      "Step:  2000\tCost: 0.054\tAcc: 100.00%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "'''\n",
    "(101, 16) (101, 1)\n",
    "'''\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot:\", Y_one_hot)\n",
    "\n",
    "# 위의 코드를 실행시키면 불필요한 차원이 생성되기 때문에 이를 제거하는 코드\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes]) \n",
    "print(\"reshape one_hot:\", Y_one_hot)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                 labels=tf.stop_gradient([Y_one_hot])))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "                                        \n",
    "        if step % 100 == 0:\n",
    "            print(\"Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}\".format(step, cost_val, acc_val))\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    10   11   12   13  \\\n",
       "0    1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  4.0  0.0   \n",
       "1    1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "2    0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0   \n",
       "3    1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  4.0  0.0   \n",
       "4    1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "5    1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "6    1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "7    0.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0   \n",
       "8    0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0   \n",
       "9    1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  0.0   \n",
       "10   1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "11   0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "12   0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0   \n",
       "13   0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "14   0.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   \n",
       "15   0.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  6.0  0.0   \n",
       "16   0.0  1.0  1.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "17   1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "18   0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0   \n",
       "19   0.0  0.0  0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  1.0   \n",
       "20   0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "21   0.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "22   1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "23   0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "24   0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  6.0  0.0   \n",
       "25   0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  4.0  0.0   \n",
       "26   0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  4.0  0.0   \n",
       "27   1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "28   1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "29   1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  2.0  0.0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "71   0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "72   0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  8.0  1.0   \n",
       "73   0.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0   \n",
       "74   1.0  0.0  0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0   \n",
       "75   1.0  0.0  0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  2.0  1.0   \n",
       "76   0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0  1.0   \n",
       "77   0.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "78   0.0  1.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "79   0.0  1.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "80   0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0   \n",
       "81   0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "82   0.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0   \n",
       "83   0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "84   1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "85   0.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  5.0  0.0   \n",
       "86   0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0   \n",
       "87   0.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "88   0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  6.0  0.0   \n",
       "89   0.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  0.0   \n",
       "90   0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "91   0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "92   0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0   \n",
       "93   1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "94   1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "95   0.0  1.0  1.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "96   1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "97   1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  6.0  0.0   \n",
       "98   1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  4.0  1.0   \n",
       "99   0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "100  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  2.0  1.0   \n",
       "\n",
       "      14   15   16  \n",
       "0    0.0  1.0  0.0  \n",
       "1    0.0  1.0  0.0  \n",
       "2    0.0  0.0  3.0  \n",
       "3    0.0  1.0  0.0  \n",
       "4    0.0  1.0  0.0  \n",
       "5    0.0  1.0  0.0  \n",
       "6    1.0  1.0  0.0  \n",
       "7    1.0  0.0  3.0  \n",
       "8    0.0  0.0  3.0  \n",
       "9    1.0  0.0  0.0  \n",
       "10   0.0  1.0  0.0  \n",
       "11   1.0  0.0  1.0  \n",
       "12   0.0  0.0  3.0  \n",
       "13   0.0  0.0  6.0  \n",
       "14   0.0  0.0  6.0  \n",
       "15   0.0  0.0  6.0  \n",
       "16   0.0  0.0  1.0  \n",
       "17   0.0  1.0  0.0  \n",
       "18   0.0  1.0  3.0  \n",
       "19   0.0  1.0  0.0  \n",
       "20   1.0  0.0  1.0  \n",
       "21   0.0  0.0  1.0  \n",
       "22   0.0  1.0  0.0  \n",
       "23   0.0  1.0  1.0  \n",
       "24   0.0  0.0  5.0  \n",
       "25   0.0  0.0  4.0  \n",
       "26   0.0  0.0  4.0  \n",
       "27   0.0  0.0  0.0  \n",
       "28   0.0  1.0  0.0  \n",
       "29   1.0  1.0  0.0  \n",
       "..   ...  ...  ...  \n",
       "71   0.0  1.0  1.0  \n",
       "72   0.0  0.0  6.0  \n",
       "73   0.0  0.0  3.0  \n",
       "74   0.0  1.0  0.0  \n",
       "75   0.0  1.0  0.0  \n",
       "76   0.0  0.0  2.0  \n",
       "77   0.0  0.0  6.0  \n",
       "78   0.0  0.0  1.0  \n",
       "79   0.0  0.0  1.0  \n",
       "80   0.0  0.0  2.0  \n",
       "81   0.0  0.0  6.0  \n",
       "82   0.0  0.0  3.0  \n",
       "83   0.0  0.0  1.0  \n",
       "84   0.0  0.0  0.0  \n",
       "85   0.0  0.0  6.0  \n",
       "86   0.0  1.0  3.0  \n",
       "87   0.0  1.0  1.0  \n",
       "88   0.0  0.0  5.0  \n",
       "89   0.0  0.0  4.0  \n",
       "90   0.0  1.0  2.0  \n",
       "91   0.0  0.0  2.0  \n",
       "92   0.0  1.0  3.0  \n",
       "93   0.0  0.0  0.0  \n",
       "94   0.0  0.0  0.0  \n",
       "95   0.0  1.0  1.0  \n",
       "96   0.0  1.0  0.0  \n",
       "97   0.0  0.0  5.0  \n",
       "98   0.0  1.0  0.0  \n",
       "99   0.0  0.0  6.0  \n",
       "100  0.0  0.0  1.0  \n",
       "\n",
       "[101 rows x 17 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "import pandas as pd\n",
    "bb = pd.DataFrame(aa)\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
