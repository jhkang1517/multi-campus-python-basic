{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver 채권분석리포트 Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 날짜, 증권사, 제목, pdf 다운받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPDF2나 pdfminer를 찾을 수 없다고 한다면, 아래 주석을 풀고 install을 시도하세요.\n",
    "# pip install PyPDF2 \n",
    "# pip install pdfminer.six\n",
    "\n",
    "# moduel import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib.request\n",
    "from PyPDF2 import PdfFileReader\n",
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "# 함수 정의\n",
    "def BondReportsCrawl():\n",
    "    pdf_list = [] # pdf의 날짜, 증권사, 제목을 담을 빈 리스트 생성\n",
    "    BASE_DIR = os.getcwd() # 현재 작업하고 있는 곳을 BASE_DIR로 정의\n",
    "    REPORTS_DIR = BASE_DIR # REPORTS_DIR = BASE_DIR\n",
    "    \n",
    "    # Naver의 채권분석리포트의 마지막 페이지는 116이므로 117까지 for문 돌릴 준비\n",
    "    for i in range(1, 117):\n",
    "        print(i,'번 페이지에 있는 문서를 가져오는 중입니다.') # 작업 상황 확인을 위함 \n",
    "        url = 'https://finance.naver.com/research/debenture_list.nhn?&page={}'.format(i) # page={}에 i 대입\n",
    "        response = requests.get(url) # url로 requests를 보냄 \n",
    "        soup = BeautifulSoup(response.content, 'html.parser') # soup 생성\n",
    "        reports = soup.find('table', class_='type_1').find_all('tr')[1:] # 찾아야할 content 범위 설정\n",
    "\n",
    "        # 날짜, 증권사, 제목, pdf 다운\n",
    "        for td in reports:\n",
    "            filename = \"\" # filename 을 만들 빈 문자열 생성, for문이 돌 때마다 초기화\n",
    "\n",
    "            # date 코드\n",
    "            date = td.find('td', class_='date')\n",
    "            if date is not None: # None 값들이 검사되므로 if문 추가\n",
    "                date = date.text # text만 추출\n",
    "                filename += date # filename 에 추가\n",
    "                filename += '_' # 다음에 들어올 author와의 구분을 위한 _ 추가 \n",
    "\n",
    "            # author 코드\n",
    "            author = td.find_all('td') \n",
    "            if len(author) == 5: # 길이를 확인해보니, len이 5인 것만 정상적으로 증권사 이름들이 추출됨 \n",
    "                author = author[1].text\n",
    "                filename += author\n",
    "                filename += '_'  # 다음에 들어올 title와의 구분을 위한 _ 추가 \n",
    "\n",
    "            # title 코드 \n",
    "            title = td.find('td').find('a')\n",
    "            if title is not None: # None 값들이 검사되므로 if문 추가\n",
    "                title = title.text\n",
    "                filename += title # filename 에 추가 \n",
    "                # computer가 읽을 수 없는 특수문자들을 대체 (후에 정규표현식으로 고칠 것)\n",
    "                # 공백제거 코드도 추가해 줄 것 \n",
    "                filename = filename.replace('?','').replace('.','').replace('!','').replace('/','')\n",
    "                filename = filename.replace(':','').replace(';','').replace('=','').replace('+','')\n",
    "                filename = filename.replace('-','').replace('*','').replace('\"','').replace('<','')\n",
    "                filename = filename.replace('>','').replace('[','').replace(']','')\n",
    "                filename = filename + '.pdf'\n",
    "                pdf_list.append(filename) # pdf_list 에 filename을 추가함 \n",
    "\n",
    "            # pdf 다운받기 코드\n",
    "            pdf_href = td.find('td', class_='file')\n",
    "            if pdf_href is not None: # None값이 있기 때문에 if문 추가\n",
    "                pdf_href = str(pdf_href.find('a')['href'])\n",
    "                pdf_url = pdf_href\n",
    "                print(pdf_url) # pdf_url 출력해서 제대로 된 url인지 확인\n",
    "\n",
    "                # 가장 중요한 부분. requests.get을 활용해도 pdf 파일을 다운 받을 수 있다!!! \n",
    "                r = requests.get(pdf_url) # url로 requests 보냄\n",
    "                naver_reports_path = os.path.join(REPORTS_DIR, filename) # 작업하고 있는 폴더에 파일 추가\n",
    "\n",
    "                with open(naver_reports_path, 'wb') as f: # 파일 쓰기\n",
    "                    f.write(r.content)\n",
    "\n",
    "    print(pdf_list)\n",
    "    print(\"총 가져온 pdf파일 개수: \", len(pdf_list))\n",
    "    \n",
    "BondReportsCrawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. pdf -> txt 파일로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf -> txt 로 변환하는 함수                \n",
    "def pdfparser(data):\n",
    "    ccount = 0\n",
    "    BASE_DIR = os.getcwd()\n",
    "    REPORTS_DIR = BASE_DIR\n",
    "    \n",
    "    try: # 오류가 발생하는 pdf 들이 존재하기 때문에, try문 사용\n",
    "        fp = open(data, 'rb')\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        retstr = io.StringIO()\n",
    "        codec = 'utf-8'\n",
    "        laparams = LAParams()\n",
    "        device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        for page in PDFPage.get_pages(fp):\n",
    "            interpreter.process_page(page)\n",
    "            data =  retstr.getvalue()\n",
    "            text_path = os.path.join(REPORTS_DIR, pdf_list[i]+'.txt')\n",
    "            with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(data)\n",
    "\n",
    "    except AttributeError:\n",
    "        ccount += 1\n",
    "        print(pdf_list[i],'는 Error로 인해 pass합니다.')\n",
    "        print(\"현재까지의 누적 Errors: \", ccount)\n",
    "        pass\n",
    "    \n",
    "# 이 작업을 통해, pdf_list에서 날짜, 증권사, 제목을 확인할 수 있고,\n",
    "# pdf 파일을 얻을 수 있다.\n",
    "# 또한, pdf 파일을 txt로 변환하는 함수까지 정의해 놓은 상태\n",
    "# 아래에 있는 코드를 실행하면, pdf를 txt로 변환하기 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 시간이 꽤 오래 걸립니다...!!\n",
    "# pdf -> txt 로 변환하는 code\n",
    "# 오류가 나면, range()를 수정하면서 변환\n",
    "for i in range(1, (len(pdf_list)+1)):\n",
    "    print(i)\n",
    "    print(pdf_list[i], '를 txt로 변환 중입니다...') \n",
    "    pdfparser(pdf_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "2 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "3 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "4 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "5 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "6 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "7 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "8 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "9 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "10 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "11 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "12 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "13 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "14 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "15 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "16 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "17 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "18 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "19 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "20 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "21 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "22 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "23 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "24 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "25 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "26 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "27 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "28 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "29 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "30 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "31 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "32 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "33 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "34 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "35 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "36 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "37 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "38 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "39 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "40 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "41 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "42 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "43 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "44 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "45 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "46 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "47 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "48 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "49 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "50 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "51 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "52 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "53 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "54 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "55 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "56 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "57 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "58 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "59 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "60 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "61 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "62 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "63 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "64 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "65 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "66 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "67 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "68 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "69 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "70 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "71 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "72 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "73 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "74 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "75 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "76 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "77 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "78 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "79 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "80 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "81 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "82 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "83 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "84 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "85 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "86 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "87 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "88 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "89 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "90 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "91 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "92 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "93 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "94 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "95 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "96 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "97 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "98 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "99 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "100 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "101 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "102 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "103 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "104 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "105 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "106 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "107 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "108 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "109 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "110 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "111 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "112 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "113 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "114 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "115 번 페이지에 있는 문서를 가져오는 중입니다.\n",
      "116 번 페이지에 있는 문서를 가져오는 중입니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3466"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyPDF2나 pdfminer를 찾을 수 없다고 한다면, 아래 주석을 풀고 install을 시도하세요.\n",
    "# pip install PyPDF2 \n",
    "# pip install pdfminer.six\n",
    "\n",
    "# moduel import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib.request\n",
    "from PyPDF2 import PdfFileReader\n",
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "# 함수 정의\n",
    "def BondReportsCrawl():\n",
    "    count = 0\n",
    "    pdf_list = [] # pdf의 날짜, 증권사, 제목을 담을 빈 리스트 생성\n",
    "    BASE_DIR = os.getcwd() # 현재 작업하고 있는 곳을 BASE_DIR로 정의\n",
    "    REPORTS_DIR = BASE_DIR # REPORTS_DIR = BASE_DIR\n",
    "    \n",
    "    # Naver의 채권분석리포트의 마지막 페이지는 116이므로 117까지 for문 돌릴 준비\n",
    "    for i in range(1, 117):\n",
    "        print(i,'번 페이지에 있는 문서를 가져오는 중입니다.') # 작업 상황 확인을 위함 \n",
    "        url = 'https://finance.naver.com/research/debenture_list.nhn?&page={}'.format(i) # page={}에 i 대입\n",
    "        response = requests.get(url) # url로 requests를 보냄 \n",
    "        soup = BeautifulSoup(response.content, 'html.parser') # soup 생성\n",
    "        reports = soup.find('table', class_='type_1').find_all('tr')[1:] # 찾아야할 content 범위 설정\n",
    "\n",
    "        # 날짜, 증권사, 제목, pdf 다운\n",
    "        for td in reports:\n",
    "            filename = \"\" # filename 을 만들 빈 문자열 생성, for문이 돌 때마다 초기화\n",
    "\n",
    "            # date 코드\n",
    "            date = td.find('td', class_='date')\n",
    "            if date is not None: # None 값들이 검사되므로 if문 추가\n",
    "                date = date.text # text만 추출\n",
    "                filename += date # filename 에 추가\n",
    "                filename += '_' # 다음에 들어올 author와의 구분을 위한 _ 추가\n",
    "                count += 1\n",
    "                \n",
    "    return count\n",
    "                \n",
    "BondReportsCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3466 - 3370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
