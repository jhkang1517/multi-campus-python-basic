{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/2d/d4ad0ef5159e87d66339f1dfaa4736687bf2699c3b8ce9e57998f28bb5a1/gym-0.13.0.tar.gz (1.6MB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.16.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.12.0)\n",
      "Collecting pyglet>=1.2.0 (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "Collecting cloudpickle~=1.2.0 (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/f4/4a080c349c1680a2086196fcf0286a65931708156f39568ed7051e42ff6a/cloudpickle-1.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\student\\AppData\\Local\\pip\\Cache\\wheels\\00\\b8\\b2\\e5dfb6be621560717e719734293eee9fe3c66b668fdc334b1a\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, cloudpickle, gym\n",
      "  Found existing installation: cloudpickle 0.8.0\n",
      "    Uninstalling cloudpickle-0.8.0:\n",
      "      Successfully uninstalled cloudpickle-0.8.0\n",
      "Successfully installed cloudpickle-1.2.1 gym-0.13.0 pyglet-1.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spyder 3.3.3 requires pyqt5<=5.12; python_version >= \"3\", which is not installed.\n"
     ]
    }
   ],
   "source": [
    "# https://gym.openai.com/\n",
    "# https://gym.openai.com/docs/\n",
    "try:\n",
    "    import gym\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install gym\n",
    "    import gym\n",
    "finally:\n",
    "    from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deteministic 환경."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 게임 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym 환경설정\n",
    "# https://gym.openai.com/envs/ 에 들어가면 다양한 환경을 살펴볼 수 있음\n",
    "# Frozen-Lake : https://gym.openai.com/envs/FrozenLake-v0/ \n",
    "register(\n",
    "    id = 'FrozenLake-v1', \n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv', \n",
    "    kwargs={\n",
    "        # 'is_slippery': False 이면 Deteministic 환경.\n",
    "        # 'is_slippery': True 이면 Stochastic 환경.\n",
    "        'map_name' : '4x4', # 4x4 크기의 맵\n",
    "        'is_slippery' : False # 미끄러질 가능성 없음 (나중에 다룰 것) # 불확실성 \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 사용법 익히기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render() # 환경들의 그림을 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(16), Discrete(4), 16, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space, env.observation_space.n, env.action_space.n\n",
    "# observation 관측을 하는 공간\n",
    "# action_space 내가 취할 수 있는 행동의 개수. 상 하 좌 우\n",
    "# observation_space.n\n",
    "# action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3\n",
    "\n",
    "# 임의로 방향 하나를 선택함.\n",
    "# 방향을 실행하는 것인가 선택하는 것인가?? \n",
    "action = env.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "# env.step(action) 행동을 취하겠다. 취했을때 리턴 값은 4개(상하좌우) 중 하나가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(1) # down\n",
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위치:  4\n",
      "보상:  0.0\n",
      "Game Over?:  False\n",
      "정보(확률):  {'prob': 1.0}\n",
      "행동:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"위치: \", observation)\n",
    "print(\"보상: \",reward) # goal에 들어가야 한다.\n",
    "print(\"Game Over?: \",done) \n",
    "\n",
    "# 내가 아래를 눌렀을 때 온전히 아래로 가는가?\n",
    "# 내가 의도한대로 갈 확률 \n",
    "# env에서 미끄러지는 것을 선택했을 경우 유의미 \n",
    "print(\"정보(확률): \", info)\n",
    "\n",
    "print(\"행동: \", action) # 누른 값. (이동횟수 아님!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기화\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 러닝이란  \n",
    "Q-learning에서는 앞으로 어떤 action을 취할지 결정하기 위해서 Q-table을 사용하는데, Q-table은 맵과 같은 모양  (Frozen-Lake의 경우 4x4)으로 만들어지며 각 칸에는 해당 칸에서 각각의 action을 취할 때 기대하는 reward 값으로 채워  진다. (한 state에서 취할 수 있는 action이 상하좌우 4개이기 때문에 각 칸마다 4개의 값이 필요하다)  \n",
    "\n",
    "Q-learning을 진행하면서 Q-table의 각 reward 값을 업데이트 하여 최상의 결과값을 리턴할 수 있도록 해야한다.  \n",
    "  \n",
    "Dummy Q-learning은 0으로 채워놓은 Q-테이블에서 시작해서 각 state에서 취할 action을 랜덤으로 선택한다. (상하좌우 모두 reward가 0이기 때문에)  \n",
    "  \n",
    "Frozen-Lake에서는 결승점에 도달할 때만 reward 1이 주어지기 때문에 결승점에 도착하기 전에는 reward는 무조건 0이 리턴된다  \n",
    "  \n",
    "action은 랜덤으로 정해지기 때문에 보통 구멍에 빠져서 게임이 종료되는 상황이 반복 되다가 우연히 결승점에 다다르면 그제서야 reward가 1이 주어진다.  \n",
    "  \n",
    "그럼 학습하는 agent는 이 전 칸에서 어떤 action(예를 들어 14번칸에서 오른쪽)을 하면 결승점에 다다른다는 것을 알게 된다. 그럼 이 전 칸의 Q-table을 업데이트 할 수 있다(14번칸에서 오른쪽의 값을 1로 업데이트).  \n",
    "  \n",
    "이처럼 Q-table은 우연히 결승점에 도착한 상황을 토대로 시작점까지 거꾸로 거슬러 올라가면서 Q값을 업데이트하는 방식을 사용한다. (결승점에 도착하기 위해서는 14번에서 오른쪽으로 가야한다. 14번에 다다르기 위해서는 10번에서 아래로 가야한다. 10번에 가기 위해서는 ... ) 이런 알고리즘을 식으로 표현하면 아래와 같다.  \n",
    "  \n",
    "Q[state, action] = reward + max(Q[new_state])  \n",
    "  \n",
    "<< Dummy Q-learning >>  \n",
    "2강의 Dummy Q-learning에서는 1이 채워지지 않는 칸에서 action이 랜덤으로 정해지기 때문에 갈림길의 경우에는 랜덤으로 돌아다니다가 어떤 길로 우연히 처음 들게 되었는지에 따라 Q-테이블의 모양이 달라지게 된다. 한 state의 여러 action 중에서 1 값을 갖는 건 무조건 하나일 수 밖에 없다. 일단 1이 채워지면 그 다음부터는 max값을 따라서 1이 채워진 방향으로만 지나갈 것이기 때문이다. 또한, 아무리 빠른 지름길이 있더라도, 1이 채워지지 않게 되면 그 길로 들어서지 못하게 되기 때문에 성공하더라도 최단거리일 가능성은 낮다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np      ####  http://yujuwon.tistory.com/entry/NumPy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순서\n",
    "### 1. Q table 만들기\n",
    "### 2. 게임을 진행하며 Q 테이블 업데이트 하기\n",
    "### 3. Q테이블로 테스트하기  \n",
    "page 163  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q table 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.zeros((16, 4))\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))    # (16,4) : 4*4 map + 상하좌우 4개\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습시키며 Q table 업데이트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000 # 게임을 몇 번 할 것인가?\n",
    "learning_rate = 0.3 # 새로운 Q를 학습했을 때, 그 보상의 비중\n",
    "discount_rate = 0.99 # 할인률. 과거의 데이터들이 얼마나 중요한가? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번 진행중..\n",
      "100 번 진행중..\n",
      "200 번 진행중..\n",
      "300 번 진행중..\n",
      "400 번 진행중..\n",
      "500 번 진행중..\n",
      "600 번 진행중..\n",
      "700 번 진행중..\n",
      "800 번 진행중..\n",
      "900 번 진행중..\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "for epi in range(num_episodes): # 게임 시작 \n",
    "    observation = env.reset() # 초기화\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    for t in range(100): # 100번을 이동시킨다. \n",
    "        # 1턴 실행 후의 위치\n",
    "        current_state = observation # 현재 위치 (Q table에서 행 = 위치)\n",
    "        \n",
    "        # 아래 코드는, 학습을 빨리 하기 위해서 넣는 코드\n",
    "        # 없으면 계속 LEFT로 가기 때문\n",
    "        # 전체가 0이면 처리\n",
    "        if not Q[current_state, :].any():\n",
    "            action = env.action_space.sample()\n",
    "        elif np.random.rand() < 0.5:\n",
    "            # 50 프로 확률로 랜덤 선택 \n",
    "            # 랜덤 선택 \n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # 최대값\n",
    "            action = np.argmax(Q[current_state])\n",
    "            \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 성공 사례를 만들어야 측정이 가능함.\n",
    "        # 현 위치의 최대값 = 성공 가능성\n",
    "        # 이전 위치의 현 방향 선택 = 기존의 성공 가능성\n",
    "        Q[current_state, action] += \\\n",
    "            learning_rate * \\\n",
    "            (reward + discount_rate * np.max(Q[observation, :]) \\\n",
    "            - Q[current_state, action]) \n",
    "        # current_state 현위치 에서 내가 어떤 action을 했는가\n",
    "        \n",
    "        if done:\n",
    "            episode_reward +=reward\n",
    "            break\n",
    "    if epi % 100 == 0:\n",
    "        print(epi, \"번 진행중..\")\n",
    "    total_reward += episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.356"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward/ num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94148015, 0.95099005, 0.95099005, 0.94148015],\n",
       "       [0.94148015, 0.        , 0.96059601, 0.95099005],\n",
       "       [0.95099005, 0.970299  , 0.95099005, 0.96059601],\n",
       "       [0.96059601, 0.        , 0.95026009, 0.95027354],\n",
       "       [0.95099004, 0.96059601, 0.        , 0.94148015],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9801    , 0.        , 0.96059601],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.96059596, 0.        , 0.970299  , 0.95098995],\n",
       "       [0.960596  , 0.97987772, 0.9801    , 0.        ],\n",
       "       [0.970299  , 0.99      , 0.        , 0.970299  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.97632505, 0.98999985, 0.970299  ],\n",
       "       [0.98009834, 0.99      , 1.        , 0.9801    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습이 끝난 Q값\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q테이블로 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0.0\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    for t in range(100):\n",
    "        #1턴 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # Q값이 최대가 되는 행동을 선택함\n",
    "        action = np.argmax(Q[current_state])\n",
    "        \n",
    "        # 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #종료\n",
    "        if done:\n",
    "            episode_reward += reward\n",
    "            \n",
    "    # 총 reward\n",
    "    total_reward += episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "# 어떻게 100% 정확도를 얻게 되었나? \n",
    "# 학습이 된 대로만 행동하면 무조건 reward를 가지게 된다. \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic 환경."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym 환경설정\n",
    "# https://gym.openai.com/envs/ 에 들어가면 다양한 환경을 살펴볼 수 있음\n",
    "# Frozen-Lake : https://gym.openai.com/envs/FrozenLake-v0/ \n",
    "register(\n",
    "    id = 'FrozenLake-v2', \n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv', \n",
    "    kwargs={\n",
    "        # 'is_slippery': False 이면 Deteministic 환경.\n",
    "        # 'is_slippery': True 이면 Stochastic 환경.\n",
    "        'map_name' : '4x4', # 4x4 크기의 맵\n",
    "        'is_slippery' : True \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v2\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위치:  1\n",
      "보상:  0.0\n",
      "Game Over?:  False\n",
      "정보(확률):  {'prob': 0.3333333333333333}\n",
      "행동:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"위치: \", observation)\n",
    "print(\"보상: \",reward)\n",
    "print(\"Game Over?: \",done)\n",
    "print(\"정보(확률): \", info) # 내가 의도한대로 갈 확률, 즉 66.6 % 확률로 다른곳으로 간다. \n",
    "print(\"행동: \", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순서\n",
    "#### 1. Q table 만들기\n",
    "#### 2. 게임을 진행하며 Q 테이블 업데이트 하기\n",
    "#### 3. Q테이블로 테스트하기  \n",
    "page 163 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q table 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.zeros((16, 4))\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))    # (16,4) : 4*4 map + 상하좌우 4개\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습시키며 Q table 업데이트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 30000\n",
    "learning_rate = 0.3\n",
    "discount_rate = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번 진행중..\n",
      "100 번 진행중..\n",
      "200 번 진행중..\n",
      "300 번 진행중..\n",
      "400 번 진행중..\n",
      "500 번 진행중..\n",
      "600 번 진행중..\n",
      "700 번 진행중..\n",
      "800 번 진행중..\n",
      "900 번 진행중..\n",
      "1000 번 진행중..\n",
      "1100 번 진행중..\n",
      "1200 번 진행중..\n",
      "1300 번 진행중..\n",
      "1400 번 진행중..\n",
      "1500 번 진행중..\n",
      "1600 번 진행중..\n",
      "1700 번 진행중..\n",
      "1800 번 진행중..\n",
      "1900 번 진행중..\n",
      "2000 번 진행중..\n",
      "2100 번 진행중..\n",
      "2200 번 진행중..\n",
      "2300 번 진행중..\n",
      "2400 번 진행중..\n",
      "2500 번 진행중..\n",
      "2600 번 진행중..\n",
      "2700 번 진행중..\n",
      "2800 번 진행중..\n",
      "2900 번 진행중..\n",
      "3000 번 진행중..\n",
      "3100 번 진행중..\n",
      "3200 번 진행중..\n",
      "3300 번 진행중..\n",
      "3400 번 진행중..\n",
      "3500 번 진행중..\n",
      "3600 번 진행중..\n",
      "3700 번 진행중..\n",
      "3800 번 진행중..\n",
      "3900 번 진행중..\n",
      "4000 번 진행중..\n",
      "4100 번 진행중..\n",
      "4200 번 진행중..\n",
      "4300 번 진행중..\n",
      "4400 번 진행중..\n",
      "4500 번 진행중..\n",
      "4600 번 진행중..\n",
      "4700 번 진행중..\n",
      "4800 번 진행중..\n",
      "4900 번 진행중..\n",
      "5000 번 진행중..\n",
      "5100 번 진행중..\n",
      "5200 번 진행중..\n",
      "5300 번 진행중..\n",
      "5400 번 진행중..\n",
      "5500 번 진행중..\n",
      "5600 번 진행중..\n",
      "5700 번 진행중..\n",
      "5800 번 진행중..\n",
      "5900 번 진행중..\n",
      "6000 번 진행중..\n",
      "6100 번 진행중..\n",
      "6200 번 진행중..\n",
      "6300 번 진행중..\n",
      "6400 번 진행중..\n",
      "6500 번 진행중..\n",
      "6600 번 진행중..\n",
      "6700 번 진행중..\n",
      "6800 번 진행중..\n",
      "6900 번 진행중..\n",
      "7000 번 진행중..\n",
      "7100 번 진행중..\n",
      "7200 번 진행중..\n",
      "7300 번 진행중..\n",
      "7400 번 진행중..\n",
      "7500 번 진행중..\n",
      "7600 번 진행중..\n",
      "7700 번 진행중..\n",
      "7800 번 진행중..\n",
      "7900 번 진행중..\n",
      "8000 번 진행중..\n",
      "8100 번 진행중..\n",
      "8200 번 진행중..\n",
      "8300 번 진행중..\n",
      "8400 번 진행중..\n",
      "8500 번 진행중..\n",
      "8600 번 진행중..\n",
      "8700 번 진행중..\n",
      "8800 번 진행중..\n",
      "8900 번 진행중..\n",
      "9000 번 진행중..\n",
      "9100 번 진행중..\n",
      "9200 번 진행중..\n",
      "9300 번 진행중..\n",
      "9400 번 진행중..\n",
      "9500 번 진행중..\n",
      "9600 번 진행중..\n",
      "9700 번 진행중..\n",
      "9800 번 진행중..\n",
      "9900 번 진행중..\n",
      "10000 번 진행중..\n",
      "10100 번 진행중..\n",
      "10200 번 진행중..\n",
      "10300 번 진행중..\n",
      "10400 번 진행중..\n",
      "10500 번 진행중..\n",
      "10600 번 진행중..\n",
      "10700 번 진행중..\n",
      "10800 번 진행중..\n",
      "10900 번 진행중..\n",
      "11000 번 진행중..\n",
      "11100 번 진행중..\n",
      "11200 번 진행중..\n",
      "11300 번 진행중..\n",
      "11400 번 진행중..\n",
      "11500 번 진행중..\n",
      "11600 번 진행중..\n",
      "11700 번 진행중..\n",
      "11800 번 진행중..\n",
      "11900 번 진행중..\n",
      "12000 번 진행중..\n",
      "12100 번 진행중..\n",
      "12200 번 진행중..\n",
      "12300 번 진행중..\n",
      "12400 번 진행중..\n",
      "12500 번 진행중..\n",
      "12600 번 진행중..\n",
      "12700 번 진행중..\n",
      "12800 번 진행중..\n",
      "12900 번 진행중..\n",
      "13000 번 진행중..\n",
      "13100 번 진행중..\n",
      "13200 번 진행중..\n",
      "13300 번 진행중..\n",
      "13400 번 진행중..\n",
      "13500 번 진행중..\n",
      "13600 번 진행중..\n",
      "13700 번 진행중..\n",
      "13800 번 진행중..\n",
      "13900 번 진행중..\n",
      "14000 번 진행중..\n",
      "14100 번 진행중..\n",
      "14200 번 진행중..\n",
      "14300 번 진행중..\n",
      "14400 번 진행중..\n",
      "14500 번 진행중..\n",
      "14600 번 진행중..\n",
      "14700 번 진행중..\n",
      "14800 번 진행중..\n",
      "14900 번 진행중..\n",
      "15000 번 진행중..\n",
      "15100 번 진행중..\n",
      "15200 번 진행중..\n",
      "15300 번 진행중..\n",
      "15400 번 진행중..\n",
      "15500 번 진행중..\n",
      "15600 번 진행중..\n",
      "15700 번 진행중..\n",
      "15800 번 진행중..\n",
      "15900 번 진행중..\n",
      "16000 번 진행중..\n",
      "16100 번 진행중..\n",
      "16200 번 진행중..\n",
      "16300 번 진행중..\n",
      "16400 번 진행중..\n",
      "16500 번 진행중..\n",
      "16600 번 진행중..\n",
      "16700 번 진행중..\n",
      "16800 번 진행중..\n",
      "16900 번 진행중..\n",
      "17000 번 진행중..\n",
      "17100 번 진행중..\n",
      "17200 번 진행중..\n",
      "17300 번 진행중..\n",
      "17400 번 진행중..\n",
      "17500 번 진행중..\n",
      "17600 번 진행중..\n",
      "17700 번 진행중..\n",
      "17800 번 진행중..\n",
      "17900 번 진행중..\n",
      "18000 번 진행중..\n",
      "18100 번 진행중..\n",
      "18200 번 진행중..\n",
      "18300 번 진행중..\n",
      "18400 번 진행중..\n",
      "18500 번 진행중..\n",
      "18600 번 진행중..\n",
      "18700 번 진행중..\n",
      "18800 번 진행중..\n",
      "18900 번 진행중..\n",
      "19000 번 진행중..\n",
      "19100 번 진행중..\n",
      "19200 번 진행중..\n",
      "19300 번 진행중..\n",
      "19400 번 진행중..\n",
      "19500 번 진행중..\n",
      "19600 번 진행중..\n",
      "19700 번 진행중..\n",
      "19800 번 진행중..\n",
      "19900 번 진행중..\n",
      "20000 번 진행중..\n",
      "20100 번 진행중..\n",
      "20200 번 진행중..\n",
      "20300 번 진행중..\n",
      "20400 번 진행중..\n",
      "20500 번 진행중..\n",
      "20600 번 진행중..\n",
      "20700 번 진행중..\n",
      "20800 번 진행중..\n",
      "20900 번 진행중..\n",
      "21000 번 진행중..\n",
      "21100 번 진행중..\n",
      "21200 번 진행중..\n",
      "21300 번 진행중..\n",
      "21400 번 진행중..\n",
      "21500 번 진행중..\n",
      "21600 번 진행중..\n",
      "21700 번 진행중..\n",
      "21800 번 진행중..\n",
      "21900 번 진행중..\n",
      "22000 번 진행중..\n",
      "22100 번 진행중..\n",
      "22200 번 진행중..\n",
      "22300 번 진행중..\n",
      "22400 번 진행중..\n",
      "22500 번 진행중..\n",
      "22600 번 진행중..\n",
      "22700 번 진행중..\n",
      "22800 번 진행중..\n",
      "22900 번 진행중..\n",
      "23000 번 진행중..\n",
      "23100 번 진행중..\n",
      "23200 번 진행중..\n",
      "23300 번 진행중..\n",
      "23400 번 진행중..\n",
      "23500 번 진행중..\n",
      "23600 번 진행중..\n",
      "23700 번 진행중..\n",
      "23800 번 진행중..\n",
      "23900 번 진행중..\n",
      "24000 번 진행중..\n",
      "24100 번 진행중..\n",
      "24200 번 진행중..\n",
      "24300 번 진행중..\n",
      "24400 번 진행중..\n",
      "24500 번 진행중..\n",
      "24600 번 진행중..\n",
      "24700 번 진행중..\n",
      "24800 번 진행중..\n",
      "24900 번 진행중..\n",
      "25000 번 진행중..\n",
      "25100 번 진행중..\n",
      "25200 번 진행중..\n",
      "25300 번 진행중..\n",
      "25400 번 진행중..\n",
      "25500 번 진행중..\n",
      "25600 번 진행중..\n",
      "25700 번 진행중..\n",
      "25800 번 진행중..\n",
      "25900 번 진행중..\n",
      "26000 번 진행중..\n",
      "26100 번 진행중..\n",
      "26200 번 진행중..\n",
      "26300 번 진행중..\n",
      "26400 번 진행중..\n",
      "26500 번 진행중..\n",
      "26600 번 진행중..\n",
      "26700 번 진행중..\n",
      "26800 번 진행중..\n",
      "26900 번 진행중..\n",
      "27000 번 진행중..\n",
      "27100 번 진행중..\n",
      "27200 번 진행중..\n",
      "27300 번 진행중..\n",
      "27400 번 진행중..\n",
      "27500 번 진행중..\n",
      "27600 번 진행중..\n",
      "27700 번 진행중..\n",
      "27800 번 진행중..\n",
      "27900 번 진행중..\n",
      "28000 번 진행중..\n",
      "28100 번 진행중..\n",
      "28200 번 진행중..\n",
      "28300 번 진행중..\n",
      "28400 번 진행중..\n",
      "28500 번 진행중..\n",
      "28600 번 진행중..\n",
      "28700 번 진행중..\n",
      "28800 번 진행중..\n",
      "28900 번 진행중..\n",
      "29000 번 진행중..\n",
      "29100 번 진행중..\n",
      "29200 번 진행중..\n",
      "29300 번 진행중..\n",
      "29400 번 진행중..\n",
      "29500 번 진행중..\n",
      "29600 번 진행중..\n",
      "29700 번 진행중..\n",
      "29800 번 진행중..\n",
      "29900 번 진행중..\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "recent_reward = []\n",
    "recent_size = 100\n",
    "\n",
    "for epi in range(num_episodes):\n",
    "    observation = env.reset() # 초기화\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    for t in range(100):\n",
    "        # 1턴 실행 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # 학습을 빨리 하기 위해서 넣는 코드\n",
    "        # 없으면 계속 LEFT로 가기 때문\n",
    "        # 전체가 0이면 처리\n",
    "        \n",
    "        # if not Q[current_state, :].any(): # 지멋대로 어쨋든 가긴 가니까 코드 삭제 \n",
    "        # action = env.action_space.sample()\n",
    "        if np.random.rand() < 0.1: # 변경 된걸 확인하세요.\n",
    "            # 랜덤 선택\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # 최대값\n",
    "            action = np.argmax(Q[current_state])\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 성공 사례를 만들어야 측정이 가능함.\n",
    "        # 현 위치의 최대값 = 성공 가능성\n",
    "        # 이전 위치의 현 방향 선택 = 기존의 성공 가능성\n",
    "        Q[current_state, action] += learning_rate * (reward + discount_rate * np.max(Q[observation, :]) - Q[current_state, action]) \n",
    "        # Q 값은 어떻게 주어지는가\n",
    "        # reward: 보상 (맨 끝에서만 일어난다.)\n",
    "        # discount_rate: 할인률\n",
    "        # np.max(Q[observation, :]\n",
    "        # reward가 1이 되면, np.max(Q[observation, :] 는 무조건 0이 된다. \n",
    "        # - Q[current_state, action] 뭔소리야 ? \n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            episode_reward +=reward\n",
    "            break\n",
    "    if epi % 100 == 0:\n",
    "        print(epi, \"번 진행중..\")\n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    # 이 코드는, 70%가 넘으면 멈추겠다는 의미 \n",
    "    recent_reward.append(episode_reward)\n",
    "    if len(recent_reward) >= recent_size:\n",
    "        # 최근 100개에 대해서 검사\n",
    "        recent_reward.pop(0)\n",
    "        \n",
    "        if sum(recent_reward) / recent_size > 0.7:\n",
    "            print(\"0.7이 넘었습니다. 멈추겠습니다.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.342"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward/ num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recent_reward)/recent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54301267, 0.50593244, 0.49871917, 0.49364589],\n",
       "       [0.1979607 , 0.37168197, 0.34169564, 0.49799239],\n",
       "       [0.42360146, 0.4127199 , 0.42242956, 0.43898857],\n",
       "       [0.27695586, 0.19448212, 0.40069606, 0.42595158],\n",
       "       [0.58250166, 0.41228543, 0.30495981, 0.48256757],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.14200955, 0.18966245, 0.24753289, 0.05300664],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.31265419, 0.34509693, 0.26844059, 0.63366763],\n",
       "       [0.20446592, 0.7610866 , 0.51132149, 0.30081073],\n",
       "       [0.63335492, 0.41731596, 0.13508941, 0.25412394],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.41929668, 0.30103607, 0.87217585, 0.53579807],\n",
       "       [0.74319912, 0.9227424 , 0.81274236, 0.7135945 ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습이 끝난 Q값\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q테이블로 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0.0\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    for t in range(100):\n",
    "        #1턴 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # Q값이 최대가 되는 행동을 선택함\n",
    "        action = np.argmax(Q[current_state])\n",
    "        \n",
    "        # 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #종료\n",
    "        if done:\n",
    "            episode_reward += reward\n",
    "            \n",
    "    # 총 reward\n",
    "    total_reward += episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752.0\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정리\n",
    "\"\"\"\n",
    "reward와 panaty를 활용하여, 학습을 시키는 것이 강화학습의 핵심\n",
    "어느정도의 확률수치를 주느냐에 따라서, 학습률이 달라진다.\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN ( p.167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 환경생성하기 \n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 신경망 구성 p.169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 96)\n",
    "        self.fc4 = nn.Linear(96, 96)\n",
    "        self.fc5 = nn.Linear(96, 64)\n",
    "        self.fc6 = nn.Linear(64, 64)\n",
    "        self.fc7 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onehot2tensor, applymodel p. 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2tensor(state):\n",
    "    tmp = np.zeros(16)\n",
    "    tmp[state] = 1\n",
    "    vector = np.array(tmp, dtype='float32')\n",
    "    tensor = torch.from_numpy(vector).float()\n",
    "    return tensor\n",
    "\n",
    "def applymodel(tensor):\n",
    "    output_tensor = model(tensor)\n",
    "    output_array = output_tensor.data.numpy()\n",
    "    return output_tensor, output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "for x in range(16):\n",
    "    print(onehot2tensor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화(학습) - (p.171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss가 수렴하지 않음 (책)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.8150, 1.8779, 1.7383, 1.8468], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.8361, 1.8867, 1.7489, 1.8695], grad_fn=<AddBackward0>)\n",
      "500번: total_loss: 0.00044148175220470876, total_reward: 18.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.0356, 2.0586, 2.0820, 2.0274], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.0371, 2.0599, 2.0842, 2.0286], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.0872, 2.0881, 2.1175, 2.0889], grad_fn=<AddBackward0>)\n",
      "1000번: total_loss: 0.0004525707645370858, total_reward: 36.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.6658, 1.7935, 1.5775, 1.5948], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.6819, 1.8147, 1.5910, 1.6094], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.6568, 1.7842, 1.5643, 1.5817], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.6691, 1.8018, 1.5753, 1.5920], grad_fn=<AddBackward0>)\n",
      "1500번: total_loss: 0.008905901437628927, total_reward: 52.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.9318, 1.0107, 0.9586, 0.8495], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.9173, 1.0015, 0.9471, 0.8288], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.9323, 1.0129, 0.9569, 0.8461], grad_fn=<AddBackward0>)\n",
      "2000번: total_loss: 0.00035030061917495914, total_reward: 62.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0950, 1.1005, 1.1032, 1.0470], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.1073, 1.1155, 1.1179, 1.0579], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.1066, 1.1146, 1.1174, 1.0572], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.1052, 1.1131, 1.1161, 1.0556], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0919, 1.1041, 1.1062, 1.0375], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0851, 1.0944, 1.0970, 1.0324], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0811, 1.0906, 1.0929, 1.0279], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0766, 1.0865, 1.0883, 1.0229], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0718, 1.0821, 1.0832, 1.0175], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0667, 1.0774, 1.0778, 1.0116], grad_fn=<AddBackward0>)\n",
      "2500번: total_loss: 0.00034793094027918414, total_reward: 79.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.6488, 1.7090, 1.6368, 1.6714], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.6746, 1.7285, 1.6529, 1.7062], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.6903, 1.7555, 1.6749, 1.7185], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.7613, 1.8342, 1.7467, 1.7943], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.7617, 1.8508, 1.7585, 1.7864], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.8027, 1.8845, 1.7893, 1.8383], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([1.7963, 1.8520, 1.7592, 1.8485], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([1.8040, 1.8609, 1.7662, 1.8572], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([2.0867, 2.1682, 2.0544, 2.1590], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([2.1036, 2.1878, 2.0699, 2.1781], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([1.8367, 1.8985, 1.7963, 1.8943], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([1.8359, 1.8974, 1.7955, 1.8934], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([1.8344, 1.8955, 1.7941, 1.8916], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([2.1049, 2.1895, 2.0708, 2.1797], grad_fn=<AddBackward0>)\n",
      "3000번: total_loss: 0.18040858659912828, total_reward: 104.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.8244, 1.8569, 1.8073, 1.8292], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.8678, 1.9038, 1.8519, 1.8773], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.8316, 1.8781, 1.8245, 1.8352], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.8385, 1.8831, 1.8290, 1.8411], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.8419, 1.8845, 1.8303, 1.8433], grad_fn=<AddBackward0>)\n",
      "3500번: total_loss: 0.0013046396561549045, total_reward: 110.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.5439, 1.5320, 1.5588, 1.5238], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.5385, 1.5271, 1.5526, 1.5181], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.5508, 1.5419, 1.5652, 1.5301], grad_fn=<AddBackward0>)\n",
      "4000번: total_loss: 0.0005326894843165064, total_reward: 138.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.5059, 2.5591, 2.5048, 2.5073], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.5323, 2.5834, 2.5316, 2.5336], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.5125, 2.5608, 2.5120, 2.5123], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.5696, 2.6175, 2.5700, 2.5701], grad_fn=<AddBackward0>)\n",
      "4500번: total_loss: 0.0028771466822945513, total_reward: 163.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.0178, 2.0380, 2.0163, 2.0066], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.0514, 2.0697, 2.0491, 2.0414], grad_fn=<AddBackward0>)\n",
      "5000번: total_loss: 0.0003062649002458784, total_reward: 182.0 \n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "num_episodes = 5000\n",
    "discount_rate = 0.99\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # 현재 게임 reward\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    # 누적 오차\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for t in range(100):\n",
    "        current_state = observation\n",
    "        \n",
    "        # 경사 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 현위치 onehot encoding \n",
    "        current_tensor = onehot2tensor(current_state)\n",
    "\n",
    "        if (i_episode+1) % 500 == 0:\n",
    "            print(current_tensor)\n",
    "            current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "            print(current_output_tensor)\n",
    "            # print(\"{}번: total_loss: {}, total_reward: {} \".format(i_episode+1, total_loss, total_reward))\n",
    "            \n",
    "            \n",
    "        # 모형에 입력\n",
    "        current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "        # 행동 선택\n",
    "        if np.random.rand() < 0.1:\n",
    "            #무작위 선택\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Q값이 최대가 되도록\n",
    "            action = np.argmax(current_output_array)\n",
    "\n",
    "        ## 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # onehot 벡터 텐서로 변환\n",
    "        observation_tensor = onehot2tensor(observation)\n",
    "        \n",
    "        # 모형에 입력\n",
    "        observation_output_tensor, observation_output_array =\\\n",
    "        applymodel(observation_tensor)\n",
    "        \n",
    "        # Q값 업데이트\n",
    "        # y값\n",
    "        q = reward + discount_rate * np.max(observation_output_array) # reward는 마지막 value를 위해 더함\n",
    "        # np.max(observation_output_array): 각각의 연산 결과에 대한 값중에서 가장 큰 값(max)를 가져온다. \n",
    "        \n",
    "        # 기존 보드의 q값 복사\n",
    "        q_array = np.copy(current_output_array) # 이전 값\n",
    "        q_array[action] = q\n",
    "        q_variable = torch.from_numpy(q_array)\n",
    "        \n",
    "        # 오차 계산 - q_array와 q\n",
    "        # 현재 아웃풋 값과, 기존 q어레이\n",
    "        # y값은 올바른 선택일 때의 최대값,\n",
    "        # q^은 현재 선택의 결과\n",
    "        \n",
    "        # current_output_tensor: 현재 측정값\n",
    "        # q_variable: 최고의 선택을 했을 때 y값\n",
    "        loss = criterion(current_output_tensor, q_variable)\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        \n",
    "        # 웨이트 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 오차 누적 계산\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if done:\n",
    "            # 종료\n",
    "            episode_reward += reward\n",
    "            \n",
    "            break\n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    # 누적오차 및 보상률\n",
    "    if (i_episode+1) % 500 == 0:\n",
    "        print(\"{}번: total_loss: {}, total_reward: {} \".format(i_episode+1, total_loss, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q러닝에 인공신경망을 접목하면, 강화학습에는 정답이 없는데 어떻게 인공신경망에 있는 것처럼 정해진 답과 예측값의 차이인 \n",
    "# 오차를 알아내는가? 에 대한 의문이 남는다. \n",
    "# 이에대한 해답은, 이동했을때의 기대값을 정답으로 한다는 것이다. \n",
    "# 이동했을대의 기대값과 현재의 기대값의 차이가 적다면, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQL 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500번:\n",
      "26.0\n",
      "------------------------------\n",
      "1000번:\n",
      "46.0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "num_episodes = 1000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # 현재 게임 reward\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    \n",
    "    for t in range(100):\n",
    "        current_state = observation\n",
    "        \n",
    "        \n",
    "        # 현위치 onehot encoding \n",
    "        current_tensor = onehot2tensor(current_state)\n",
    "        # 모형에 입력\n",
    "        current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "        \n",
    "        # Q테이블에서 제일 큰거 선택\n",
    "        action = np.argmax(current_output_array)\n",
    "\n",
    "        ## 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # 종료\n",
    "            episode_reward += reward\n",
    "            break\n",
    "            \n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    # 누적오차 및 보상률\n",
    "    if (i_episode+1) % 500 == 0:\n",
    "        print(\"{}번:\".format(i_episode+1))\n",
    "        print(total_reward)\n",
    "        print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.0\n",
      "0.046\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)  \n",
    "print(total_reward/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
