{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data#45dd5dd61f25'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "eng_news = soup.select('p') #[class=\"speakable-paragraph\"]')\n",
    "eng_text = eng_news[3].get_text()\n",
    "\n",
    "eng_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 영문 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세가지의 단어토큰화 방법 \n",
    "word_tokenize  \n",
    "WordPunctTokenizer  \n",
    "TreebankWordTokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize(): 마침표와 구두점(온점(.), 콤마(,), 물음표(?), 세미콜론(;)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "token1 = word_tokenize('Barack Obama likes fried chicken very much')\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "token1 = word_tokenize(eng_text)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordPunctTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'\", 's', 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i', '.', 'e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer(): 알파벳과, 알파벳이 아닌 문자를 구분하여 토큰화\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "token2 = WordPunctTokenizer().tokenize(eng_text)\n",
    "print(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `TreebankWordTokenizer` not found.\n"
     ]
    }
   ],
   "source": [
    "TreebankWordTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e.', 'every', 'job.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "# TreebankWordTokenizer() : 정규 표현식에 기반한 토큰화\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "token3 = TreebankWordTokenizer().tokenize(eng_text)\n",
    "print(token3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. 영문 품사 부착 (PoS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('yes', 'UH'), (',', ','), ('she', 'PRP'), ('does', 'VBZ'), ('mean', 'VB'), ('everybody', 'NN'), (\"'s\", 'POS'), ('job', 'NN'), ('from', 'IN'), ('yours', 'NNS'), ('to', 'TO'), ('mine', 'VB'), ('and', 'CC'), ('onward', 'VB'), ('to', 'TO'), ('the', 'DT'), ('role', 'NN'), ('of', 'IN'), ('grain', 'NN'), ('farmers', 'NNS'), ('in', 'IN'), ('Egypt', 'NNP'), (',', ','), ('pastry', 'NN'), ('chefs', 'NNS'), ('in', 'IN'), ('Paris', 'NNP'), ('and', 'CC'), ('dog', 'NN'), ('walkers', 'NNS'), ('in', 'IN'), ('Oregon', 'NNP'), ('i.e', 'NN'), ('.', '.'), ('every', 'DT'), ('job', 'NN'), ('.', '.'), ('We', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('help', 'VB'), ('direct', 'VB'), ('all', 'DT'), ('workers', 'NNS'), ('’', 'VBP'), ('actions', 'NNS'), ('and', 'CC'), ('behavior', 'NN'), ('with', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('degree', 'NN'), ('of', 'IN'), ('intelligence', 'NN'), ('that', 'WDT'), ('comes', 'VBZ'), ('from', 'IN'), ('predictive', 'JJ'), ('analytics', 'NNS'), (',', ','), ('all', 'DT'), ('stemming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('engines', 'VBZ'), ('we', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('increasingly', 'RB'), ('depend', 'VBP'), ('upon', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "taggedToken = pos_tag(token1)\n",
    "print(taggedToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CC: 접속사  \n",
    "UH:  \n",
    "PRP:  \n",
    "VBZ:  \n",
    "NN:  \n",
    "POS:  \n",
    "IN:  \n",
    "NNS:  \n",
    "TO:  \n",
    "VB:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. 개체명 인식 (NER, Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  And/CC\n",
      "  yes/UH\n",
      "  ,/,\n",
      "  she/PRP\n",
      "  does/VBZ\n",
      "  mean/VB\n",
      "  everybody/NN\n",
      "  's/POS\n",
      "  job/NN\n",
      "  from/IN\n",
      "  yours/NNS\n",
      "  to/TO\n",
      "  mine/VB\n",
      "  and/CC\n",
      "  onward/VB\n",
      "  to/TO\n",
      "  the/DT\n",
      "  role/NN\n",
      "  of/IN\n",
      "  grain/NN\n",
      "  farmers/NNS\n",
      "  in/IN\n",
      "  (GPE Egypt/NNP)\n",
      "  ,/,\n",
      "  pastry/NN\n",
      "  chefs/NNS\n",
      "  in/IN\n",
      "  (GPE Paris/NNP)\n",
      "  and/CC\n",
      "  dog/NN\n",
      "  walkers/NNS\n",
      "  in/IN\n",
      "  (GPE Oregon/NNP)\n",
      "  i.e/NN\n",
      "  ./.\n",
      "  every/DT\n",
      "  job/NN\n",
      "  ./.\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  now/RB\n",
      "  be/VB\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  help/VB\n",
      "  direct/VB\n",
      "  all/DT\n",
      "  workers/NNS\n",
      "  ’/VBP\n",
      "  actions/NNS\n",
      "  and/CC\n",
      "  behavior/NN\n",
      "  with/IN\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  degree/NN\n",
      "  of/IN\n",
      "  intelligence/NN\n",
      "  that/WDT\n",
      "  comes/VBZ\n",
      "  from/IN\n",
      "  predictive/JJ\n",
      "  analytics/NNS\n",
      "  ,/,\n",
      "  all/DT\n",
      "  stemming/VBG\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION AI/NNP)\n",
      "  engines/VBZ\n",
      "  we/PRP\n",
      "  will/MD\n",
      "  now/RB\n",
      "  increasingly/RB\n",
      "  depend/VBP\n",
      "  upon/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "neToken = ne_chunk(taggedToken)\n",
    "print(neToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-5. 원형 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5-1. 어간추출\n",
    "* 규칙에 기반하여 토큰을 표준화  \n",
    "* ning제거, ful 제거 등 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n",
      "beautiful -> beauti\n",
      "believes -> believ\n",
      "studies -> studi\n",
      "using - > use\n",
      "conversation -> convers\n",
      "organization -> organ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"running -> \" + ps.stem(\"running\")) # ning \n",
    "print(\"beautiful -> \" + ps.stem(\"beautiful\")) # ful\n",
    "print(\"believes -> \" + ps.stem(\"believes\")) # es\n",
    "print(\"studies -> \" + ps.stem(\"studies\"))  # es\n",
    "print(\"using - > \" + ps.stem(\"using\")) # ing > e\n",
    "print(\"conversation -> \" + ps.stem(\"conversation\")) # ation\n",
    "print(\"organization -> \" + ps.stem(\"organization\")) # ation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5-2. 표제어추출 (Lemmatization)\n",
    "* 품사정보를 보존하여 토큰을 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> running\n",
      "beautiful -> beatiful\n",
      "believes -> belief\n",
      "using -> using\n",
      "conversation -> conversation\n",
      "organization -> organization\n",
      "studies -> study\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
    "print(\"beautiful -> \" + wl.lemmatize(\"beatiful\"))\n",
    "print(\"believes -> \" + wl.lemmatize(\"believes\"))\n",
    "print(\"using -> \" + wl.lemmatize(\"using\"))\n",
    "print(\"conversation -> \" + wl.lemmatize(\"conversation\"))\n",
    "print(\"organization -> \" + wl.lemmatize(\"organization\"))\n",
    "print(\"studies -> \" + wl.lemmatize(\"studies\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6. 불용어 처리 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', ','), 3),\n",
       " (('from', 'IN'), 3),\n",
       " (('to', 'TO'), 3),\n",
       " (('and', 'CC'), 3),\n",
       " (('in', 'IN'), 3),\n",
       " (('.', '.'), 3),\n",
       " (('job', 'NN'), 2),\n",
       " (('the', 'DT'), 2),\n",
       " (('of', 'IN'), 2),\n",
       " (('will', 'MD'), 2),\n",
       " (('now', 'RB'), 2),\n",
       " (('all', 'DT'), 2),\n",
       " (('And', 'CC'), 1),\n",
       " (('yes', 'UH'), 1),\n",
       " (('she', 'PRP'), 1),\n",
       " (('does', 'VBZ'), 1),\n",
       " (('mean', 'VB'), 1),\n",
       " (('everybody', 'NN'), 1),\n",
       " ((\"'s\", 'POS'), 1),\n",
       " (('yours', 'NNS'), 1),\n",
       " (('mine', 'VB'), 1),\n",
       " (('onward', 'VB'), 1),\n",
       " (('role', 'NN'), 1),\n",
       " (('grain', 'NN'), 1),\n",
       " (('farmers', 'NNS'), 1),\n",
       " (('Egypt', 'NNP'), 1),\n",
       " (('pastry', 'NN'), 1),\n",
       " (('chefs', 'NNS'), 1),\n",
       " (('Paris', 'NNP'), 1),\n",
       " (('dog', 'NN'), 1),\n",
       " (('walkers', 'NNS'), 1),\n",
       " (('Oregon', 'NNP'), 1),\n",
       " (('i.e', 'NN'), 1),\n",
       " (('every', 'DT'), 1),\n",
       " (('We', 'PRP'), 1),\n",
       " (('be', 'VB'), 1),\n",
       " (('able', 'JJ'), 1),\n",
       " (('help', 'VB'), 1),\n",
       " (('direct', 'VB'), 1),\n",
       " (('workers', 'NNS'), 1),\n",
       " (('’', 'VBP'), 1),\n",
       " (('actions', 'NNS'), 1),\n",
       " (('behavior', 'NN'), 1),\n",
       " (('with', 'IN'), 1),\n",
       " (('a', 'DT'), 1),\n",
       " (('new', 'JJ'), 1),\n",
       " (('degree', 'NN'), 1),\n",
       " (('intelligence', 'NN'), 1),\n",
       " (('that', 'WDT'), 1),\n",
       " (('comes', 'VBZ'), 1),\n",
       " (('predictive', 'JJ'), 1),\n",
       " (('analytics', 'NNS'), 1),\n",
       " (('stemming', 'VBG'), 1),\n",
       " (('AI', 'NNP'), 1),\n",
       " (('engines', 'VBZ'), 1),\n",
       " (('we', 'PRP'), 1),\n",
       " (('increasingly', 'RB'), 1),\n",
       " (('depend', 'VBP'), 1),\n",
       " (('upon', 'NN'), 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ', 'VBP']\n",
    "\n",
    "# 최빈어 조회, 최빈어를 조회하여 불용어 제거 대상을 선정\n",
    "from collections import Counter\n",
    "Counter(taggedToken).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'mean', 'everybody', \"'s\", 'job', 'yours', 'mine', 'onward', 'role', 'grain', 'farmers', 'Egypt', 'pastry', 'chefs', 'Paris', 'dog', 'walkers', 'Oregon', 'i.e', '.', 'job', '.', 'We', 'now', 'help', 'direct', 'workers', 'actions', 'behavior', 'new', 'degree', 'intelligence', 'that', 'predictive', 'analytics', 'stemming', 'AI', 'we', 'now', 'increasingly', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "stopWord = [',','be','able']\n",
    "\n",
    "word = []\n",
    "for tag in taggedToken:\n",
    "    if tag[1] not in stopPos:\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])\n",
    "            \n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-7. 영문 텍스트 전처리 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
