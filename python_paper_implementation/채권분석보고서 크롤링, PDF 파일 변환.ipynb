{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver 채권분석리포트 Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 날짜, 증권사, 제목, pdf 다운받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPDF2나 pdfminer를 찾을 수 없다고 한다면, 아래 주석을 풀고 install을 시도하세요.\n",
    "# pip install PyPDF2 \n",
    "# pip install pdfminer.six\n",
    "\n",
    "# moduel import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib.request\n",
    "from PyPDF2 import PdfFileReader\n",
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "# 함수 정의\n",
    "def BondReportsCrawl():\n",
    "    pdf_list = [] # pdf의 날짜, 증권사, 제목을 담을 빈 리스트 생성\n",
    "    BASE_DIR = os.getcwd() # 현재 작업하고 있는 곳을 BASE_DIR로 정의\n",
    "    REPORTS_DIR = BASE_DIR # REPORTS_DIR = BASE_DIR\n",
    "    \n",
    "    # Naver의 채권분석리포트의 마지막 페이지는 116이므로 117까지 for문 돌릴 준비\n",
    "    for i in range(1, 117):\n",
    "        print(i,'번 페이지에 있는 문서를 가져오는 중입니다.') # 작업 상황 확인을 위함 \n",
    "        url = 'https://finance.naver.com/research/debenture_list.nhn?&page={}'.format(i) # page={}에 i 대입\n",
    "        response = requests.get(url) # url로 requests를 보냄 \n",
    "        soup = BeautifulSoup(response.content, 'html.parser') # soup 생성\n",
    "        reports = soup.find('table', class_='type_1').find_all('tr')[1:] # 찾아야할 content 범위 설정\n",
    "\n",
    "        # 날짜, 증권사, 제목, pdf 다운\n",
    "        for td in reports:\n",
    "            filename = \"\" # filename 을 만들 빈 문자열 생성, for문이 돌 때마다 초기화\n",
    "\n",
    "            # date 코드\n",
    "            date = td.find('td', class_='date')\n",
    "            if date is not None: # None 값들이 검사되므로 if문 추가\n",
    "                date = date.text # text만 추출\n",
    "                filename += date # filename 에 추가\n",
    "                filename += '_' # 다음에 들어올 author와의 구분을 위한 _ 추가 \n",
    "\n",
    "            # author 코드\n",
    "            author = td.find_all('td') \n",
    "            if len(author) == 5: # 길이를 확인해보니, len이 5인 것만 정상적으로 증권사 이름들이 추출됨 \n",
    "                author = author[1].text\n",
    "                filename += author\n",
    "                filename += '_'  # 다음에 들어올 title와의 구분을 위한 _ 추가 \n",
    "\n",
    "            # title 코드 \n",
    "            title = td.find('td').find('a')\n",
    "            if title is not None: # None 값들이 검사되므로 if문 추가\n",
    "                title = title.text\n",
    "                filename += title # filename 에 추가 \n",
    "                # computer가 읽을 수 없는 특수문자들을 대체 (후에 정규표현식으로 고칠 것)\n",
    "                # 공백제거 코드도 추가해 줄 것 \n",
    "                filename = filename.replace('?','').replace('.','').replace('!','').replace('/','')\n",
    "                filename = filename.replace(':','').replace(';','').replace('=','').replace('+','')\n",
    "                filename = filename.replace('-','').replace('*','').replace('\"','').replace('<','')\n",
    "                filename = filename.replace('>','').replace('[','').replace(']','')\n",
    "                filename = filename + '.pdf'\n",
    "                pdf_list.append(filename) # pdf_list 에 filename을 추가함 \n",
    "\n",
    "            # pdf 다운받기 코드\n",
    "            pdf_href = td.find('td', class_='file')\n",
    "            if pdf_href is not None: # None값이 있기 때문에 if문 추가\n",
    "                pdf_href = str(pdf_href.find('a')['href'])\n",
    "                pdf_url = pdf_href\n",
    "                print(pdf_url) # pdf_url 출력해서 제대로 된 url인지 확인\n",
    "\n",
    "                # 가장 중요한 부분. requests.get을 활용해도 pdf 파일을 다운 받을 수 있다!!! \n",
    "                r = requests.get(pdf_url) # url로 requests 보냄\n",
    "                naver_reports_path = os.path.join(REPORTS_DIR, filename) # 작업하고 있는 폴더에 파일 추가\n",
    "\n",
    "                with open(naver_reports_path, 'wb') as f: # 파일 쓰기\n",
    "                    f.write(r.content)\n",
    "\n",
    "    print(pdf_list)\n",
    "    print(\"총 가져온 pdf파일 개수: \", len(pdf_list))\n",
    "    \n",
    "BondReportsCrawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. pdf -> txt 파일로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf -> txt 로 변환하는 함수                \n",
    "def pdfparser(data):\n",
    "    ccount = 0\n",
    "    BASE_DIR = os.getcwd()\n",
    "    REPORTS_DIR = BASE_DIR\n",
    "    \n",
    "    try: # 오류가 발생하는 pdf 들이 존재하기 때문에, try문 사용\n",
    "        fp = open(data, 'rb')\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        retstr = io.StringIO()\n",
    "        codec = 'utf-8'\n",
    "        laparams = LAParams()\n",
    "        device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        for page in PDFPage.get_pages(fp):\n",
    "            interpreter.process_page(page)\n",
    "            data =  retstr.getvalue()\n",
    "            text_path = os.path.join(REPORTS_DIR, pdf_list[i]+'.txt')\n",
    "            with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(data)\n",
    "\n",
    "    except AttributeError:\n",
    "        ccount += 1\n",
    "        print(pdf_list[i],'는 Error로 인해 pass합니다.')\n",
    "        print(\"현재까지의 누적 Errors: \", ccount)\n",
    "        pass\n",
    "    \n",
    "# 이 작업을 통해, pdf_list에서 날짜, 증권사, 제목을 확인할 수 있고,\n",
    "# pdf 파일을 얻을 수 있다.\n",
    "# 또한, pdf 파일을 txt로 변환하는 함수까지 정의해 놓은 상태\n",
    "# 아래에 있는 코드를 실행하면, pdf를 txt로 변환하기 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 시간이 꽤 오래 걸립니다...!!\n",
    "# pdf -> txt 로 변환하는 code\n",
    "# 오류가 나면, range()를 수정하면서 변환\n",
    "for i in range(1, (len(pdf_list)+1)):\n",
    "    print(i)\n",
    "    print(pdf_list[i], '를 txt로 변환 중입니다...') \n",
    "    pdfparser(pdf_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## txt 파일 통합, csv로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 파일 개수 확인 코드 (불필요)\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "\n",
    "# source_folder=r\"C:\\Users\\student\\Documents\\python_basic\\paper_implementation\\text_data\"\n",
    "# output_file=r\"C:\\Users\\student\\Documents\\python_basic\\paper_implementation\\bondreports.csv\"\n",
    "\n",
    "# txt_files = [f for f in listdir(source_folder) if isfile(join(source_folder, f))]\n",
    "# print(len(txt_files), \"개의 파일이 존재합니다.\")\n",
    "\n",
    "## 채권분석보고서 파일 개수는 3370개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import csv\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# 파일 경로 확인할 것\n",
    "def txt2csv(source_folder=r\"C:\\Users\\student\\Documents\\python_basic\\paper_implementation\\text_data\",\n",
    "            output_file=r\"C:\\Users\\student\\Documents\\python_basic\\paper_implementation\\bondreports.csv\") :\n",
    "    \n",
    "    # 지정 폴더 내 파일 목록 조회 (파일만)\n",
    "    txt_files = [f for f in listdir(source_folder) if isfile(join(source_folder, f))]\n",
    "    df = pd.DataFrame(columns=['date', 'firm', 'title', 'report'])\n",
    "    print(len(txt_files), \"개의 파일이 존재합니다.\")\n",
    "    \n",
    "    for txt_file in txt_files :\n",
    "        print(txt_file)\n",
    "        if txt_file[-3:] != 'txt' :\n",
    "            continue\n",
    "        with open(source_folder + \"\\\\\" + txt_file, 'r', encoding='utf-8') as f:\n",
    "            txt = f.read().strip()\n",
    "            date = txt_file.split('_')[0]\n",
    "            firm = txt_file.split('_')[1]\n",
    "            title = txt_file.split('_')[2].split('.')[0]\n",
    "            df.loc[len(df)] = [date, firm, title, txt]\n",
    "\n",
    "    df.to_csv(output_file, index=False)\n",
    "        \n",
    "txt2csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"bondreports.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token, n-gram 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ekonlpy.sentiment import MPCK\n",
    "\n",
    "def text2ngram(text) :\n",
    "    mpck = MPCK()\n",
    "    bef_tokens = mpck.tokenize(text)\n",
    "    ngrams = mpck.ngramize(bef_tokens)\n",
    "    tokens = []\n",
    "    print(ngrams)\n",
    "\n",
    "    stoppos = [\n",
    "        'SC','SY','SF','SE','SS','SP','SO','SW',\n",
    "        'SSC','JKS','JKC','JKG','JKO','JKB','JKV',\n",
    "        'JKQ','JX','JC','EF','EC','ETN','ETM','JKS',\n",
    "        'JKC','JKG','JKO','JKB','JKV','JKQ','JC','JX',\n",
    "        'EP','EF','EC','ETN','ETM','XPN','XSN','XSV','XSA',\n",
    "        'XR','SF','SE','SSO','SSC','SC','SY','SH','SL','SN'\n",
    "    ]\n",
    "    stopword = ['']\n",
    "\n",
    "    for tag in bef_tokens:\n",
    "        if tag[1] not in stoppos:\n",
    "            if tag[0] not in stopword:\n",
    "                tokens.append(tag)\n",
    "                \n",
    "    return tokens, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제\n",
    "# text2ngram('글로벌 신용경색 완화 조짐과 주식의 반등, 인플레이션 및 금통위 부담 등에도 불구하고 금리가 잘 버티고 있다. 이는 물론, 대내외 펀더멘틀 및 궁극적인 통화정책 방향에대한 믿음이 강하기 때문일 것이다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의, 오랜 시간이 소요됩니다!! \n",
    "import pandas as pd\n",
    "from ekonlpy.sentiment import MPCK\n",
    "\n",
    "def Preprocessing(input_file=r\"C:\\Users\\student\\Documents\\python_basic\\paper_implementation\\bondreports.csv\",\n",
    "                  output_file=r\"C:\\Users\\student\\Documents\\python_basic\\paper_implementation\\bondreports_preprocessed.csv\") :\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['tokens'] = ''\n",
    "    df['ngram'] = ''\n",
    "\n",
    "    #return \n",
    "    for row_idx in df.index: \n",
    "        text = df['report'][row_idx]\n",
    "        \n",
    "        try : \n",
    "            tokens,ngrams = text2ngram(text)\n",
    "            df['tokens'][row_idx] = \",\".join(tokens)\n",
    "            df['ngram'][row_idx] = \",\".join(ngrams)\n",
    "            print(str(row_idx+1) +'/'+ str(df.shape[0]) +'번째 파일 전처리 완료')\n",
    "        except Exception as e:\n",
    "            print(\"============ {} ===========\".format(str(e)))\n",
    "            print(\"Error :\" + text)\n",
    "        \n",
    "    df.to_csv(output_file, index=False)\n",
    "        \n",
    "Preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜형식 통합, 콜금리와 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Bondreports_preprocessed = pd.read_csv('bondreports_preprocessed.csv')\n",
    "Bondreports_preprocessed['date'] = Bondreports_preprocessed['date'] + 20000000\n",
    "Bondreports_preprocessed['date'] = Bondreports_preprocessed['date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "\n",
    "Callrate_df = pd.read_csv('(최종)_callrate_fulldays.csv', index_col=0)\n",
    "Callrate_df['date'] = pd.to_datetime(Callrate_df['date'])\n",
    "Merged_df = pd.merge(Bondreports_preprocessed, Callrate_df)\n",
    "Merged_df.to_csv('(최종)_채권분석보고서_전처리.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요없는 열 삭제\n",
    "Bondreports_df = pd.read_csv('(최종)_채권분석보고서_전처리.csv', index_col=0)\n",
    "Bondreports_df['ngrams'] = \"\"\n",
    "Bondreports_df['ngrams'] = Bondreports_df['tokens'] + Bondreports_df['ngram']\n",
    "Bondreports_df.drop(['title','firm', 'report','tokens','ngram','old_date','callrate','old_callrate'],axis='columns', inplace=True)\n",
    "Bondreports_df = Bondreports_df.rename(columns={'ngrams':'ngram'})\n",
    "Bondreports_df.to_csv(\"(병합용)_채권분석보고서_전처리.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"(병합용)_채권분석보고서_전처리.csv\", index_col=0)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
