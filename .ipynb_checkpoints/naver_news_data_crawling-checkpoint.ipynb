{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. url을 읽는다.\n",
    "# 최종 정답. 아래꺼 볼 필요 x\n",
    "NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "query = input(str(\"키워드를 입력하세요: \"))\n",
    "# 나중에 input으로 바꿀 수있다. \n",
    "for i in range(1,11):\n",
    "    params = {\n",
    "        'where': 'news',\n",
    "        'query': query,\n",
    "        'start': int((i*10)+1)\n",
    "    }\n",
    "    resp = requests.get(NAVER_SEARCH_URL, params=params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    ul_contents = soup.find('ul', class_='type01')\n",
    "    new_ul = []\n",
    "    for content in ul_contents:\n",
    "        ul_contents.contents\n",
    "        if not str(content).strip():\n",
    "            continue\n",
    "        new_ul.append(content)\n",
    "    news_list = []\n",
    "\n",
    "    for li in new_ul:\n",
    "        news_dict = {}\n",
    "        a_tag = li.find('dl').find('dt').find('a')\n",
    "#    print(a_tag)\n",
    "        news_dict['link'] = a_tag['href']\n",
    "        news_dict['title'] = a_tag.text\n",
    "\n",
    "        b_tag = li.find('dl').find('dd', class_ = \"txt_inline\").find('span')\n",
    "        news_dict['company'] = b_tag.contents[0]\n",
    "    # 왜 contents[0] 인가요? 리스트로 접근하는 방법입니다.\n",
    "    \n",
    "        c_tag = b_tag.find_next('dd')\n",
    "        news_dict['script'] = c_tag.text\n",
    "#    print(news_dict)\n",
    "        news_list.append(news_dict)\n",
    "    \n",
    "    pprint(news_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_ul)\n",
    "# 몇개가 있어?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 심화. \n",
    "#### 한 페이지에 있는 기사내용 긁어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답.\n",
    "news_list = []\n",
    "NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "# 찾고자하는 페이지의 URL과 변수선언\n",
    "query = input(str(\"키워드를 입력하세요: \"))\n",
    "# 찾고자 하는 키워드를 입력하세요.\n",
    "\n",
    "for i in range(1,11):\n",
    "    # start_var = 1\n",
    "    params = {\n",
    "        'where': 'news',\n",
    "        'query': query,\n",
    "        'start': int(i*10)+1\n",
    "    }\n",
    "    resp = requests.get(NAVER_SEARCH_URL, params=params)\n",
    "    # 검색결과를 가져온다. (page 별로)\n",
    "    \n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    # 검색결과를 soup로 만든다.\n",
    "    \n",
    "    # ul의 ' ' 제거\n",
    "    ul_contents = soup.find('ul', class_='type01')\n",
    "    new_ul = []\n",
    "    for content in ul_contents:\n",
    "        ul_contents.contents\n",
    "        if not str(content).strip():\n",
    "            continue\n",
    "        new_ul.append(content)\n",
    "    # 컨텐츠 추가 완료 \n",
    "                \n",
    "    # 리스트 탐색\n",
    "    for li in new_ul:\n",
    "        news_dict = {}\n",
    "        a_tag = li.find('dl').find('dt').find('a')\n",
    "\n",
    "        news_dict['link'] = a_tag['href']\n",
    "        news_dict['title'] = a_tag.text\n",
    "        \n",
    "        print(news_dict['link'], '에 들어갑니다.')\n",
    "        # 디테일 페이지 링크를 들어갑니다.\n",
    "        detail_resp = requests.get(news_dict['link'])\n",
    "        detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')\n",
    "        news_dict['body'] = detail_soup.find('body')\n",
    "\n",
    "        b_tag = li.find('dl').find('dd', class_ = \"txt_inline\").find('span')\n",
    "        news_dict['company'] = b_tag.contents[0]\n",
    "        # 왜 contents[0] 인가요? 리스트로 접근하는 방법입니다.\n",
    "    \n",
    "        c_tag = b_tag.find_next('dd')\n",
    "        news_dict['script'] = c_tag.text\n",
    "        news_list.append(news_dict)\n",
    "    \n",
    "    pprint(news_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개발 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. url을 파악한다.\n",
    "2. url에 접속한다.\n",
    "3. html 파일을 soup로 만든다. \n",
    "4. soup에서 각 항목들을 정리한다. # 제일어려움\n",
    "5. soup에서 상세페이지의 href를 가져온다.\n",
    "6. 각 상세페이지의 href를 requests.get() 한다.\n",
    "7. soup로 만들고 상세페이지의 각 항목들을 정리한다.\n",
    "8. 반목문을 통해서 pagination을 이동한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예시\n",
    "\n",
    "< a href = 뭐있음, class = 'link' active >는 예시가 있습니다.\n",
    "href를 가지고 오자.\n",
    "1. a_tag['href']\n",
    "2. a_tag.attr\n",
    "    {'href': ......\n",
    "     'class': 'link' active\n",
    "    }\n",
    "3. a_tag.attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스토픽 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5c96b425ee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mNAVER_NEWS_TOPIC\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnews_topic_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0ma_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"keyword\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mnews_topic_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma_tag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# url을 파악한다.\n",
    "# url에 접속한다.\n",
    "# html 파일을 soup로 만든다.\n",
    "# soup에서 각 항목들을 정리한다. # 제일어려움\n",
    "# soup에서 상세페이지의 href를 가져온다.\n",
    "# 각 상세페이지의 href를 requests.get() 한다.\n",
    "# soup로 만들고 상세페이지의 각 항목들을 정리한다.\n",
    "# 반목문을 통해서 pagination을 이동한다.\n",
    "\n",
    "NAVER_SEARCH = \"https://search.naver.com/search.naver\"\n",
    "resp = requests.get(NAVER_SEARCH)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "NAVER_NEWS_TOPIC = soup.find('div', class_= \"realtime_srch _aside_news_tab\")\n",
    "\n",
    "for ol in NAVER_NEWS_TOPIC:\n",
    "    news_topic_dict = {}\n",
    "    a_tag = ol.find('li').find('a').find('span', class_=\"keyword\")\n",
    "    news_topic_dict['title'] = a_tag['span']\n",
    "\n",
    "print(news_topic_dict)\n",
    "# 오류: 'NoneType' object is not iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 방법3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ol class=\"lst_realtime_srch _tab_area\"><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%EB%B6%88%EB%B2%95%ED%8F%90%EA%B8%B0%EB%AC%BC+%ED%8A%B9%EB%B3%84%EC%88%98%EC%82%AC%EB%8B%A8+%EB%B0%9C%EC%A1%B1\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=1&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">1</em><span class=\"tit\">불법폐기물 특별수사단 발족</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%ED%95%9C%EC%84%A0%EA%B5%90+%EA%B8%B0%EC%9E%90%EB%93%A4+%ED%96%A5%ED%95%B4\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=2&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">2</em><span class=\"tit\">한선교 기자들 향해</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%EA%B2%80%EC%B0%B0+%EC%BD%94%EC%98%A4%EB%A1%B1%EC%83%9D%EB%AA%85%EA%B3%BC%ED%95%99+%EC%95%95%EC%88%98%EC%88%98%EC%83%89\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=3&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">3</em><span class=\"tit\">검찰 코오롱생명과학 압수수색</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%EA%B5%AD%EC%84%B8%EC%B2%AD+%EC%82%AC%EC%B9%AD\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=4&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">4</em><span class=\"tit\">국세청 사칭</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%EB%8C%80%EC%9A%B0%EA%B1%B4%EC%84%A4+%EC%9D%84%EC%A7%80%EB%A1%9C%EC%8B%9C%EB%8C%80+%EA%B0%9C%EB%A7%89\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=5&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">5</em><span class=\"tit\">대우건설 을지로시대 개막</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%EC%A0%95%EA%B2%BD%EB%91%90+%EA%B5%AD%EB%B0%A9\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=6&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">6</em><span class=\"tit\">정경두 국방</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%EB%B0%98%EB%A0%A4%EA%B2%AC+%EC%88%98%EC%98%81%EB%8C%80%ED%9A%8C\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=7&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">7</em><span class=\"tit\">반려견 수영대회</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%ED%95%9C%EB%AF%B8+%EA%B5%AD%EB%B0%A9%EC%9E%A5%EA%B4%80%ED%9A%8C%EB%8B%B4\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=8&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">8</em><span class=\"tit\">한미 국방장관회담</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx bx_item\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%EC%84%9C%EC%9A%B8%EC%97%B0%EA%B5%AC%EC%9B%90+%EB%AF%BC%EC%A3%BC%EC%97%B0%EA%B5%AC%EC%9B%90\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=9&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">9</em><span class=\"tit\">서울연구원 민주연구원</span></span><em class=\"rank new\"><span class=\"spim\"></span><span>NEW</span></em></a></li><li><a class=\"bx\" href=\"//search.naver.com/search.naver?where=nexearch&amp;sm=tab_htk.nws&amp;ie=utf8&amp;query=%EC%9C%A0%EC%8B%9C%EB%AF%BC+%ED%99%8D%EC%A4%80%ED%91%9C\" onclick=\"return goOtherCR(this, 'a=htk.nwslist&amp;r=10&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\"><span class=\"keyword\"><em class=\"num\">10</em><span class=\"tit\">유시민 홍준표</span></span></a></li></ol>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_topic = soup.find('ol', class_=\"lst_realtime_srch _tab_area\")\n",
    "news_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. contents 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['불법폐기물 특별수사단 발족',\n",
       " '한선교 기자들 향해',\n",
       " '검찰 코오롱생명과학 압수수색',\n",
       " '국세청 사칭',\n",
       " '대우건설 을지로시대 개막',\n",
       " '정경두 국방',\n",
       " '반려견 수영대회',\n",
       " '한미 국방장관회담',\n",
       " '서울연구원 민주연구원',\n",
       " '유시민 홍준표']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_tags = news_topic.contents\n",
    "new_list = []\n",
    "for t in li_tags:\n",
    "    title = t.find('span', class_='tit').text\n",
    "    new_list.append(title)\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. sibilings 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'li' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f14552884816>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnew_list2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mnew_list2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mli_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 첫번째 리스트는 for 문을 돌지 않는다. 아래 코드가 sibilings를 찾는 것이기 때문이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'li' is not defined"
     ]
    }
   ],
   "source": [
    "li_tag = news_topic.li\n",
    "\n",
    "new_list2 = []\n",
    "\n",
    "title = li.find('span', class_='tit').text\n",
    "new_list2.append(li_tag.text)\n",
    "# 첫번째 리스트는 for 문을 돌지 않는다. 아래 코드가 sibilings를 찾는 것이기 때문이다.\n",
    "\n",
    "for x in li_tag.find_next_siblings('li'):\n",
    "    title - li.find('span', class_='tit').text\n",
    "    new_list2.append(li.text)\n",
    "    \n",
    "new_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_all(recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_lists = news_topic.find_all('li', recursive=False)\n",
    "len(li_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['불법폐기물 특별수사단 발족',\n",
       " '한선교 기자들 향해',\n",
       " '검찰 코오롱생명과학 압수수색',\n",
       " '국세청 사칭',\n",
       " '대우건설 을지로시대 개막',\n",
       " '정경두 국방',\n",
       " '반려견 수영대회',\n",
       " '한미 국방장관회담',\n",
       " '서울연구원 민주연구원',\n",
       " '유시민 홍준표']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list3 = []\n",
    "for li in li_lists:\n",
    "    title = li.find('span', class_='tit').text\n",
    "    new_list3.append(title)\n",
    "    \n",
    "new_list3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
