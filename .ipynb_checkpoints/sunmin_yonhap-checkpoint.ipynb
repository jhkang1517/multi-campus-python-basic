{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과거 뉴스 크롤러(네이버 뉴스를 통해 크롤링하기)\n",
    "# keyword와 날짜 그리고 if문에서 언론사명만 바꿔주면 원하는 언론사 크롤링 가능\n",
    "\n",
    "# 2005 ~ 2011까지의 연합인포맥스 뉴스 가져오기\n",
    "# 네이버 뉴스에서 기간 짧게(하루-이틀정도) 설정하고 나오는 뉴스 목록 중\n",
    "# 언론사명이 '연합인포맥스'인 경우 링크 타고 들어가고\n",
    "# 그 뉴스 상세 페이지에서 원하는 내용 크롤링하기\n",
    "# 날짜 짧게 설정해서 request 날리는것 필요함. \n",
    "\n",
    "# span._sp_each_source 또는 span.press 에서 '연합인포맥스'일 경우 가져온다.\n",
    "# span.press는 줄기가 아니라 밑에 딸린 가지 뉴스임 -> 관련뉴스\n",
    "\n",
    "import scrapy\n",
    "import re\n",
    "from datetime import timedelta, date\n",
    "from urllib import parse\n",
    "import time\n",
    "import random\n",
    "from time import sleep\n",
    "# https://search.naver.com/search.naver?\n",
    "# &where=news\n",
    "# &query=%EA%B8%88%EB%A6%AC\n",
    "# &nso=so:r,p:from20050101to20050102,a:all\n",
    "# &start=1\n",
    "\n",
    "start_date = date(2005, 1, 1)\n",
    "end_date = date(2011, 2, 28)\n",
    "cnt_per_page = 10\n",
    "keyword = \"금리\"\n",
    "url_format = 'https://search.naver.com/search.naver?&where=news&query={0}&start={1}&nso=so:r,p:from{2}to{2},a:all'\n",
    "\n",
    "item = {}\n",
    "class NewscrawlSpider(scrapy.Spider):\n",
    "    def date_range(start_date, end_date):\n",
    "        for i in range(int ((end_date - start_date).days)+1):\n",
    "            yield start_date + timedelta(i)\n",
    "\n",
    "    name = 'newscrawl'\n",
    "    start_urls = []\n",
    "\n",
    "    for single_date in date_range(start_date, end_date):\n",
    "        start_urls.append(url_format.format(keyword,1,single_date.strftime('%Y%m%d')))     \n",
    "\n",
    "    # allowed_domains = ['naver.com']\n",
    "    def parse(self, response):\n",
    "        for news in response.css('dl'):  \n",
    "            title = news.css('dd span._sp_each_source::text').get()\n",
    "\n",
    "            if title == '연합인포맥스':\n",
    "                link = news.css('dd a._sp_each_url::attr(href)').get()\n",
    "\n",
    "                yield response.follow(link, self.parse_detail)\n",
    "            else:\n",
    "                pass\n",
    "            ############################\n",
    "            ##### 관련뉴스 수집하기 #####\n",
    "            ############################\n",
    "            title2 = news.css('span.press::text').getall()\n",
    "\n",
    "            if len(title2) > 0 :\n",
    "                for idx, val in enumerate(title2):\n",
    "                    if val == '연합인포맥스': \n",
    "                        link = news.css('ul.relation_lst li a::attr(href)').getall()\n",
    "                        # print('***************************', link)\n",
    "                        link = link[2*idx]\n",
    "                        yield response.follow(link, self.parse_detail) \n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        total_cnt = int(re.sub('건', '', response.css('div.title_desc.all_my span::text').get().split('/')[1])) \n",
    "        query_str = parse.parse_qs(parse.urlsplit(response.url).query)   # url에서 query 분해해서 저장하는 변수\n",
    "        currpage = int(query_str['start'][0])\n",
    "    \n",
    "        startdate = query_str['nso'][0]\n",
    "        print(\"=================== [\" + startdate + '] ' + str(currpage) + '/' + str(total_cnt) + \"===================\") \n",
    "        ################################################\n",
    "        ###########    TIME ############################\n",
    "        ################################################\n",
    "        sleep(1.5)\n",
    "        ################################################\n",
    "        ################################################\n",
    "        if currpage  < total_cnt : \n",
    "            yield response.follow(url_format.format(keyword, currpage+10, startdate) , self.parse)\n",
    "            \n",
    "    def parse_detail(self, response):\n",
    "        table = response.css('div.content')\n",
    "        \n",
    "        press = table.css('div.press_logo a img::attr(title)').get()\n",
    "        title = table.css('h3::text').get()\n",
    "        date = table.css('span.t11::text').get().split(' ')[0].replace('.','')\n",
    "        link = response.url\n",
    "        content = str(table.xpath('//div[@id=\"articleBodyContents\"]/text()').getall())\n",
    "        content = re.sub(' +', ' ', str(re.sub(re.compile('<.*?>'), ' ', content.replace('\"','')).replace('\\r\\n','').replace('\\n','').replace('\\t','').replace('\\u200b','').replace('\\\\n\\\\t','').replace('\\\\n\\\\n','').replace('\\\\n','').replace('\\\\n\\\\t\\\\n\\\\t','').replace('\\\\n\\\\n\\\\t','').replace(' ','').strip()))\n",
    "\n",
    "        item['press'] = press\n",
    "        item['date'] = date\n",
    "        item['title'] = title \n",
    "        item['link'] = link\n",
    "        item['content'] = content\n",
    "\n",
    "        yield item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
